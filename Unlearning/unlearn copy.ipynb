{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e8eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning\n",
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50571c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "import quadprog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360b5c4aaa0b84a",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545fa36453e0dda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T15:16:56.387836Z",
     "start_time": "2024-11-13T15:16:56.266657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# split the data into 10 classes by doing sort by key where in the keys are the labels and the values are the data\n",
    "train_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(train_data, train_labels):\n",
    "    train_split[CLASSES[label]].append(img)\n",
    "    \n",
    "# this makes more sense to me, effectively indexes 0-5000 are all airplanes, 5000-10000 are all automobiles etc\n",
    "test_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(test_data, test_labels):\n",
    "    test_split[CLASSES[label]].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849681c0f0bdd16",
   "metadata": {},
   "source": [
    "# PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b345dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "# make cuda available if available\n",
    "\n",
    "from cifar import load_cifar10_data, show_image\n",
    "\n",
    "# we want to create a resnet 18 model for 32x32 images\n",
    "# avoid upscaling, the model will take 32x32 images on input as opposed to 224x224\n",
    "\n",
    "initialisation = time.time()\n",
    "\n",
    "class ResNet18CIFAR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        # change the first layer to accept 32x32 images with 3 channels rather than 224x224 images\n",
    "        # check the size of the input layer\n",
    "        print(\"|| conv1 weight size: \", self.resnet.conv1.weight.size())\n",
    "        print(\"|| fc weight size: \", self.resnet.fc.weight.size())\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet.fc = torch.nn.Linear(512, 10)\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "        # change input layer to accept 32x32 images\n",
    "\n",
    "\n",
    "        \n",
    "        # List all layers in the resnet18 model\n",
    "        for name, layer in self.resnet.named_children():\n",
    "            print(f\"Layer: {name} -> {layer}\")\n",
    "        \n",
    "        #print(\"|| conv1 weight size: \", self.resnet.conv1.weight.size())\n",
    "        #print(\"|| fc weight size: \", self.resnet.fc.weight.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7e1518a20b7df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18CIFAR(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('models/resnet18_cifar77ACC.pth',  map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe76d5",
   "metadata": {},
   "source": [
    "# INITIAL MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9dd6044c0b8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||===================START TEST/TRAIN ACCURACY=================||\n",
      "|| Test Set Accuracy: 77.34%\n",
      "||==============================END==========================||\n",
      "|| Time to load data: 1731585207.84s\n",
      "|| Time to get accuracy: 422.21s\n",
      "|| Total time: 1731585630.05s\n",
      "||==============================END==========================||\n"
     ]
    }
   ],
   "source": [
    "# turn the data into a tensor\n",
    "test_data_tensor = torch.tensor(test_data).float()\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# keep track of accuracy\n",
    "correct1 = 0\n",
    "correct2 = 0\n",
    "total1 = len(train_data)\n",
    "total2 = len(test_data)\n",
    "\n",
    "# start test/train accuracy timer\n",
    "accuracy = time.time()\n",
    "\n",
    "print(\"||===================START TEST/TRAIN ACCURACY=================||\")\n",
    "\n",
    "# move tensors to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    test_data_tensor = test_data_tensor.cuda()\n",
    "    test_labels_tensor = test_labels_tensor.cuda()\n",
    "    \n",
    "            \n",
    "# test the model\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_data), 1000):\n",
    "        # get the input and output\n",
    "        img = test_data_tensor[i:i+1000]\n",
    "        label = test_labels_tensor[i:i+1000]\n",
    "        \n",
    "\n",
    "        # normalise the image\n",
    "        #img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "\n",
    "        # get the prediction\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # get the number of correct predictions\n",
    "        correct2 += (predicted == label).sum().item()\n",
    "        \n",
    "        del img\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"|| Test Set Accuracy: {correct2 / total2 * 100:.2f}%\")\n",
    "\n",
    "print(\"||==============================END==========================||\")\n",
    "\n",
    "# calculate the time taken to get the accuracy\n",
    "accuracy = time.time() - accuracy\n",
    "\n",
    "# print timing information\n",
    "print(f\"|| Time to load data: {initialisation:.2f}s\")\n",
    "print(f\"|| Time to get accuracy: {accuracy:.2f}s\")\n",
    "print(f\"|| Total time: {initialisation + accuracy:.2f}s\")\n",
    "\n",
    "print(\"||==============================END==========================||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55059d",
   "metadata": {},
   "source": [
    "# UNLEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c22b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning/Unlearning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "umodel = copy.deepcopy(model)\n",
    "%cd Unlearning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c590ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn.scrub import train_distill, iterative_unlearn, scrub\n",
    "from utils import DistillKL, AverageMeter, accuracy\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, ConcatDataset, Subset\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "train_dataset = TensorDataset(torch.tensor(train_data), torch.tensor(train_labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_data), torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e88c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Learning rate: 0.1\n",
      "len(r_loader): 704, len(f_loader): 79\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m module_list \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([model_s, model_t])\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Now, call the RL function with adjusted arguments\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mscrub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# The unlearned model is now in 'model'\u001b[39;00m\n\u001b[1;32m     93\u001b[0m unlearned_model \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/CS407/Machine-Unlearning-x-Continual-Learning/Unlearning/unlearn/impl.py:251\u001b[0m, in \u001b[0;36m_iterative_unlearn_impl.<locals>._wrapped\u001b[0;34m(data_loaders, model, criterion, args, mask, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch #\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Learning rate: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    247\u001b[0m             epoch, optimizer\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m     )\n\u001b[0;32m--> 251\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43munlearn_iter_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone epoch duration:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "File \u001b[0;32m~/CS407/Machine-Unlearning-x-Continual-Learning/Unlearning/unlearn/scrub.py:146\u001b[0m, in \u001b[0;36mscrub\u001b[0;34m(data_loaders, module_list, criterion, optimizer, epoch, args, mask)\u001b[0m\n\u001b[1;32m    144\u001b[0m iter_per_epoch_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mmsteps:\n\u001b[0;32m--> 146\u001b[0m     maximize_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_distill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43miter_per_epoch_forget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m train_distill(epoch, r_loader, module_list, criterion_list, optimizer,\n\u001b[1;32m    149\u001b[0m                                       iter_per_epoch_train, args\u001b[38;5;241m.\u001b[39mgamma, args\u001b[38;5;241m.\u001b[39mbeta,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m train-acc:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m train-loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/CS407/Machine-Unlearning-x-Continual-Learning/Unlearning/unlearn/scrub.py:42\u001b[0m, in \u001b[0;36mtrain_distill\u001b[0;34m(epoch, train_loader, module_list, criterion_list, optimizer, max_iter, gamma, beta, split, print_freq, quiet)\u001b[0m\n\u001b[1;32m     39\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     44\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n",
      "File \u001b[0;32m~/CS407/Machine-Unlearning-x-Continual-Learning/.venv/lib64/python3.9/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from unlearn.RL import RL\n",
    "from unlearn.scrub import scrub\n",
    "\n",
    "# Assume 'train_data' and 'train_labels' are your training data and labels\n",
    "# 'test_data' and 'test_labels' are your test data and labels\n",
    "# 'model' is your pretrained ResNet model\n",
    "\n",
    "# Define the class you want to unlearn\n",
    "class_to_unlearn = 0  # Change this to the class you want to unlearn\n",
    "\n",
    "\n",
    "# Convert data and labels to tensors if they aren't already\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# Create TensorDatasets directly for 'forget' and 'retain' subsets\n",
    "forget_indices = (train_labels_tensor == class_to_unlearn).nonzero(as_tuple=True)[0]\n",
    "retain_indices = (train_labels_tensor != class_to_unlearn).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Create 'forget' dataset\n",
    "forget_data = train_data_tensor[forget_indices]\n",
    "forget_labels = train_labels_tensor[forget_indices]\n",
    "forget_dataset = TensorDataset(forget_data, forget_labels)\n",
    "\n",
    "# Create 'retain' dataset\n",
    "retain_data = train_data_tensor[retain_indices]\n",
    "retain_labels = train_labels_tensor[retain_indices]\n",
    "retain_dataset = TensorDataset(retain_data, retain_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=64, shuffle=True)\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Prepare the data_loaders dictionary\n",
    "data_loaders = {'forget': forget_loader, 'retain': retain_loader}\n",
    "\n",
    "# Create the test DataLoader\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Move the model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.unlearn_lr = 0.1         # Learning rate for unlearning\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 5e-4\n",
    "        self.dataset = ''      # Change as needed\n",
    "        self.num_classes = 10         # Number of classes in the dataset\n",
    "        self.batch_size = 64\n",
    "        self.print_freq = 10\n",
    "        self.warmup = 0               # Number of warmup epochs\n",
    "        self.imagenet_arch = False    # Set to True if using ImageNet architecture\n",
    "        self.seed = 42                # For reproducibility\n",
    "        self.kd_T = 1\n",
    "        self.msteps = 1\n",
    "        self.gamma = 10\n",
    "        self.beta = 1\n",
    "\n",
    "\n",
    "        # Add the following attributes to ensure compatibility\n",
    "        self.decreasing_lr = '50,75'  # Comma-separated epochs where LR decays\n",
    "        self.rewind_epoch = 0         # Epoch to rewind to; set to 0 if not using rewinding\n",
    "        self.rewind_pth = ''          # Path to the rewind checkpoint\n",
    "        self.gpu = 0                  # GPU ID to use; adjust as needed\n",
    "        self.surgical = False         # Whether to use surgical unlearning\n",
    "        self.choice = []              # Layers to unlearn surgically; list of layer names\n",
    "        self.unlearn = 'retrain'      # Unlearning method, e.g., 'retrain'\n",
    "        self.unlearn_epochs = 10     # Number of epochs for unlearning\n",
    "        self.epochs = 100             # Total number of epochs (used for scheduler)    # For reproducibility\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model_s = copy.deepcopy(model)\n",
    "model_t = copy.deepcopy(model)\n",
    "module_list = nn.ModuleList([model_s, model_t])\n",
    "\n",
    "# Now, call the RL function with adjusted arguments\n",
    "scrub(data_loaders, module_list, criterion, args=args)\n",
    "\n",
    "# The unlearned model is now in 'model'\n",
    "unlearned_model = model\n",
    "\n",
    "# Evaluate the unlearned model on the test dataset\n",
    "unlearned_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        outputs = unlearned_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the unlearned model on the test data: {:.2f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
