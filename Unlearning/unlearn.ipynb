{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e8eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning\n",
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50571c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "import quadprog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360b5c4aaa0b84a",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545fa36453e0dda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T15:16:56.387836Z",
     "start_time": "2024-11-13T15:16:56.266657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# split the data into 10 classes by doing sort by key where in the keys are the labels and the values are the data\n",
    "train_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(train_data, train_labels):\n",
    "    train_split[CLASSES[label]].append(img)\n",
    "    \n",
    "# this makes more sense to me, effectively indexes 0-5000 are all airplanes, 5000-10000 are all automobiles etc\n",
    "test_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(test_data, test_labels):\n",
    "    test_split[CLASSES[label]].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849681c0f0bdd16",
   "metadata": {},
   "source": [
    "# PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b345dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "# make cuda available if available\n",
    "\n",
    "from cifar import load_cifar10_data, show_image\n",
    "\n",
    "# we want to create a resnet 18 model for 32x32 images\n",
    "# avoid upscaling, the model will take 32x32 images on input as opposed to 224x224\n",
    "\n",
    "initialisation = time.time()\n",
    "\n",
    "class ResNet18CIFAR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        # change the first layer to accept 32x32 images with 3 channels rather than 224x224 images\n",
    "        # check the size of the input layer\n",
    "        print(\"|| conv1 weight size: \", self.resnet.conv1.weight.size())\n",
    "        print(\"|| fc weight size: \", self.resnet.fc.weight.size())\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet.fc = torch.nn.Linear(512, 10)\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "        # change input layer to accept 32x32 images\n",
    "\n",
    "\n",
    "        \n",
    "        # List all layers in the resnet18 model\n",
    "        for name, layer in self.resnet.named_children():\n",
    "            print(f\"Layer: {name} -> {layer}\")\n",
    "        \n",
    "        #print(\"|| conv1 weight size: \", self.resnet.conv1.weight.size())\n",
    "        #print(\"|| fc weight size: \", self.resnet.fc.weight.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7e1518a20b7df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18CIFAR(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('models/resnet18_cifar77ACC.pth',  map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe76d5",
   "metadata": {},
   "source": [
    "# INITIAL MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9dd6044c0b8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||===================START TEST/TRAIN ACCURACY=================||\n",
      "|| Test Set Accuracy: 77.34%\n",
      "||==============================END==========================||\n",
      "|| Time to load data: 1731559331.98s\n",
      "|| Time to get accuracy: 57.80s\n",
      "|| Total time: 1731559389.78s\n",
      "||==============================END==========================||\n"
     ]
    }
   ],
   "source": [
    "# turn the data into a tensor\n",
    "test_data_tensor = torch.tensor(test_data).float()\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# keep track of accuracy\n",
    "correct1 = 0\n",
    "correct2 = 0\n",
    "total1 = len(train_data)\n",
    "total2 = len(test_data)\n",
    "\n",
    "# start test/train accuracy timer\n",
    "accuracy = time.time()\n",
    "\n",
    "print(\"||===================START TEST/TRAIN ACCURACY=================||\")\n",
    "\n",
    "# move tensors to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    test_data_tensor = test_data_tensor.cuda()\n",
    "    test_labels_tensor = test_labels_tensor.cuda()\n",
    "    \n",
    "            \n",
    "# test the model\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_data), 1000):\n",
    "        # get the input and output\n",
    "        img = test_data_tensor[i:i+1000]\n",
    "        label = test_labels_tensor[i:i+1000]\n",
    "        \n",
    "\n",
    "        # normalise the image\n",
    "        #img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "\n",
    "        # get the prediction\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # get the number of correct predictions\n",
    "        correct2 += (predicted == label).sum().item()\n",
    "        \n",
    "        del img\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"|| Test Set Accuracy: {correct2 / total2 * 100:.2f}%\")\n",
    "\n",
    "print(\"||==============================END==========================||\")\n",
    "\n",
    "# calculate the time taken to get the accuracy\n",
    "accuracy = time.time() - accuracy\n",
    "\n",
    "# print timing information\n",
    "print(f\"|| Time to load data: {initialisation:.2f}s\")\n",
    "print(f\"|| Time to get accuracy: {accuracy:.2f}s\")\n",
    "print(f\"|| Total time: {initialisation + accuracy:.2f}s\")\n",
    "\n",
    "print(\"||==============================END==========================||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55059d",
   "metadata": {},
   "source": [
    "# UNLEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c22b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning/Unlearning\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "umodel = copy.deepcopy(model)\n",
    "%cd Unlearning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c590ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn.scrub import train_distill, iterative_unlearn, scrub\n",
    "from utils import DistillKL, AverageMeter, accuracy\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, ConcatDataset, Subset\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "train_dataset = TensorDataset(torch.tensor(train_data), torch.tensor(train_labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_data), torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e88c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Learning rate: 0.1\n",
      "Epoch: [0][79/783]\tLoss 26.1102 (26.1102)\tAccuracy 9.375 (9.375)\tTime 5.30\n",
      "Epoch: [0][89/783]\tLoss 8.1634 (11.9757)\tAccuracy 20.312 (13.210)\tTime 39.00\n",
      "Epoch: [0][99/783]\tLoss 3.2917 (8.0729)\tAccuracy 23.438 (16.667)\tTime 29.50\n",
      "Epoch: [0][109/783]\tLoss 2.1213 (6.2596)\tAccuracy 29.688 (17.692)\tTime 23.70\n",
      "Epoch: [0][119/783]\tLoss 1.9278 (5.2624)\tAccuracy 23.438 (19.398)\tTime 21.10\n",
      "Epoch: [0][129/783]\tLoss 2.1709 (4.6324)\tAccuracy 23.438 (20.925)\tTime 53.20\n",
      "Epoch: [0][139/783]\tLoss 1.9407 (4.1934)\tAccuracy 25.000 (21.901)\tTime 64.20\n",
      "Epoch: [0][149/783]\tLoss 1.9613 (3.8677)\tAccuracy 28.125 (23.063)\tTime 42.50\n",
      "Epoch: [0][159/783]\tLoss 1.8104 (3.6219)\tAccuracy 25.000 (24.190)\tTime 125.60\n",
      "Epoch: [0][169/783]\tLoss 1.7255 (3.4248)\tAccuracy 39.062 (24.914)\tTime 72.89\n",
      "Epoch: [0][179/783]\tLoss 1.7884 (3.2651)\tAccuracy 34.375 (25.402)\tTime 71.50\n",
      "Epoch: [0][189/783]\tLoss 1.9527 (3.1358)\tAccuracy 28.125 (25.915)\tTime 43.70\n",
      "Epoch: [0][199/783]\tLoss 1.7274 (3.0324)\tAccuracy 29.688 (26.123)\tTime 20.60\n",
      "Epoch: [0][209/783]\tLoss 1.7599 (2.9379)\tAccuracy 37.500 (26.455)\tTime 23.90\n",
      "Epoch: [0][219/783]\tLoss 1.8644 (2.8576)\tAccuracy 29.688 (26.961)\tTime 26.90\n",
      "Epoch: [0][229/783]\tLoss 1.6647 (2.7852)\tAccuracy 28.125 (27.349)\tTime 29.80\n",
      "Epoch: [0][239/783]\tLoss 1.6699 (2.7189)\tAccuracy 42.188 (27.844)\tTime 22.00\n",
      "Epoch: [0][249/783]\tLoss 1.6498 (2.6588)\tAccuracy 35.938 (28.399)\tTime 20.10\n",
      "Epoch: [0][259/783]\tLoss 1.8627 (2.6097)\tAccuracy 29.688 (28.747)\tTime 97.30\n",
      "Epoch: [0][269/783]\tLoss 1.7463 (2.5636)\tAccuracy 43.750 (29.188)\tTime 486.00\n",
      "Epoch: [0][279/783]\tLoss 1.6499 (2.5206)\tAccuracy 34.375 (29.423)\tTime 25.70\n",
      "Epoch: [0][289/783]\tLoss 1.6423 (2.4787)\tAccuracy 45.312 (29.873)\tTime 22.40\n",
      "Epoch: [0][299/783]\tLoss 1.5018 (2.4414)\tAccuracy 46.875 (30.352)\tTime 19.70\n",
      "Epoch: [0][309/783]\tLoss 1.5405 (2.4091)\tAccuracy 37.500 (30.668)\tTime 20.70\n",
      "Epoch: [0][319/783]\tLoss 1.7521 (2.3808)\tAccuracy 37.500 (30.965)\tTime 21.10\n",
      "Epoch: [0][329/783]\tLoss 1.4617 (2.3538)\tAccuracy 43.750 (31.057)\tTime 21.20\n",
      "Epoch: [0][339/783]\tLoss 1.6384 (2.3263)\tAccuracy 43.750 (31.406)\tTime 23.40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from unlearn.RL import RL\n",
    "\n",
    "# Assume 'train_data' and 'train_labels' are your training data and labels\n",
    "# 'test_data' and 'test_labels' are your test data and labels\n",
    "# 'model' is your pretrained ResNet model\n",
    "\n",
    "# Define the class you want to unlearn\n",
    "class_to_unlearn = 0  # Change this to the class you want to unlearn\n",
    "\n",
    "\n",
    "# Convert data and labels to tensors if they aren't already\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# Create TensorDatasets directly for 'forget' and 'retain' subsets\n",
    "forget_indices = (train_labels_tensor == class_to_unlearn).nonzero(as_tuple=True)[0]\n",
    "retain_indices = (train_labels_tensor != class_to_unlearn).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Create 'forget' dataset\n",
    "forget_data = train_data_tensor[forget_indices]\n",
    "forget_labels = train_labels_tensor[forget_indices]\n",
    "forget_dataset = TensorDataset(forget_data, forget_labels)\n",
    "\n",
    "# Create 'retain' dataset\n",
    "retain_data = train_data_tensor[retain_indices]\n",
    "retain_labels = train_labels_tensor[retain_indices]\n",
    "retain_dataset = TensorDataset(retain_data, retain_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=64, shuffle=True)\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Prepare the data_loaders dictionary\n",
    "data_loaders = {'forget': forget_loader, 'retain': retain_loader}\n",
    "\n",
    "# Create the test DataLoader\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Move the model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.unlearn_lr = 0.1         # Learning rate for unlearning\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 5e-4\n",
    "        self.dataset = ''      # Change as needed\n",
    "        self.num_classes = 10         # Number of classes in the dataset\n",
    "        self.batch_size = 64\n",
    "        self.print_freq = 10\n",
    "        self.warmup = 0               # Number of warmup epochs\n",
    "        self.imagenet_arch = False    # Set to True if using ImageNet architecture\n",
    "        self.seed = 42                # For reproducibility\n",
    "\n",
    "\n",
    "        # Add the following attributes to ensure compatibility\n",
    "        self.decreasing_lr = '50,75'  # Comma-separated epochs where LR decays\n",
    "        self.rewind_epoch = 0         # Epoch to rewind to; set to 0 if not using rewinding\n",
    "        self.rewind_pth = ''          # Path to the rewind checkpoint\n",
    "        self.gpu = 0                  # GPU ID to use; adjust as needed\n",
    "        self.surgical = False         # Whether to use surgical unlearning\n",
    "        self.choice = []              # Layers to unlearn surgically; list of layer names\n",
    "        self.unlearn = 'retrain'      # Unlearning method, e.g., 'retrain'\n",
    "        self.unlearn_epochs = 10     # Number of epochs for unlearning\n",
    "        self.epochs = 100             # Total number of epochs (used for scheduler)    # For reproducibility\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Now, call the RL function with adjusted arguments\n",
    "RL(data_loaders, model, criterion, args=args)\n",
    "\n",
    "# The unlearned model is now in 'model'\n",
    "unlearned_model = model\n",
    "\n",
    "# Evaluate the unlearned model on the test dataset\n",
    "unlearned_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        outputs = unlearned_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the unlearned model on the test data: {:.2f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
