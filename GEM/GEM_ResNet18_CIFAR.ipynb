{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Episodic Memory for Continual Learning\n",
    "\n",
    "We adapt the GEM algorithm to train the ResNet18 model on the CIFAR-10 Dataset. The GEM algorithm is a continual learning algorithm that prevents catastrophic forgetting by constraining the gradient updates on the new task to be orthogonal to the gradients of the previous tasks. The algorithm is implemented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "import quadprog\n",
    "\n",
    "AGEM = True\n",
    "\n",
    "# Globals \n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraped code fundamental for GEM this is the code required to create a resnet18 model from scratch \n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(nclasses, nf=20):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now lets make the GEM model --  we will scrape this from the GEM code\n",
    "def compute_offsets(task, nc_per_task, is_cifar):\n",
    "    \"\"\"\n",
    "        Compute offsets for cifar to determine which\n",
    "        outputs to select for a given task.\n",
    "    \"\"\"\n",
    "    offset1 = task * nc_per_task\n",
    "    offset2 = (task + 1) * nc_per_task\n",
    "    \n",
    "    return offset1, offset2\n",
    "\n",
    "\n",
    "def store_grad(pp, grads, grad_dims, tid):\n",
    "    \"\"\"\n",
    "        This stores parameter gradients of past tasks.\n",
    "        pp: parameters\n",
    "        grads: gradients\n",
    "        grad_dims: list with number of parameters per layers\n",
    "        tid: task id\n",
    "    \"\"\"\n",
    "    # store the gradients\n",
    "    grads[:, tid].fill_(0.0)\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def overwrite_grad(pp, newgrad, grad_dims):\n",
    "    \"\"\"\n",
    "        This is used to overwrite the gradients with a new gradient\n",
    "        vector, whenever violations occur.\n",
    "        pp: parameters\n",
    "        newgrad: corrected gradient\n",
    "        grad_dims: list storing number of parameters at each layer\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            this_grad = newgrad[beg: en].contiguous().view(\n",
    "                param.grad.data.size())\n",
    "            param.grad.data.copy_(this_grad)\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\"\n",
    "        Solves the GEM dual QP described in the paper given a proposed\n",
    "        gradient \"gradient\", and a memory of task gradients \"memories\".\n",
    "        Overwrites \"gradient\" with the final projected update.\n",
    "\n",
    "        input:  gradient, p-vector\n",
    "        input:  memories, (t * p)-vector\n",
    "        output: x, p-vector\n",
    "    \"\"\"\n",
    "    memories_np = memories.cpu().t().double().numpy()\n",
    "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = memories_np.shape[0]\n",
    "    P = np.dot(memories_np, memories_np.transpose())\n",
    "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
    "    q = np.dot(memories_np, gradient_np) * -1\n",
    "    G = np.eye(t)\n",
    "    h = np.zeros(t) + margin\n",
    "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "    x = np.dot(v, memories_np) + gradient_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "def agemprojection(gradient, gradient_memory, margin=0.5):\n",
    "    \"\"\"\n",
    "    Projection of gradients for A-GEM with the memory approach\n",
    "    Use averaged gradient memory for projection\n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "    \n",
    "    gref  = gradient_memory.cpu().t().double().numpy()\n",
    "\n",
    "    g = gradient.cpu().contiguous().view(-1).double().numpy().mean(0)\n",
    "    g_transpose = g.transpose()\n",
    "    gref_transpose = gref.transpose()\n",
    "    # check g_transpose and gref are of the same shape\n",
    "    print(\"GREF SHAPE:\", gref.shape)\n",
    "    print(\"G SHAPE:\", g.shape)\n",
    "    dot_prod = np.dot(g_transpose, gref.squeeze())\n",
    "    print(\"DOT PROD 1 SHAPE:\",dot_prod.shape)\n",
    "    # dot product constraint has already been checked\n",
    "\n",
    "    # projection\n",
    "    dot_prod = dot_prod / np.dot(gref, gref_transpose)\n",
    "    print(\"DOT PROD 2 SHAPE:\", dot_prod.shape)\n",
    "    g = g - np.dot(dot_prod, gref)\n",
    "    print(\"G SHAPE:\",g.shape)\n",
    "    gradient.copy_(torch.Tensor(g).view(-1, 1))\n",
    "    \n",
    "    \"\"\"\n",
    "    gref = gradient_memory.t().double()\n",
    "    g = gradient.contiguous().view(-1).double()\n",
    "    g_transpose = g.view(1, -1)\n",
    "    gref_transpose = gref.view(1, -1)\n",
    "    dot_prod = torch.dot(g_transpose.squeeze(), gref.squeeze())\n",
    "    dot_prod = dot_prod / torch.dot(gref.squeeze(), gref_transpose.squeeze())\n",
    "    g = g - gref.squeeze() * dot_prod\n",
    "    gradient.copy_(torch.Tensor(g).view(-1, 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    GREF SHAPE: (1, 1094750)\n",
    "    G SHAPE: (1094750,)\n",
    "    DOT PROD 1 SHAPE: ()\n",
    "    DOT PROD 2 SHAPE: (1, 1)\n",
    "    G SHAPE: (1, 1094750)\n",
    "    \"\"\"\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_outputs,\n",
    "                 n_tasks,\n",
    "                 args):\n",
    "        super(Net, self).__init__()\n",
    "        nl, nh = args.n_layers, args.n_hiddens\n",
    "        self.margin = args.memory_strength\n",
    "        self.net = ResNet18(n_outputs)\n",
    "     \n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.opt = torch.optim.SGD(self.parameters(), args.lr)\n",
    "\n",
    "        self.n_memories = args.n_memories\n",
    "        self.gpu = args.cuda\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Allocate episodic memory\n",
    "        n_tasks: number of tasks\n",
    "        n_memories: number of memories per task\n",
    "        n_inputs: number of input features\n",
    "        \"\"\"\n",
    "\n",
    "        # allocate episodic memory\n",
    "        self.memory_data = torch.FloatTensor(\n",
    "            n_tasks, self.n_memories, n_inputs)\n",
    "        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)\n",
    "        if args.cuda:\n",
    "            self.memory_data = self.memory_data.cuda()\n",
    "            self.memory_labs = self.memory_labs.cuda()\n",
    "\n",
    "        # allocate temporary synaptic memory\n",
    "        \"\"\" This is the memory that stores the gradients of the parameters of the network\n",
    "            FOR each task. This is used to check for violations of the GEM constraint\n",
    "            Assume:\n",
    "\n",
    "            The model has 3 parameters with sizes 100, 200, and 300 elements respectively.\n",
    "            n_tasks = 5 (number of tasks).\n",
    "            The allocated tensors would have the following shapes:\n",
    "\n",
    "            self.grad_dims: [100, 200, 300]\n",
    "            self.grads: Shape [600, 5] (600 is the sum of 100, 200, and 300).\n",
    "        \"\"\"\n",
    "        self.grad_dims = []\n",
    "        for param in self.parameters():\n",
    "            self.grad_dims.append(param.data.numel())\n",
    "        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)\n",
    "        if args.cuda:\n",
    "            self.grads = self.grads.cuda()\n",
    "\n",
    "        # allocate counters\n",
    "        self.observed_tasks = []\n",
    "        self.old_task = -1\n",
    "        self.mem_cnt = 0\n",
    "        self.nc_per_task = int(n_outputs / n_tasks)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        output = self.net(x)\n",
    "        if t == -1:\n",
    "            return output\n",
    "        # make sure we predict classes within the current task\n",
    "        offset1 = int(t * self.nc_per_task)\n",
    "        offset2 = int((t + 1) * self.nc_per_task)\n",
    "        if offset1 > 0:\n",
    "            output[:, :offset1].data.fill_(-10e10)\n",
    "        if offset2 < self.n_outputs:\n",
    "            output[:, offset2:self.n_outputs].data.fill_(-10e10)\n",
    "        return output\n",
    "\n",
    "    def observe(self, x, t, y):\n",
    "        # update memory\n",
    "        if t != self.old_task:\n",
    "            self.observed_tasks.append(t)\n",
    "            self.old_task = t\n",
    "\n",
    "        # Update ring buffer storing examples from current task\n",
    "        bsz = y.data.size(0)\n",
    "        endcnt = min(self.mem_cnt + bsz, self.n_memories) #256\n",
    "        effbsz = endcnt - self.mem_cnt # 256\n",
    "        self.memory_data[t, self.mem_cnt: endcnt].copy_(\n",
    "            x.data[: effbsz])\n",
    "        if bsz == 1:\n",
    "            self.memory_labs[t, self.mem_cnt] = y.data[0]\n",
    "        else:\n",
    "            self.memory_labs[t, self.mem_cnt: endcnt].copy_(\n",
    "                y.data[: effbsz])\n",
    "        self.mem_cnt += effbsz\n",
    "        if self.mem_cnt == self.n_memories:\n",
    "            self.mem_cnt = 0\n",
    "\n",
    "        # compute gradient on previous tasks\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            for tt in range(len(self.observed_tasks) - 1):\n",
    "                self.zero_grad()\n",
    "                # fwd/bwd on the examples in the memory\n",
    "                past_task = self.observed_tasks[tt]\n",
    "\n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                   self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                           past_task)\n",
    "\n",
    "        # now compute the grad on the current minibatch\n",
    "        self.zero_grad()\n",
    "\n",
    "        offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar) \n",
    "        loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)\n",
    "        loss.backward()\n",
    "\n",
    "        # check if gradient violates constraints\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            if AGEM:\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    agemprojection(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "            # copy gradient\n",
    "            else:\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    project2cone2(self.grads[:, t].unsqueeze(1),\n",
    "                                self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "        self.opt.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define main function to run on the cifar dataset\n",
    "def run_cifar(args):\n",
    "    # Set up the model\n",
    "    n_tasks = 2 #[2 tasks [airplane, automobile, etc], [dog , frog, etc]]\n",
    "    size_of_task = 5\n",
    "    n_outputs = 10\n",
    "    n_inputs = 32 * 32 * 3\n",
    "    model = Net(n_inputs, n_outputs, n_tasks, args)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    model.is_cifar = True\n",
    "    test_bs = 1000\n",
    "    \n",
    "    test_accuracies = []\n",
    "\n",
    "    # Load data\n",
    "    train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "    task1data, task1labels = split_into_classes(train_data, train_labels, ['airplane', 'automobile', 'bird', 'cat', 'deer'])\n",
    "    task2data, task2labels = split_into_classes(train_data, train_labels, ['dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "    tasks = [[task1data, task1labels], [task2data,task2labels]]\n",
    "    test_data_per_class_1, test_labels_per_class_1 = split_into_classes(test_data, test_labels, ['airplane', 'automobile', 'bird', 'cat', 'deer'])\n",
    "    test_data_per_class_2, test_labels_per_class_2 = split_into_classes(test_data, test_labels, ['dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "    # Train the model\n",
    "    for task in range(n_tasks):\n",
    "        print(\"Training task: \", task  + 1)\n",
    "        # train_indexes = []\n",
    "        # for i in range(size_of_task):\n",
    "        #     train_indexes.append(class_indexes[i + task * size_of_task])\n",
    "        # random.shuffle(train_indexes)\n",
    "        # train_loader = DataLoader(train_data, batch_size=args.batch_size, sampler=torch.utils.data.sampler.SubsetRandomSampler(train_indexes))\n",
    "        \n",
    "        x = torch.Tensor(tasks[task][0].reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(tasks[task][1]).long()\n",
    "        \n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "        for epoch in range(args.n_epochs):\n",
    "            for j in range(0, len(tasks[task][0]), args.batch_size):\n",
    "                current_data = x[j: j + args.batch_size]\n",
    "                current_labels = y[j: j + args.batch_size]\n",
    "                model.train()\n",
    "                torch.cuda.synchronize()\n",
    "                model.observe(current_data, task, current_labels)\n",
    "                torch.cuda.synchronize()\n",
    "            print(\"Epoch: \", epoch)\n",
    "            \n",
    "            #test the model after each epoch\n",
    "            correct = 0\n",
    "            total = len(tasks[task][0])\n",
    "            for j in range(0,len(tasks[task][0]), test_bs):\n",
    "                current_data = x[j: j + test_bs]\n",
    "                current_labels = y[j: j + test_bs]\n",
    "                output = model.forward(current_data, task)\n",
    "                pred = output.data.max(1)[1]\n",
    "                correct += (pred == current_labels).sum().item()\n",
    "            print(\"Accuracy: \", correct / total)\n",
    "            if correct / total > 0.85:\n",
    "                break\n",
    "            #   output loss only\n",
    "\n",
    "        correct = 0\n",
    "        total = len(test_data_per_class_1)     \n",
    "\n",
    "        # Test the model\n",
    "        print(\"Testing task: 1\")\n",
    "        \n",
    "        x = torch.Tensor(test_data_per_class_1.reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(test_labels_per_class_1).long()\n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        for j in range(0,len(test_data_per_class_1), test_bs):\n",
    "            current_data = x[j: j + test_bs]\n",
    "            current_labels = y[j: j + test_bs]\n",
    "            output = model.forward(current_data, 0)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += (pred == current_labels).sum().item()\n",
    "        print(\"Accuracy: \", correct / total)\n",
    "        test_accuracies.append(correct / total)\n",
    "        \n",
    "        # test task 2\n",
    "        correct = 0\n",
    "        total = len(test_data_per_class_2)\n",
    "        \n",
    "        x = torch.Tensor(test_data_per_class_2.reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(test_labels_per_class_2).long()\n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        \n",
    "        print(\"Testing task: 2\")\n",
    "        for j in range(0,len(test_data_per_class_2), test_bs):\n",
    "            current_data = x[j: j + test_bs]\n",
    "            current_labels = y[j: j + test_bs]\n",
    "            output = model.forward(current_data, 1)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += (pred == current_labels).sum().item()\n",
    "        print(\"Accuracy: \", correct / total)\n",
    "        test_accuracies.append(correct / total)\n",
    "        \n",
    "    return model, test_accuracies\n",
    "\n",
    "#test = torch.Tensor([[1,2,3,4,5,6,7,8,9]] * 9)\n",
    "\n",
    "#test2 = torch.Tensor([[1,2,3,4,5,6,7,8,9]] * 9)\n",
    "#agemprojection(test, test2)\n",
    "#print(test)\n",
    "\n",
    "   \n",
    "model, test_accuracies = run_cifar(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bs = 1000\n",
    "\n",
    "# Load data\n",
    "#train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "#args = Args()\n",
    "# test the model on task 1 and task 2\n",
    "#correct = 0\n",
    "#\n",
    "#total = len(test_data)\n",
    "#\n",
    "#for j in range(0,len(test_data), test_bs):\n",
    "#    x = torch.Tensor(test_data.reshape(-1, 32*32*3)).float()\n",
    "#    y = torch.Tensor(test_labels).long()\n",
    "#    if args.cuda:\n",
    "#        x, y = x.cuda(), y.cuda()\n",
    "#    x = x[j: j + test_bs]\n",
    "#    y = y[j: j + test_bs]\n",
    "#    output = model.forward(x, -1)\n",
    "#    pred = output.data.max(1)[1]\n",
    "#    correct += (pred == y).sum().item()\n",
    "#print(\"Accuracy: \", correct / total)\n",
    "\n",
    "# plot task 1 and task 2 test accuracies on a bar chart\n",
    "plt.bar(['Task 1', 'Task 2'], [test_accuracies[0]*100, test_accuracies[1]*100])\n",
    "plt.title('Test Set Accuracy for Task 1 and Task 2 (Post Task 1 Training)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()\n",
    "\n",
    "# plot task 1 and task 2 test accuracies on a bar chart\n",
    "plt.bar(['Task 1', 'Task 2'], [test_accuracies[2]*100, test_accuracies[3]*100])\n",
    "plt.title('Test Set Accuracy for Task 1 and Task 2  (Post Task 2 Training)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
