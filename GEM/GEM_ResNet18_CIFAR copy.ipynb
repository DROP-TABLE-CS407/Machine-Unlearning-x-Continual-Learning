{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Episodic Memory for Continual Learning\n",
    "\n",
    "We adapt the GEM algorithm to train the ResNet18 model on the CIFAR-10 Dataset. The GEM algorithm is a continual learning algorithm that prevents catastrophic forgetting by constraining the gradient updates on the new task to be orthogonal to the gradients of the previous tasks. The algorithm is implemented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "import quadprog\n",
    "\n",
    "\n",
    "# Globals \n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraped code fundamental for GEM this is the code required to create a resnet18 model from scratch \n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(nclasses, nf=20):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now lets make the GEM model --  we will scrape this from the GEM code\n",
    "def compute_offsets(task, nc_per_task, is_cifar):\n",
    "    \"\"\"\n",
    "        Compute offsets for cifar to determine which\n",
    "        outputs to select for a given task.\n",
    "    \"\"\"\n",
    "    offset1 = task * nc_per_task\n",
    "    offset2 = (task + 1) * nc_per_task\n",
    "    \n",
    "    return offset1, offset2\n",
    "\n",
    "\n",
    "def store_grad(pp, grads, grad_dims, tid):\n",
    "    \"\"\"\n",
    "        This stores parameter gradients of past tasks.\n",
    "        pp: parameters\n",
    "        grads: gradients\n",
    "        grad_dims: list with number of parameters per layers\n",
    "        tid: task id\n",
    "    \"\"\"\n",
    "    # store the gradients\n",
    "    grads[:, tid].fill_(0.0)\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def overwrite_grad(pp, newgrad, grad_dims):\n",
    "    \"\"\"\n",
    "        This is used to overwrite the gradients with a new gradient\n",
    "        vector, whenever violations occur.\n",
    "        pp: parameters\n",
    "        newgrad: corrected gradient\n",
    "        grad_dims: list storing number of parameters at each layer\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            this_grad = newgrad[beg: en].contiguous().view(\n",
    "                param.grad.data.size())\n",
    "            param.grad.data.copy_(this_grad)\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\"\n",
    "        Solves the GEM dual QP described in the paper given a proposed\n",
    "        gradient \"gradient\", and a memory of task gradients \"memories\".\n",
    "        Overwrites \"gradient\" with the final projected update.\n",
    "\n",
    "        input:  gradient, p-vector\n",
    "        input:  memories, (t * p)-vector\n",
    "        output: x, p-vector\n",
    "    \"\"\"\n",
    "    memories_np = memories.cpu().t().double().numpy()\n",
    "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = memories_np.shape[0]\n",
    "    P = np.dot(memories_np, memories_np.transpose())\n",
    "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
    "    q = np.dot(memories_np, gradient_np) * -1\n",
    "    G = np.eye(t)\n",
    "    h = np.zeros(t) + margin\n",
    "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "    x = np.dot(v, memories_np) + gradient_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_outputs,\n",
    "                 n_tasks,\n",
    "                 args):\n",
    "        super(Net, self).__init__()\n",
    "        nl, nh = args.n_layers, args.n_hiddens\n",
    "        self.margin = args.memory_strength\n",
    "        self.net = ResNet18(n_outputs)\n",
    "     \n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.opt = torch.optim.SGD(self.parameters(), args.lr)\n",
    "\n",
    "        self.n_memories = args.n_memories\n",
    "        self.gpu = args.cuda\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Allocate episodic memory\n",
    "        n_tasks: number of tasks\n",
    "        n_memories: number of memories per task\n",
    "        n_inputs: number of input features\n",
    "        \"\"\"\n",
    "\n",
    "        # allocate episodic memory\n",
    "        self.memory_data = torch.FloatTensor(\n",
    "            n_tasks, self.n_memories, n_inputs)\n",
    "        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)\n",
    "        if args.cuda:\n",
    "            self.memory_data = self.memory_data.cuda()\n",
    "            self.memory_labs = self.memory_labs.cuda()\n",
    "\n",
    "        # allocate temporary synaptic memory\n",
    "        \"\"\" This is the memory that stores the gradients of the parameters of the network\n",
    "            FOR each task. This is used to check for violations of the GEM constraint\n",
    "            Assume:\n",
    "\n",
    "            The model has 3 parameters with sizes 100, 200, and 300 elements respectively.\n",
    "            n_tasks = 5 (number of tasks).\n",
    "            The allocated tensors would have the following shapes:\n",
    "\n",
    "            self.grad_dims: [100, 200, 300]\n",
    "            self.grads: Shape [600, 5] (600 is the sum of 100, 200, and 300).\n",
    "        \"\"\"\n",
    "        self.grad_dims = []\n",
    "        for param in self.parameters():\n",
    "            self.grad_dims.append(param.data.numel())\n",
    "        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)\n",
    "        if args.cuda:\n",
    "            self.grads = self.grads.cuda()\n",
    "\n",
    "        # allocate counters\n",
    "        self.observed_tasks = []\n",
    "        self.old_task = -1\n",
    "        self.mem_cnt = 0\n",
    "        self.nc_per_task = int(n_outputs / n_tasks)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        output = self.net(x)\n",
    "        if t == -1:\n",
    "            print(\"test all\")\n",
    "            return output\n",
    "        # make sure we predict classes within the current task\n",
    "        offset1 = int(t * self.nc_per_task)\n",
    "        offset2 = int((t + 1) * self.nc_per_task)\n",
    "        if offset1 > 0:\n",
    "            output[:, :offset1].data.fill_(-10e10)\n",
    "        if offset2 < self.n_outputs:\n",
    "            output[:, offset2:self.n_outputs].data.fill_(-10e10)\n",
    "        return output\n",
    "\n",
    "    def observe(self, x, t, y):\n",
    "        # update memory\n",
    "        if t != self.old_task:\n",
    "            self.observed_tasks.append(t)\n",
    "            self.old_task = t\n",
    "\n",
    "        # Update ring buffer storing examples from current task\n",
    "        bsz = y.data.size(0)\n",
    "        endcnt = min(self.mem_cnt + bsz, self.n_memories) #256\n",
    "        effbsz = endcnt - self.mem_cnt # 256\n",
    "        self.memory_data[t, self.mem_cnt: endcnt].copy_(\n",
    "            x.data[: effbsz])\n",
    "        if bsz == 1:\n",
    "            self.memory_labs[t, self.mem_cnt] = y.data[0]\n",
    "        else:\n",
    "            self.memory_labs[t, self.mem_cnt: endcnt].copy_(\n",
    "                y.data[: effbsz])\n",
    "        self.mem_cnt += effbsz\n",
    "        if self.mem_cnt == self.n_memories:\n",
    "            self.mem_cnt = 0\n",
    "\n",
    "        # compute gradient on previous tasks\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            for tt in range(len(self.observed_tasks) - 1):\n",
    "                self.zero_grad()\n",
    "                # fwd/bwd on the examples in the memory\n",
    "                past_task = self.observed_tasks[tt]\n",
    "\n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                   self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                           past_task)\n",
    "\n",
    "        # now compute the grad on the current minibatch\n",
    "        self.zero_grad()\n",
    "\n",
    "        offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar) \n",
    "        loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)\n",
    "        loss.backward()\n",
    "\n",
    "        # check if gradient violates constraints\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            # copy gradient\n",
    "            store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "            indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                else torch.LongTensor(self.observed_tasks[:-1])\n",
    "            dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                            self.grads.index_select(1, indx))\n",
    "            if (dotp < 0).sum() != 0:\n",
    "                project2cone2(self.grads[:, t].unsqueeze(1),\n",
    "                              self.grads.index_select(1, indx), self.margin)\n",
    "                # copy gradients back\n",
    "                overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                               self.grad_dims)\n",
    "        self.opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training task:  1\n",
      "Epoch:  0\n",
      "Accuracy:  0.65924\n",
      "Epoch:  1\n",
      "Accuracy:  0.75852\n",
      "Epoch:  2\n",
      "Accuracy:  0.81072\n",
      "Testing task: 1\n",
      "Accuracy:  0.7538\n",
      "Testing task: 2\n",
      "Accuracy:  0.2276\n",
      "Training task:  2\n",
      "Epoch:  0\n",
      "Accuracy:  0.88464\n",
      "Epoch:  1\n",
      "Accuracy:  0.92448\n",
      "Epoch:  2\n",
      "Accuracy:  0.94292\n",
      "Testing task: 1\n",
      "Accuracy:  0.6562\n",
      "Testing task: 2\n",
      "Accuracy:  0.8838\n"
     ]
    }
   ],
   "source": [
    "# define main function to run on the cifar dataset\n",
    "def run_cifar(args):\n",
    "    # Set up the model\n",
    "    n_tasks = 2 #[2 tasks [airplane, automobile, etc], [dog , frog, etc]]\n",
    "    size_of_task = 5\n",
    "    n_outputs = 10\n",
    "    n_inputs = 32 * 32 * 3\n",
    "    model = Net(n_inputs, n_outputs, n_tasks, args)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    model.is_cifar = True\n",
    "\n",
    "    # Load data\n",
    "    train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "    task1data, task1labels = split_into_classes(train_data, train_labels, ['airplane', 'automobile', 'bird', 'cat', 'deer'])\n",
    "    task2data, task2labels = split_into_classes(train_data, train_labels, ['dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "    tasks = [[task1data, task1labels], [task2data,task2labels]]\n",
    "    test_data_per_class_1, test_labels_per_class_1 = split_into_classes(test_data, test_labels, ['airplane', 'automobile', 'bird', 'cat', 'deer'])\n",
    "    test_data_per_class_2, test_labels_per_class_2 = split_into_classes(test_data, test_labels, ['dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "    # Train the model\n",
    "    for task in range(n_tasks):\n",
    "        print(\"Training task: \", task  + 1)\n",
    "        # train_indexes = []\n",
    "        # for i in range(size_of_task):\n",
    "        #     train_indexes.append(class_indexes[i + task * size_of_task])\n",
    "        # random.shuffle(train_indexes)\n",
    "        # train_loader = DataLoader(train_data, batch_size=args.batch_size, sampler=torch.utils.data.sampler.SubsetRandomSampler(train_indexes))\n",
    "        \n",
    "        x = torch.Tensor(tasks[task][0].reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(tasks[task][1]).long()\n",
    "    \n",
    "        for epoch in range(args.n_epochs):\n",
    "            for j in range(0, len(tasks[task][0]), args.batch_size):\n",
    "                x = torch.Tensor(tasks[task][0].reshape(-1, 32*32*3)).float()\n",
    "                y = torch.Tensor(tasks[task][1]).long()\n",
    "                if args.cuda:\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                x = x[j: j + args.batch_size]\n",
    "                y = y[j: j + args.batch_size]\n",
    "                model.train()\n",
    "                model.observe(x, task, y)\n",
    "            print(\"Epoch: \", epoch)\n",
    "            \n",
    "            #test the model after each epoch\n",
    "            correct = 0\n",
    "            total = len(tasks[task][0])\n",
    "            for j in range(0,len(tasks[task][0]), args.batch_size):\n",
    "                x = torch.Tensor(tasks[task][0].reshape(-1, 32*32*3)).float()\n",
    "                y = torch.Tensor(tasks[task][1]).long()\n",
    "                if args.cuda:\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                x = x[j: j + args.batch_size]\n",
    "                y = y[j: j + args.batch_size]\n",
    "                output = model.forward(x, task)\n",
    "                pred = output.data.max(1)[1]\n",
    "                correct += (pred == y).sum().item()\n",
    "            print(\"Accuracy: \", correct / total)\n",
    "            #   output loss only\n",
    "\n",
    "        correct = 0\n",
    "        total = len(test_data_per_class_1)     \n",
    "\n",
    "        # Test the model\n",
    "        print(\"Testing task: 1\")\n",
    "        for j in range(0,len(test_data_per_class_1), args.batch_size):\n",
    "            x = torch.Tensor(test_data_per_class_1.reshape(-1, 32*32*3)).float()\n",
    "            y = torch.Tensor(test_labels_per_class_1).long()\n",
    "            if args.cuda:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            x = x[j: j + args.batch_size]\n",
    "            y = y[j: j + args.batch_size]\n",
    "            output = model.forward(x, 0)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += (pred == y).sum().item()\n",
    "        print(\"Accuracy: \", correct / total)\n",
    "        \n",
    "        # test task 2\n",
    "        correct = 0\n",
    "        total = len(test_data_per_class_2)\n",
    "        \n",
    "        print(\"Testing task: 2\")\n",
    "        for j in range(0,len(test_data_per_class_2), args.batch_size):\n",
    "            x = torch.Tensor(test_data_per_class_2.reshape(-1, 32*32*3)).float()\n",
    "            y = torch.Tensor(test_labels_per_class_2).long()\n",
    "            if args.cuda:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            x = x[j: j + args.batch_size]\n",
    "            y = y[j: j + args.batch_size]\n",
    "            output = model.forward(x, 1)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += (pred == y).sum().item()\n",
    "        print(\"Accuracy: \", correct / total)\n",
    "        \n",
    "    return model\n",
    "\n",
    "        \n",
    "model = run_cifar(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "test all\n",
      "Accuracy:  0.5579\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "args = Args()\n",
    "# test the model on task 1 and task 2\n",
    "correct = 0\n",
    "\n",
    "total = len(test_data)\n",
    "\n",
    "for j in range(0,len(test_data), args.batch_size):\n",
    "    x = torch.Tensor(test_data.reshape(-1, 32*32*3)).float()\n",
    "    y = torch.Tensor(test_labels).long()\n",
    "    if args.cuda:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "    x = x[j: j + args.batch_size]\n",
    "    y = y[j: j + args.batch_size]\n",
    "    output = model.forward(x, -1)\n",
    "    pred = output.data.max(1)[1]\n",
    "    correct += (pred == y).sum().item()\n",
    "print(\"Accuracy: \", correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
