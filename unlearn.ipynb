{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e8eb68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "\n",
    "# import quadprog\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360b5c4aaa0b84a",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fa36453e0dda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T15:16:56.387836Z",
     "start_time": "2024-11-13T15:16:56.266657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# split the data into 10 classes by doing sort by key where in the keys are the labels and the values are the data\n",
    "train_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(train_data, train_labels):\n",
    "    train_split[CLASSES[label]].append(img)\n",
    "    \n",
    "# this makes more sense to me, effectively indexes 0-5000 are all airplanes, 5000-10000 are all automobiles etc\n",
    "test_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(test_data, test_labels):\n",
    "    test_split[CLASSES[label]].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849681c0f0bdd16",
   "metadata": {},
   "source": [
    "# PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "# make cuda available if available\n",
    "\n",
    "from cifar import load_cifar10_data, show_image\n",
    "\n",
    "# we want to create a resnet 18 model for 32x32 images\n",
    "# avoid upscaling, the model will take 32x32 images on input as opposed to 224x224\n",
    "\n",
    "initialisation = time.time()\n",
    "\n",
    "class ResNet18CIFAR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        # change the first layer to accept 32x32 images with 3 channels rather than 224x224 images\n",
    "        # check the size of the input layer\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.resnet.bn1 = torch.nn.BatchNorm2d(128)\n",
    "        # change number of blocks per layer\n",
    "        self.resnet.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.resnet.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.resnet.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.resnet.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024)\n",
    "        )\n",
    "        # change input layer to accept 32x32 images by utilising smaller convolutional kernel\n",
    "        self.resnet.fc = torch.nn.Linear(1024, 10)\n",
    "        # start with 5 classes and add more as needed\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "        # maxpool worsens performance and is unnecessary for small image sizes\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "model = ResNet18CIFAR()\n",
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9dd6044c0b8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the data into a tensor\n",
    "test_data_tensor = torch.tensor(test_data).float()\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# Define the number of classes in your dataset\n",
    "num_classes = 10\n",
    "# Initialize counters for each class\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "print(\"||===================START CLASS-BY-CLASS ACCURACY=================||\")\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    test_data_tensor = test_data_tensor.cuda()\n",
    "    test_labels_tensor = test_labels_tensor.cuda()\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_data), 1000):\n",
    "        # Get the input and output\n",
    "        img = test_data_tensor[i:i + 1000]\n",
    "        label = test_labels_tensor[i:i + 1000]\n",
    "        \n",
    "        model = model.cuda()\n",
    "\n",
    "        # Get the prediction\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update per-class counters\n",
    "        for lbl, pred in zip(label, predicted):\n",
    "            class_total[lbl.item()] += 1\n",
    "            if lbl.item() == pred.item():\n",
    "                class_correct[lbl.item()] += 1\n",
    "\n",
    "        del img\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_accuracy = sum(class_correct) / sum(class_total) * 100\n",
    "print(f\"|| Overall Test Accuracy: {overall_accuracy:.2f}% ||\")\n",
    "\n",
    "# Print accuracy for each class\n",
    "for cls in range(num_classes):\n",
    "    if class_total[cls] > 0:\n",
    "        class_accuracy = class_correct[cls] / class_total[cls] * 100\n",
    "        print(f\"|| Accuracy for Class {cls}: {class_accuracy:.2f}% ||\")\n",
    "    else:\n",
    "        print(f\"|| No samples for Class {cls} ||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55059d",
   "metadata": {},
   "source": [
    "# UNLEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c22b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /dcs/large/u2145461/cs407/Machine-Unlearning-x-Continual-Learning/Unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8076b",
   "metadata": {},
   "source": [
    "### Prepare Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to be Unlearnt\n",
    "classes_to_unlearn = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c590ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from unlearn.scrub import train_distill, iterative_unlearn, scrub\n",
    "from utils import DistillKL, AverageMeter, accuracy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert data and labels to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32).cuda()\n",
    "train_labels_tensor = torch.tensor(train_labels).cuda()\n",
    "\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32).cuda()\n",
    "test_labels_tensor = torch.tensor(test_labels).cuda()\n",
    "\n",
    "# Split training data into forget and retain sets\n",
    "forget_mask = torch.isin(train_labels_tensor, torch.tensor(classes_to_unlearn).cuda())\n",
    "retain_mask = ~forget_mask\n",
    "\n",
    "# Get the indices of the forget and retain subsets\n",
    "forget_indices = forget_mask.nonzero(as_tuple=True)[0]\n",
    "retain_indices = retain_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Create the forget and retain datasets using the indices\n",
    "forget_dataset = TensorDataset(train_data_tensor[forget_indices], train_labels_tensor[forget_indices])\n",
    "retain_dataset = TensorDataset(train_data_tensor[retain_indices], train_labels_tensor[retain_indices])\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "data_loaders = OrderedDict(\n",
    "    forget = DataLoader(forget_dataset, batch_size=64, shuffle=True),\n",
    "    retain = DataLoader(retain_dataset, batch_size=64, shuffle=True),\n",
    "    test =  DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1726b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to device\n",
    "import copy\n",
    "umodel = copy.deepcopy(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "umodel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be992da",
   "metadata": {},
   "source": [
    "### DEFINE ALGORITHM HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e88c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.unlearn_lr = 0.0001         # Learning rate for unlearning\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 0.0005\n",
    "        self.dataset = ''      # Change as needed\n",
    "        self.num_classes = 10         # Number of classes in the dataset\n",
    "        self.batch_size = 64\n",
    "        self.print_freq = 10\n",
    "        self.warmup = 0               # Number of warmup epochs\n",
    "        self.imagenet_arch = False    # Set to True if using ImageNet architecture\n",
    "        self.seed = 42       \n",
    "        \n",
    "        # SCRUB SPECIFIC\n",
    "        self.kd_T = 1\n",
    "        self.msteps = 1\n",
    "        self.gamma = 10\n",
    "        self.beta = 1\n",
    "\n",
    "\n",
    "        # Add the following attributes to ensure compatibility\n",
    "        self.decreasing_lr = '50,75'  # Comma-separated epochs where LR decays\n",
    "        self.rewind_epoch = 0         # Epoch to rewind to; set to 0 if not using rewinding\n",
    "        self.rewind_pth = ''          # Path to the rewind checkpoint\n",
    "        self.gpu = 0                  # GPU ID to use; adjust as needed\n",
    "        self.surgical = False         # Whether to use surgical unlearning\n",
    "        self.unlearn = 'SCRUB'      # Unlearning method, e.g., 'retrain'\n",
    "        self.choice = []              # Layers to unlearn surgically; list of layer names\n",
    "        self.unlearn_epochs = 20     # Number of epochs for unlearning\n",
    "        self.epochs = 100   \n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79564f8",
   "metadata": {},
   "source": [
    "### Apply Unlearning Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08fd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unlearn\n",
    "unlearn_method = unlearn.get_unlearn_method(args.unlearn)\n",
    "if args.unlearn == 'SCRUB':\n",
    "    model_s = copy.deepcopy(umodel)\n",
    "    model_t = copy.deepcopy(umodel)\n",
    "    module_list = nn.ModuleList([model_s, model_t])\n",
    "    unlearn_method(data_loaders, module_list, criterion, args)\n",
    "    umodel = module_list[0]\n",
    "else:\n",
    "    unlearn_method(data_loaders, umodel, criterion, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431fe87",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for each class\n",
    "from Unlearning import utils\n",
    "from Unlearning.trainer.val import validate\n",
    "\n",
    "\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "# Evaluate the unlearned model\n",
    "umodel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in data_loaders[\"test\"]:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        outputs = umodel(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update total and correct counts for each class\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            class_total[label.item()] += 1\n",
    "            if label.item() == prediction.item():\n",
    "                class_correct[label.item()] += 1\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_accuracy = 100 * sum(class_correct) / sum(class_total)\n",
    "print('Overall accuracy of the unlearned model on the test data: {:.2f}%'.format(overall_accuracy))\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        class_accuracy = 100 * class_correct[i] / class_total[i]\n",
    "        print('Accuracy for class {}: {:.2f}%'.format(i, class_accuracy))\n",
    "    else:\n",
    "        print('No samples for class {}'.format(i))\n",
    "\n",
    "\n",
    "for name, loader in data_loaders.items():\n",
    "        utils.dataset_convert_to_test(loader.dataset, args)\n",
    "        val_acc = validate(loader, umodel, criterion, args)\n",
    "        print(f\"{name} acc: {val_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
