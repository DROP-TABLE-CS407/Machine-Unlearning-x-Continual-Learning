{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e8eb68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "\n",
    "# import quadprog\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360b5c4aaa0b84a",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545fa36453e0dda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T15:16:56.387836Z",
     "start_time": "2024-11-13T15:16:56.266657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# split the data into 10 classes by doing sort by key where in the keys are the labels and the values are the data\n",
    "train_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(train_data, train_labels):\n",
    "    train_split[CLASSES[label]].append(img)\n",
    "    \n",
    "# this makes more sense to me, effectively indexes 0-5000 are all airplanes, 5000-10000 are all automobiles etc\n",
    "test_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(test_data, test_labels):\n",
    "    test_split[CLASSES[label]].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849681c0f0bdd16",
   "metadata": {},
   "source": [
    "# PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b345dbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Tom/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\Tom\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tom\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of ResNet18CIFAR(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "# make cuda available if available\n",
    "\n",
    "from cifar import load_cifar10_data, show_image\n",
    "\n",
    "# we want to create a resnet 18 model for 32x32 images\n",
    "# avoid upscaling, the model will take 32x32 images on input as opposed to 224x224\n",
    "\n",
    "initialisation = time.time()\n",
    "\n",
    "class ResNet18CIFAR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        # change the first layer to accept 32x32 images with 3 channels rather than 224x224 images\n",
    "        # check the size of the input layer\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.resnet.bn1 = torch.nn.BatchNorm2d(128)\n",
    "        # change number of blocks per layer\n",
    "        self.resnet.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.resnet.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.resnet.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.resnet.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024)\n",
    "        )\n",
    "        # change input layer to accept 32x32 images by utilising smaller convolutional kernel\n",
    "        self.resnet.fc = torch.nn.Linear(1024, 10)\n",
    "        # start with 5 classes and add more as needed\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "        # maxpool worsens performance and is unnecessary for small image sizes\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "model = ResNet18CIFAR()\n",
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9dd6044c0b8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||===================START CLASS-BY-CLASS ACCURACY=================||\n",
      "|| Overall Test Accuracy: 10.77% ||\n",
      "|| Accuracy for Class 0: 11.80% ||\n",
      "|| Accuracy for Class 1: 8.30% ||\n",
      "|| Accuracy for Class 2: 9.10% ||\n",
      "|| Accuracy for Class 3: 8.80% ||\n",
      "|| Accuracy for Class 4: 6.30% ||\n",
      "|| Accuracy for Class 5: 7.30% ||\n",
      "|| Accuracy for Class 6: 10.80% ||\n",
      "|| Accuracy for Class 7: 16.30% ||\n",
      "|| Accuracy for Class 8: 22.70% ||\n",
      "|| Accuracy for Class 9: 6.30% ||\n"
     ]
    }
   ],
   "source": [
    "# turn the data into a tensor\n",
    "test_data_tensor = torch.tensor(test_data).float()\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# Define the number of classes in your dataset\n",
    "num_classes = 10\n",
    "# Initialize counters for each class\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "print(\"||===================START CLASS-BY-CLASS ACCURACY=================||\")\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    test_data_tensor = test_data_tensor.cuda()\n",
    "    test_labels_tensor = test_labels_tensor.cuda()\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_data), 1000):\n",
    "        # Get the input and output\n",
    "        img = test_data_tensor[i:i + 1000]\n",
    "        label = test_labels_tensor[i:i + 1000]\n",
    "        \n",
    "        model = model.cuda()\n",
    "\n",
    "        # Get the prediction\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update per-class counters\n",
    "        for lbl, pred in zip(label, predicted):\n",
    "            class_total[lbl.item()] += 1\n",
    "            if lbl.item() == pred.item():\n",
    "                class_correct[lbl.item()] += 1\n",
    "\n",
    "        del img\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_accuracy = sum(class_correct) / sum(class_total) * 100\n",
    "print(f\"|| Overall Test Accuracy: {overall_accuracy:.2f}% ||\")\n",
    "\n",
    "# Print accuracy for each class\n",
    "for cls in range(num_classes):\n",
    "    if class_total[cls] > 0:\n",
    "        class_accuracy = class_correct[cls] / class_total[cls] * 100\n",
    "        print(f\"|| Accuracy for Class {cls}: {class_accuracy:.2f}% ||\")\n",
    "    else:\n",
    "        print(f\"|| No samples for Class {cls} ||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55059d",
   "metadata": {},
   "source": [
    "# UNLEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c22b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /dcs/large/u2145461/cs407/Machine-Unlearning-x-Continual-Learning/Unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8076b",
   "metadata": {},
   "source": [
    "### Prepare Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to be Unlearnt\n",
    "classes_to_unlearn = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c590ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from unlearn.scrub import train_distill, iterative_unlearn, scrub\n",
    "from utils import DistillKL, AverageMeter, accuracy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert data and labels to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32).cuda()\n",
    "train_labels_tensor = torch.tensor(train_labels).cuda()\n",
    "\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32).cuda()\n",
    "test_labels_tensor = torch.tensor(test_labels).cuda()\n",
    "\n",
    "# Split training data into forget and retain sets\n",
    "forget_mask = torch.isin(train_labels_tensor, torch.tensor(classes_to_unlearn).cuda())\n",
    "retain_mask = ~forget_mask\n",
    "\n",
    "# Get the indices of the forget and retain subsets\n",
    "forget_indices = forget_mask.nonzero(as_tuple=True)[0]\n",
    "retain_indices = retain_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Create the forget and retain datasets using the indices\n",
    "forget_dataset = TensorDataset(train_data_tensor[forget_indices], train_labels_tensor[forget_indices])\n",
    "retain_dataset = TensorDataset(train_data_tensor[retain_indices], train_labels_tensor[retain_indices])\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "data_loaders = OrderedDict(\n",
    "    forget = DataLoader(forget_dataset, batch_size=64, shuffle=True),\n",
    "    retain = DataLoader(retain_dataset, batch_size=64, shuffle=True),\n",
    "    test =  DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1726b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to device\n",
    "import copy\n",
    "umodel = copy.deepcopy(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "umodel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be992da",
   "metadata": {},
   "source": [
    "### DEFINE ALGORITHM HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e88c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.unlearn_lr = 0.0001         # Learning rate for unlearning\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 0.0005\n",
    "        self.dataset = ''      # Change as needed\n",
    "        self.num_classes = 10         # Number of classes in the dataset\n",
    "        self.batch_size = 64\n",
    "        self.print_freq = 10\n",
    "        self.warmup = 0               # Number of warmup epochs\n",
    "        self.imagenet_arch = False    # Set to True if using ImageNet architecture\n",
    "        self.seed = 42       \n",
    "        \n",
    "        # SCRUB SPECIFIC\n",
    "        self.kd_T = 1\n",
    "        self.msteps = 1\n",
    "        self.gamma = 10\n",
    "        self.beta = 1\n",
    "\n",
    "\n",
    "        # Add the following attributes to ensure compatibility\n",
    "        self.decreasing_lr = '50,75'  # Comma-separated epochs where LR decays\n",
    "        self.rewind_epoch = 0         # Epoch to rewind to; set to 0 if not using rewinding\n",
    "        self.rewind_pth = ''          # Path to the rewind checkpoint\n",
    "        self.gpu = 0                  # GPU ID to use; adjust as needed\n",
    "        self.surgical = False         # Whether to use surgical unlearning\n",
    "        self.unlearn = 'SCRUB'      # Unlearning method, e.g., 'retrain'\n",
    "        self.choice = []              # Layers to unlearn surgically; list of layer names\n",
    "        self.unlearn_epochs = 20     # Number of epochs for unlearning\n",
    "        self.epochs = 100   \n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79564f8",
   "metadata": {},
   "source": [
    "### Apply Unlearning Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08fd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unlearn\n",
    "unlearn_method = unlearn.get_unlearn_method(args.unlearn)\n",
    "if args.unlearn == 'SCRUB':\n",
    "    model_s = copy.deepcopy(umodel)\n",
    "    model_t = copy.deepcopy(umodel)\n",
    "    module_list = nn.ModuleList([model_s, model_t])\n",
    "    unlearn_method(data_loaders, module_list, criterion, args)\n",
    "    umodel = module_list[0]\n",
    "else:\n",
    "    unlearn_method(data_loaders, umodel, criterion, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431fe87",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for each class\n",
    "from Unlearning import utils\n",
    "from Unlearning.trainer.val import validate\n",
    "\n",
    "\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "# Evaluate the unlearned model\n",
    "umodel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in data_loaders[\"test\"]:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        outputs = umodel(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update total and correct counts for each class\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            class_total[label.item()] += 1\n",
    "            if label.item() == prediction.item():\n",
    "                class_correct[label.item()] += 1\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_accuracy = 100 * sum(class_correct) / sum(class_total)\n",
    "print('Overall accuracy of the unlearned model on the test data: {:.2f}%'.format(overall_accuracy))\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        class_accuracy = 100 * class_correct[i] / class_total[i]\n",
    "        print('Accuracy for class {}: {:.2f}%'.format(i, class_accuracy))\n",
    "    else:\n",
    "        print('No samples for class {}'.format(i))\n",
    "\n",
    "\n",
    "for name, loader in data_loaders.items():\n",
    "        utils.dataset_convert_to_test(loader.dataset, args)\n",
    "        val_acc = validate(loader, umodel, criterion, args)\n",
    "        print(f\"{name} acc: {val_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
