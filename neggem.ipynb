{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Idea of this notebook is to implement neggrad with the constraints of GEM. We want to be able to give a series of commands i.e. learn 1, learn 2 , unlearn 1 and it does the learning and unlearning respectively.\n",
    "\n",
    "The only change we would need to do to implement neggrad consistency. This is easy, for the given tasks that are learnt, X and the task number y that we wish to unlearn, we ensure the constraint in GEM with task y is <= 0 while the rest are >= 0 as the same as it would usually be. \n",
    "\n",
    "The hard part of this is ensuring consistency w.r.t the storing of gradients and memory buffers.\n",
    "\n",
    "If we can do this we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Setting Up the Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set directory /dcs/large/u2145461/cs407/Machine-Unlearning-x-Continual-Learning\n",
    "# please change these dependent on your own specific path variable\n",
    "\n",
    "os.chdir('/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning')\n",
    "\n",
    "save_path_1 = '/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning/GEM/Results4/'\n",
    "\n",
    "save_path_2 = '/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning/GEM/Results/'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes, load_data\n",
    "import cifar\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "import quadprog\n",
    "\n",
    "# we provide some top level initial parameters depending on if we want to work in cifar-10 or cifar-100\n",
    "\n",
    "AGEM = True\n",
    "PRETRAIN = 0 # number of initial classes to pretrain on\n",
    "# Globals \n",
    "DATASET = 'cifar-100'\n",
    "DATASET_PATH = 'cifar-100-python' \n",
    "CLASSES = cifar.CLASSES\n",
    "SHUFFLEDCLASSES = CLASSES.copy()\n",
    "CONFIDENCE_SAMPLES = 5\n",
    "if DATASET == 'cifar-10':\n",
    "    CLASSES = cifar.CLASSES\n",
    "    CLASSES = CLASSES.copy()\n",
    "elif DATASET == 'cifar-100':\n",
    "    CLASSES = cifar.CLASSES_100_UNORDERED\n",
    "    SHUFFLEDCLASSES = CLASSES.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "# C\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(nclasses, nf=20):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The brains - here we define the memory facilities and the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offsets(task, nc_per_task, is_cifar):\n",
    "    \"\"\"\n",
    "        Compute offsets for cifar to determine which\n",
    "        outputs to select for a given task.\n",
    "    \"\"\"\n",
    "    val1 = max(PRETRAIN - nc_per_task, 0)\n",
    "    val2 = max(PRETRAIN - nc_per_task, 0)\n",
    "    if task == 0:\n",
    "        val1 = 0\n",
    "        val2 = max(PRETRAIN - nc_per_task, 0)\n",
    "    offset1 = task * nc_per_task + val1\n",
    "    offset2 = (task + 1) * nc_per_task + val2    \n",
    "    return offset1, offset2\n",
    "\n",
    "\n",
    "def store_grad(pp, grads, grad_dims, tid):\n",
    "    \"\"\"\n",
    "        This stores parameter gradients of past tasks.\n",
    "        pp: parameters\n",
    "        grads: gradients\n",
    "        grad_dims: list with number of parameters per layers\n",
    "        tid: task id\n",
    "    \"\"\"\n",
    "    # store the gradients\n",
    "    grads[:, tid].fill_(0.0)\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def overwrite_grad(pp, newgrad, grad_dims):\n",
    "    \"\"\"\n",
    "        This is used to overwrite the gradients with a new gradient\n",
    "        vector, whenever violations occur.\n",
    "        pp: parameters\n",
    "        newgrad: corrected gradient\n",
    "        grad_dims: list storing number of parameters at each layer\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            this_grad = newgrad[beg: en].contiguous().view(\n",
    "                param.grad.data.size())\n",
    "            param.grad.data.copy_(this_grad)\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\"\n",
    "        Solves the GEM dual QP described in the paper given a proposed\n",
    "        gradient \"gradient\", and a memory of task gradients \"memories\".\n",
    "        Overwrites \"gradient\" with the final projected update.\n",
    "\n",
    "        input:  gradient, p-vector\n",
    "        input:  memories, (t * p)-vector\n",
    "        output: x, p-vector\n",
    "    \"\"\"\n",
    "    memories_np = memories.cpu().t().double().numpy()\n",
    "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = memories_np.shape[0]\n",
    "    P = np.dot(memories_np, memories_np.transpose())\n",
    "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
    "    q = np.dot(memories_np, gradient_np) * -1\n",
    "    G = np.eye(t)\n",
    "    h = np.zeros(t) + margin\n",
    "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "    x = np.dot(v, memories_np) + gradient_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "def agemprojection(gradient, gradient_memory, margin=0.5, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Projection of gradients for A-GEM with the memory approach\n",
    "    Use averaged gradient memory for projection\n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "\n",
    "    gref = gradient_memory.t().double().mean(axis=0).cuda() # * margin\n",
    "    g = gradient.contiguous().view(-1).double().cuda()\n",
    "\n",
    "    dot_prod = torch.dot(g, gref)\n",
    "    \n",
    "    #if dot_prod < 0:\n",
    "    #    x = g\n",
    "    #    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    #    return\n",
    "    \n",
    "    # avoid division by zero\n",
    "    dot_prod = dot_prod/(torch.dot(gref, gref))\n",
    "    \n",
    "    # epsvector = torch.Tensor([eps]).cuda()\n",
    "    \n",
    "    x = g*0.5 + gref * abs(dot_prod)  # + epsvector\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    \n",
    "def replay(gradient, gradient_memory):\n",
    "    \"\"\"\n",
    "    Adds the gradients of the current task to the memory \n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "    g = gradient_memory.t().double().sum(axis=0).cuda()\n",
    "    gref = gradient.contiguous().view(-1).double().cuda()\n",
    "    # simply add the gradients\n",
    "    x = g + gref\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    \n",
    "def naiveretraining(gradient):\n",
    "    \"\"\"\n",
    "    Naive retraining of the model on the current task\n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "    g = gradient.t().double().mean(axis=0).cuda()\n",
    "    gradient.copy_(torch.Tensor(g).view(-1, 1))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_outputs,\n",
    "                 n_tasks,\n",
    "                 args):\n",
    "        super(Net, self).__init__()\n",
    "        nl, nh = args.n_layers, args.n_hiddens\n",
    "        self.margin = args.memory_strength\n",
    "        self.net = ResNet18(n_outputs)\n",
    "\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.opt = torch.optim.SGD(self.parameters(), args.lr)\n",
    "\n",
    "        self.n_memories = args.n_memories\n",
    "        self.gpu = args.cuda\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Allocate episodic memory\n",
    "        n_tasks: number of tasks\n",
    "        n_memories: number of memories per task\n",
    "        n_inputs: number of input features\n",
    "        \"\"\"\n",
    "\n",
    "        # allocate episodic memory\n",
    "        self.memory_data = torch.FloatTensor(\n",
    "            n_tasks, self.n_memories, n_inputs)\n",
    "        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)\n",
    "        if args.cuda:\n",
    "            self.memory_data = self.memory_data.cuda()\n",
    "            self.memory_labs = self.memory_labs.cuda()\n",
    "\n",
    "        # allocate temporary synaptic memory\n",
    "        \"\"\" This is the memory that stores the gradients of the parameters of the network\n",
    "            FOR each task. This is used to check for violations of the GEM constraint\n",
    "            Assume:\n",
    "\n",
    "            The model has 3 parameters with sizes 100, 200, and 300 elements respectively.\n",
    "            n_tasks = 5 (number of tasks).\n",
    "            The allocated tensors would have the following shapes:\n",
    "\n",
    "            self.grad_dims: [100, 200, 300]\n",
    "            self.grads: Shape [600, 5] (600 is the sum of 100, 200, and 300).\n",
    "        \"\"\"\n",
    "        self.grad_dims = []\n",
    "        for param in self.parameters():\n",
    "            self.grad_dims.append(param.data.numel())\n",
    "        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)\n",
    "        if args.cuda:\n",
    "            self.grads = self.grads.cuda()\n",
    "\n",
    "        # allocate counters\n",
    "        self.observed_tasks = []\n",
    "        self.old_task = -1\n",
    "        self.mem_cnt = 0\n",
    "        minus = 0\n",
    "        if PRETRAIN > 0:\n",
    "            minus = 1\n",
    "        else: \n",
    "            minus = 0\n",
    "        self.nc_per_task = int((n_outputs - PRETRAIN) / (n_tasks - minus))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        output = self.net(x)\n",
    "        if t == -1:\n",
    "            return output\n",
    "        # make sure we predict classes within the current task\n",
    "        val1 = 0\n",
    "        val2 = 0\n",
    "        if t != 0:\n",
    "            val1 = max(PRETRAIN - self.nc_per_task, 0)\n",
    "            val2 = val1\n",
    "        else:\n",
    "            val1 = 0\n",
    "            val2 = max(PRETRAIN - self.nc_per_task, 0)                                                 \n",
    "        offset1 = int(t * self.nc_per_task + val1) #t = 0 0, 5 -----t = 1 5 , 6 ## t = 0 0 ,5 --- t =1 5, 7\n",
    "        offset2 = int((t + 1) * self.nc_per_task + val2) \n",
    "        if offset1 > 0:\n",
    "            output[:, :offset1].data.fill_(-10e10)\n",
    "        if offset2 < self.n_outputs:\n",
    "            output[:, offset2:self.n_outputs].data.fill_(-10e10)\n",
    "        return output\n",
    "\n",
    "    def observe(self, algorithm, x, t, y):\n",
    "        # update memory\n",
    "        if t != self.old_task or t not in self.observed_tasks:\n",
    "            self.observed_tasks.append(t)\n",
    "            self.old_task = t\n",
    "            \n",
    "        val = 0\n",
    "        if t == 0:\n",
    "            val = max(PRETRAIN,1)\n",
    "        else:\n",
    "            val = 1\n",
    "        # Update ring buffer storing examples from current task\n",
    "        bsz = y.data.size(0)\n",
    "        if (algorithm == 'NAIVE'):\n",
    "            self.zero_grad()\n",
    "            loss = self.ce(self.forward(x, t), y)\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            return\n",
    "        \n",
    "        endcnt = min(self.mem_cnt + bsz, self.n_memories) #256\n",
    "        effbsz = endcnt - self.mem_cnt # 256\n",
    "        self.memory_data[t, self.mem_cnt: endcnt].copy_(\n",
    "            x.data[: effbsz])\n",
    "        if bsz == 1:\n",
    "            self.memory_labs[t, self.mem_cnt] = y.data[0]\n",
    "        else:\n",
    "            self.memory_labs[t, self.mem_cnt: endcnt].copy_(\n",
    "                y.data[: effbsz])\n",
    "        self.mem_cnt += effbsz\n",
    "        if self.mem_cnt == self.n_memories:\n",
    "            self.mem_cnt = 0\n",
    "\n",
    "        # compute gradient on previous tasks\n",
    "        # if PRETRAIN == 0:\n",
    "        #     val = 1\n",
    "        # else:\n",
    "        #     val = 0\n",
    "        if len(self.observed_tasks) > 0: ### CHANGED FROM 1 to 0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "            for tt in range(len(self.observed_tasks) -1): ### CHANGED FROM -1 to -0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "                self.zero_grad()\n",
    "                # fwd/bwd on the examples in the memory\n",
    "                past_task = self.observed_tasks[tt]\n",
    "                \n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                   self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                           past_task)\n",
    "\n",
    "        # now compute the grad on the current minibatch\n",
    "        self.zero_grad()\n",
    "\n",
    "        offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar) \n",
    "        loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)\n",
    "        loss.backward()\n",
    "\n",
    "        # check if gradient violates constraints\n",
    "        if len(self.observed_tasks) > 0: ### CHANGED FROM 1 to 0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "            if algorithm == 'AGEM':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    agemprojection(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "            # copy gradient\n",
    "            elif algorithm == 'GEM':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    project2cone2(self.grads[:, t].unsqueeze(1),\n",
    "                                self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "            elif algorithm == 'REPLAY':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                replay(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx))\n",
    "                # copy gradients back\n",
    "                overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                            self.grad_dims)\n",
    "        self.opt.step()\n",
    "\n",
    "\n",
    "    def unlearn(self, algorithm, t):\n",
    "        ## first check if task t has been learned\n",
    "        if t not in self.observed_tasks:\n",
    "            print(\"Task , \", t, \" has not been learned yet - No change\")\n",
    "            return\n",
    "\n",
    "        ## now check if the task is not the first task learned \n",
    "        if len(self.observed_tasks) == 1:\n",
    "            print(\"Only one task has been learned - resetting the model\")\n",
    "            self.reset()\n",
    "            return\n",
    "\n",
    "        ## otherwise we need to unlearn the task \n",
    "        ## we compute the gradients of all learnt tasks\n",
    "        #### perhaps we need to have a temporary store of the gradients that were already stored in the model itself.\n",
    "        current_grad = []\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                current_grad.append(param.grad.data.view(-1))\n",
    "        current_grad = torch.cat(current_grad).unsqueeze(1)\n",
    "\n",
    "\n",
    "        # now find the grads of the previous tasks\n",
    "        for tt in range(len(self.observed_tasks)):\n",
    "            self.zero_grad()\n",
    "            past_task = self.observed_tasks[tt]\n",
    "            offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                   self.is_cifar)\n",
    "            ptloss = self.ce(\n",
    "                self.forward(\n",
    "                    self.memory_data[past_task],\n",
    "                    past_task)[:, offset1: offset2],\n",
    "                self.memory_labs[past_task] - offset1)\n",
    "            ptloss.backward()\n",
    "            store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                           past_task)\n",
    "        \n",
    "        ## so now, we have the gradients of all the tasks we can now do our projection,\n",
    "        ## first we check if it is even neccessary to do so, if not simply do a optimiser.step()\n",
    "        if algorithm == 'GEM':\n",
    "            forget_grads = self.grads[:, t].unsqueeze(1).clone()\n",
    "            retain_indices = torch.tensor([i for i in range(self.grads.size(1)) if i in self.observed_tasks and i != t], device=self.grads.device)\n",
    "            retain_grads = self.grads.index_select(1, retain_indices)\n",
    "            project2cone2_neggrad(current_grad, forget_grads, retain_grads, self.margin)\n",
    "            overwrite_grad(self.parameters, current_grad, self.grad_dims)\n",
    "            self.opt.step()\n",
    "\n",
    "def project2cone2_neggrad(gradient, forget_memories, retain_memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\" \n",
    "        Solves and alternative version of the GEM dual QP described in the paper given a proposed\n",
    "        The Main change is that we project the gradient to the nearest valid gradient in the L2-Norm which has the following properties:\n",
    "        1. The dot product of the gradient with the forget memories is less than margin (0.5)\n",
    "        2. The dot product of the gradient with the retain memories is greater than margin (0.5)\n",
    "        3. The gradient is as close as possible to the original gradient\n",
    "        input:  gradient, p-vector\n",
    "        input:  forget_memories, p-vector\n",
    "        input:  retain_memories, (t-1 * p)-vector\n",
    "        output: x, p-vector\n",
    "    \"\"\"\n",
    "    margin = 0.1\n",
    "    # Convert inputs\n",
    "    forget_mem_np = forget_memories.cpu().t().double().numpy()  # shape: (t, p). For our case, t==1.\n",
    "    retain_mem_np = retain_memories.cpu().t().double().numpy()  # shape: (r, p)\n",
    "    grad_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = forget_mem_np.shape[0]  # should be 1\n",
    "    \n",
    "    # Build the QP for the forgotten task constraints.\n",
    "    # We want to correct grad by adding a combination of forget_mem_np.\n",
    "    # Let x = grad_np + forget_mem_np.T * v. Our goal is\n",
    "    #   minimize 0.5 || x - grad_np ||^2\n",
    "    # subject to:\n",
    "    #   forget: forget_mem_np @ x <= margin\n",
    "    #   retain: retain_mem_np @ x >= margin   (for each retained task)\n",
    "    #\n",
    "    # In dual form, one typical formulation is:\n",
    "    P = np.dot(forget_mem_np, forget_mem_np.T)\n",
    "    P = 0.5 * (P + P.T) + np.eye(t)*eps\n",
    "    q = np.dot(forget_mem_np, grad_np) * -1\n",
    "\n",
    "    # For forgotten task: we want: forget_mem_np @ x <= margin  <=> -[forget_mem_np @ x] >= -margin\n",
    "    # This yields one set of constraints on v.\n",
    "    # Since t==1, we’ll have one constraint:\n",
    "    G_forget = -np.eye(t)     # shape: (t,t)\n",
    "    h_forget = (-margin * np.ones(t)).ravel() \n",
    "\n",
    "    # For retained tasks: For each retained memory vector m, we want m @ x >= margin.\n",
    "    # Write: m @ (grad_np + forget_mem_np.T * v) >= margin  => m @ forget_mem_np.T * v >= margin - m @ grad_np.\n",
    "    if retain_mem_np.shape[0] > 0:\n",
    "        # For each retained memory row i, m_i isn't necessarily scalar, so we write constraint row as:\n",
    "        # Let A = retain_mem_np @ forget_mem_np.T, b = margin - retain_mem_np @ grad_np.\n",
    "        A = np.dot(retain_mem_np, forget_mem_np.T)  # shape: (r, t)\n",
    "        b = np.zeros(retain_mem_np.shape[0]) + margin #margin - np.dot(retain_mem_np, grad_np)   # shape: (r,)\n",
    "        # Write the constraint as A * v >= b.\n",
    "        G_retain = A\n",
    "        h_retain = b.ravel()\n",
    "        # Combine constraints\n",
    "        G = np.concatenate((G_forget, G_retain), axis=0)\n",
    "        h = np.concatenate((h_forget, h_retain), axis=0).ravel()\n",
    "    else:\n",
    "        G = G_forget\n",
    "        h = h_forget\n",
    "\n",
    "    # Solve the quadprog: min 0.5 * v^T P v + q^T v  s.t. G v >= h.\n",
    "    v = quadprog.solve_qp(P, q, G.T, h)[0]\n",
    "    # Recover the projected gradient:\n",
    "    x = np.dot(v, forget_mem_np) + grad_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "    # # Convert inputs to NumPy arrays (with appropriate shapes)\n",
    "    # forget_mem_np = forget_memories.cpu().t().double().numpy()    # shape: (t, p) with t==1\n",
    "    # retain_mem_np = retain_memories.cpu().t().double().numpy()     # shape: (r, p)\n",
    "    # grad_np = gradient.cpu().contiguous().view(-1).double().numpy()  # shape: (p,)\n",
    "    # t = forget_mem_np.shape[0]  # should be 1\n",
    "    # print(\"retain_mem_np.shape:\", retain_mem_np.shape)\n",
    "    # # Build the QP matrices (dual) for the forgotten task correction.\n",
    "    # # We work with the candidate update: x = grad_np + forget_mem_np.T * v\n",
    "    # # and want to solve: min_v  0.5 ||x - grad_np||^2  subject to constraints.\n",
    "    # P = np.dot(forget_mem_np, forget_mem_np.T)             # shape: (t, t) → (1,1)\n",
    "    # P = 0.5*(P + P.T) + np.eye(t)*eps\n",
    "    # q = -np.dot(forget_mem_np, grad_np)                      # shape: (t,)\n",
    "\n",
    "    # # Constraints for forgotten task: we want\n",
    "    # #    forget_mem_np @ x <= margin \n",
    "    # # i.e. -[forget_mem_np @ x] >= -margin.\n",
    "    # # With x = grad_np + forget_mem_np.T * v this is equivalent to\n",
    "    # #    - I * v >= -margin  →  v <= margin.\n",
    "    # G_forget = -np.eye(t)                                    # shape: (t, t) i.e. (1,1)\n",
    "    # h_forget = (-margin * np.ones(t)).ravel()                # shape: (t,)\n",
    "\n",
    "    # # For retained tasks: for each retained vector m, we require:\n",
    "    # #    m @ (grad_np + forget_mem_np.T * v) >= margin\n",
    "    # # Rearranging gives: m @ forget_mem_np.T * v >= margin - m @ grad_np.\n",
    "    # if retain_mem_np.shape[0] > 0:\n",
    "    #     A = np.dot(retain_mem_np, forget_mem_np.T)           # shape: (r, t)  e.g. (r,1)\n",
    "    #     print(\"A.shape: \", A.shape)\n",
    "    #     # np.dot(retain_mem_np, grad_np) might be (r,) or (r,1); force it to be 1-D:\n",
    "    #     b = margin - np.squeeze(np.dot(retain_mem_np, grad_np))  # shape: (r,)\n",
    "    #     # In case squeeze returns a scalar when r==1, ensure it's an array:\n",
    "    #     if np.isscalar(b):\n",
    "    #         b = np.array([b])\n",
    "    #     G_retain = A                                      # shape: (r, t)\n",
    "    #     h_retain = np.array(b).ravel()                                # shape: (r,)\n",
    "    #     # Combine the constraints (first the forgotten constraint, then retained ones)\n",
    "    #     print(\"shape of G_forget: \", G_forget.shape, \"shape of G_retain: \", G_retain.shape)\n",
    "    #     G = np.concatenate((G_forget, G_retain), axis=0)    # shape: ((t+r), t)\n",
    "    #     print(\"shape of h_forget: \", h_forget.shape, \"shape of h_retain: \", h_retain.shape)\n",
    "    #     h = np.concatenate((h_forget, h_retain), axis=0).ravel()  # shape: (t+r,)\n",
    "    #     print(\"shape of G: \", G.shape, \"shape of h: \", h.shape)\n",
    "    # else:\n",
    "    #     G = G_forget\n",
    "    #     h = h_forget\n",
    "\n",
    "    # # Debug (optional): Uncomment to print shapes\n",
    "    # # print(\"P:\", P.shape, \"q:\", q.shape, \"G:\", G.shape, \"h:\", h.shape)\n",
    "\n",
    "    # # Ensure G and h are in the correct 2D and 1D formats:\n",
    "    # G = np.atleast_2d(G)\n",
    "    # h = np.asarray(h).flatten()\n",
    "    # # print(\"shape of G: \", G.shape, \"shape of h: \", h.shape)\n",
    "    # # Solve the QP: minimize 0.5 v^T P v + q^T v s.t. G v >= h.\n",
    "    # print(\"shape of P: \", P.shape, \"shape of q: \", q.shape, \"shape of G: \", G.shape, \"shape of h: \", h.shape)\n",
    "    # G_in = np.ascontiguousarray(G, dtype=float)\n",
    "    # h_in = np.ascontiguousarray(h, dtype=float).flatten()\n",
    "    # v = quadprog.solve_qp(P, q, G_in, h_in)[0]\n",
    "    # # Recover the projected gradient: x = grad_np + forget_mem_np.T * v.\n",
    "    # x = np.dot(v, forget_mem_np) + grad_np  # v has shape (t,), forget_mem_np shape (t, p) so result shape (p,)\n",
    "    # gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuum Learning (TIL)\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - Initializes the neural network `model` using the specified number of inputs, outputs, and tasks.\n",
    "   - Moves the model to CUDA if enabled in the arguments.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Loads the CIFAR dataset and shuffles the class labels according to the specified new order (`newclasses`).\n",
    "   - Maps the old class labels to their new ordering.\n",
    "   - Splits the dataset into pretraining and task-specific subsets:\n",
    "     - Pretraining data is used if `PRETRAIN > 0`.\n",
    "     - The remaining data is partitioned into tasks of size `size_of_task`.\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - Iterates through each task, training the model on its corresponding data.\n",
    "   - For each epoch:\n",
    "     - Divides the task's data into batches and trains the model using the `observe` method.\n",
    "   - Tests the model after each epoch and stops early if the training accuracy exceeds 75%.\n",
    "\n",
    "4. **Testing**:\n",
    "   - Evaluates the trained model on the test data for all tasks.\n",
    "   - Computes:\n",
    "     - **Test accuracy**: The fraction of correct predictions for each task.\n",
    "     - **Average confidence scores**: The mean confidence of the model's predictions (using softmax probabilities).\n",
    "\n",
    "5. **Return Values**:\n",
    "   - The function returns:\n",
    "     - The trained model.\n",
    "     - A list of test accuracies for all tasks.\n",
    "     - A list of average confidence scores for each task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define main function to run on the cifar dataset\n",
    "N_TASKS = 20 #[2 tasks [airplane, automobile, etc], [dog , frog, etc]]\n",
    "SIZE_OF_TASKS = 5\n",
    "N_OUTPUTS = 100\n",
    "N_INPUTS = 32 * 32 * 3\n",
    "def run_cifar(algorithm, args, n_inputs=N_INPUTS, n_outputs=N_OUTPUTS, n_tasks=N_TASKS, size_of_task=SIZE_OF_TASKS, newclasses = SHUFFLEDCLASSES):\n",
    "    # Set up the model\n",
    "    model = Net(n_inputs, n_outputs, n_tasks, args)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    model.is_cifar = True\n",
    "    test_bs = 1000\n",
    "    print(args.batch_size)\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Load data\n",
    "    train_data, train_labels, test_data, test_labels = load_data(DATASET_PATH, DATASET)\n",
    "    ## NEWCLASSES = shuffled CLASSES\n",
    "    NEWCLASSES = newclasses\n",
    "    print(\"new ordering of classes: \", NEWCLASSES)\n",
    "    oldClasstoNewClass = {}\n",
    "    for i in range(len(CLASSES)):\n",
    "        oldClasstoNewClass[i] = NEWCLASSES.index(CLASSES[i])\n",
    "    for i in range(len(train_labels)):\n",
    "        train_labels[i] = oldClasstoNewClass[train_labels[i]]\n",
    "    for i in range(len(test_labels)):\n",
    "        test_labels[i] = oldClasstoNewClass[test_labels[i]]\n",
    "\n",
    "    pretrain_classses = NEWCLASSES[:PRETRAIN]\n",
    "    pretrain_data, pretrain_labels = split_into_classes(train_data, train_labels, pretrain_classses, NEWCLASSES)\n",
    "    pretest_data, pretest_labels = split_into_classes(test_data, test_labels, pretrain_classses, NEWCLASSES)\n",
    "    tasks = []\n",
    "    tests = []\n",
    "    if PRETRAIN > 0:\n",
    "        tasks = [[pretrain_data, pretrain_labels]]\n",
    "        tests = [pretest_data, pretest_labels]\n",
    "    else:\n",
    "\n",
    "        tasks = []\n",
    "        tests = []\n",
    "    for i in range(n_tasks):\n",
    "        if i == 0 and PRETRAIN > 0: ## as task 1 we already grab from \n",
    "            continue\n",
    "        ## Since we have already pretrain on the first x classes, we need to offset our counter by x to learn the next set of classes\n",
    "        elif PRETRAIN > 0:\n",
    "            task_data, task_labels = split_into_classes(train_data, train_labels, NEWCLASSES[PRETRAIN + size_of_task * (i-1) : PRETRAIN + size_of_task * (i)], NEWCLASSES)\n",
    "            tasks.append([task_data, task_labels])\n",
    "            partition_test_data, partition_test_labels = split_into_classes(test_data, test_labels, NEWCLASSES[PRETRAIN + size_of_task * (i-1) : PRETRAIN + size_of_task * (i)], NEWCLASSES)\n",
    "            tests.append(partition_test_data)\n",
    "            tests.append(partition_test_labels)\n",
    "        ## no pretraining, carry on as normal\n",
    "        else:\n",
    "            task_data, task_labels = split_into_classes(train_data, train_labels, NEWCLASSES[size_of_task * i : size_of_task * (i + 1)] , NEWCLASSES)\n",
    "            tasks.append([task_data, task_labels])\n",
    "            partition_test_data, partition_test_labels = split_into_classes(test_data, test_labels, NEWCLASSES[size_of_task * i : size_of_task * (i + 1)] , NEWCLASSES)\n",
    "            tests.append(partition_test_data)\n",
    "            tests.append(partition_test_labels)\n",
    "\n",
    "    # Train the model\n",
    "    for task in range(n_tasks):\n",
    "        print(\"Training task: \", task  + 1)\n",
    "        \n",
    "        x = torch.Tensor(tasks[task][0].reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(tasks[task][1]).long()\n",
    "        \n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "        for epoch in range(args.n_epochs):\n",
    "            for j in range(0, len(tasks[task][0]), args.batch_size):\n",
    "                current_data = x[j: j + args.batch_size]\n",
    "                current_labels = y[j: j + args.batch_size]\n",
    "                model.train()\n",
    "                model.observe(algorithm, current_data, task, current_labels)\n",
    "            \n",
    "            #test the model after each epoch\n",
    "            correct = 0\n",
    "            total = len(tasks[task][0])\n",
    "            for j in range(0,len(tasks[task][0]), test_bs):\n",
    "                current_data = x[j: j + test_bs]\n",
    "                current_labels = y[j: j + test_bs]\n",
    "                output = model.forward(current_data, task)\n",
    "                pred = output.data.max(1)[1]\n",
    "\n",
    "            if correct / total > 0.75:\n",
    "                break\n",
    "            #   output loss only\n",
    "\n",
    "\n",
    "    # Test the model after training\n",
    "        average_confidence = []\n",
    "        for i in range(0, len(tests), 2):\n",
    "            correct = 0\n",
    "            total = len(tests[i])     \n",
    "\n",
    "            # Test the model\n",
    "            \n",
    "            x = torch.Tensor(tests[i].reshape(-1, 32*32*3)).float()\n",
    "            y = torch.Tensor(tests[i+1]).long()\n",
    "            if args.cuda:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            model.eval()\n",
    "            average_confidence_task = []\n",
    "            # keep track of average confidence score\n",
    "            for j in range(0,len(tests[i]), test_bs):\n",
    "                current_data = x[j: j + test_bs]\n",
    "                current_labels = y[j: j + test_bs]\n",
    "                output = model.forward(current_data, i // 2)\n",
    "                # apply softmax to get predictions\n",
    "                probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "                # get the maximum value of the probabilities for each image\n",
    "                predicted = torch.max(probabilities, 1).values\n",
    "                # get the average confidence score of the batch\n",
    "                average_confidence_task.append(torch.mean(predicted).item())\n",
    "                \n",
    "                pred = output.data.max(1)[1]\n",
    "                correct += (pred == current_labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct / total)\n",
    "            average_confidence.append(sum(average_confidence_task) / len(average_confidence_task))  \n",
    "    \n",
    "\n",
    "    ## after training lets unlearn the last task and test the model again\n",
    "    print(\"Unlearning task: \", n_tasks)\n",
    "    model.unlearn(algorithm, n_tasks - 1)\n",
    "    after_unlearn_accuracies = []\n",
    "    average_confidence_after_unlearn = []\n",
    "    for i in range(0, len(tests), 2):\n",
    "        correct = 0\n",
    "        total = len(tests[i])     \n",
    "\n",
    "        # Test the model\n",
    "        \n",
    "        x = torch.Tensor(tests[i].reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(tests[i+1]).long()\n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        model.eval()\n",
    "        average_confidence_task = []\n",
    "        # keep track of average confidence score\n",
    "        for j in range(0,len(tests[i]), test_bs):\n",
    "            current_data = x[j: j + test_bs]\n",
    "            current_labels = y[j: j + test_bs]\n",
    "            output = model.forward(current_data, i // 2)\n",
    "            # apply softmax to get predictions\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            # get the maximum value of the probabilities for each image\n",
    "            predicted = torch.max(probabilities, 1).values\n",
    "            # get the average confidence score of the batch\n",
    "            average_confidence_task.append(torch.mean(predicted).item())\n",
    "            \n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += (pred == current_labels).sum().item()\n",
    "\n",
    "        after_unlearn_accuracies.append(correct / total)\n",
    "        average_confidence_after_unlearn.append(sum(average_confidence_task) / len(average_confidence_task))\n",
    "    return model, test_accuracies, average_confidence , after_unlearn_accuracies, average_confidence_after_unlearn\n",
    "\n",
    "# sample usage\n",
    "# model, test_accuracies = run_cifar(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "new ordering of classes:  ['cattle', 'man', 'caterpillar', 'cloud', 'spider', 'chimpanzee', 'shrew', 'baby', 'plates', 'television', 'butterfly', 'bear', 'plain', 'tank', 'sweet peppers', 'kangaroo', 'oranges', 'beetle', 'rabbit', 'train', 'pears', 'couch', 'flatfish', 'aquarium fish', 'otter', 'skunk', 'bottles', 'bridge', 'telephone', 'wolf', 'wardrobe', 'mountain', 'trout', 'tractor', 'palm', 'bowls', 'leopard', 'hamster', 'chair', 'possum', 'roses', 'elephant', 'lion', 'orchids', 'bed', 'bicycle', 'cans', 'bee', 'bus', 'poppies', 'dolphin', 'lizard', 'shark', 'porcupine', 'crab', 'whale', 'streetcar', 'boy', 'mouse', 'woman', 'castle', 'motorcycle', 'maple', 'willow', 'fox', 'sunflowers', 'lawn-mower', 'girl', 'tiger', 'forest', 'crocodile', 'cockroach', 'beaver', 'lobster', 'seal', 'clock', 'tulips', 'sea', 'pickup truck', 'oak', 'house', 'skyscraper', 'mushrooms', 'snake', 'raccoon', 'camel', 'lamp', 'dinosaur', 'ray', 'cups', 'rocket', 'computer keyboard', 'pine', 'squirrel', 'table', 'apples', 'snail', 'turtle', 'worm', 'road']\n",
      "Training task:  1\n",
      "Training task:  2\n",
      "Training task:  3\n",
      "Training task:  4\n",
      "Training task:  5\n"
     ]
    }
   ],
   "source": [
    "test_accuracies_GEM_all = []\n",
    "prediction_confidence_GEM_all = []\n",
    "\n",
    "# flush the cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "random.shuffle(SHUFFLEDCLASSES)\n",
    "model, test_accuracies_GEM, confidence , after_unlearn_acc, after_unlearn_conf = run_cifar('GEM', Args())\n",
    "print(\"GEM last task confidence: \", confidence)\n",
    "print(test_accuracies_GEM)\n",
    "prediction_confidence_GEM_all.append(confidence)\n",
    "test_accuracies_GEM_all.append(test_accuracies_GEM)\n",
    "\n",
    "average_GEM_accuracies = np.mean(test_accuracies_GEM_all, axis=0)\n",
    "task_1_accuracies_GEM = []\n",
    "task_1_accuracies_GEM.append(0.2)\n",
    "\n",
    "for i in range(0, N_TASKS * N_TASKS, N_TASKS):\n",
    "    task_1_accuracies_GEM.append(average_GEM_accuracies[i])\n",
    "\n",
    "plt.plot(task_1_accuracies_GEM, label='GEM')\n",
    "plt.title('Task 1 Test Set Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Tasks')\n",
    "plt.legend()\n",
    "# plt.savefig('task_1_accuracy_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# plot a bar graph of the accuracies of all tasks before unlearning this uses the last 20 entries of the test_accuracies_GEM\n",
    "plt.bar(range(1, 21), test_accuracies_GEM[-20:])\n",
    "plt.title('Test Set for each task after learning all 20 Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Tasks')\n",
    "# plt.savefig('task_20_accuracy_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# plot a bar graph of the accuracies of all tasks after unlearning this uses the last 20 entries of the after_unlearn_acc\n",
    "plt.bar(range(1, 21), after_unlearn_acc)\n",
    "plt.title('Test Set for each task after unlearning the last task Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Tasks')\n",
    "# plt.savefig('task_20_accuracy_comparison_after_unlearning.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
