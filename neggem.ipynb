{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Idea of this notebook is to implement neggrad with the constraints of GEM. We want to be able to give a series of commands i.e. learn 1, learn 2 , unlearn 1 and it does the learning and unlearning respectively.\n",
    "\n",
    "The only change we would need to do to implement neggrad consistency. This is easy, for the given tasks that are learnt, X and the task number y that we wish to unlearn, we ensure the constraint in GEM with task y is <= 0 while the rest are >= 0 as the same as it would usually be. \n",
    "\n",
    "The hard part of this is ensuring consistency w.r.t the storing of gradients and memory buffers.\n",
    "\n",
    "If we can do this we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Setting Up the Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set directory /dcs/large/u2145461/cs407/Machine-Unlearning-x-Continual-Learning\n",
    "# please change these dependent on your own specific path variable\n",
    "\n",
    "os.chdir('/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning')\n",
    "\n",
    "save_path_1 = '/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning/GEM/Results4/'\n",
    "\n",
    "save_path_2 = '/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning/GEM/Results/'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes, load_data\n",
    "import cifar\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "import quadprog\n",
    "\n",
    "# we provide some top level initial parameters depending on if we want to work in cifar-10 or cifar-100\n",
    "\n",
    "AGEM = True\n",
    "PRETRAIN = 0 # number of initial classes to pretrain on\n",
    "# Globals \n",
    "DATASET = 'cifar-100'\n",
    "DATASET_PATH = 'cifar-100-python' \n",
    "CLASSES = cifar.CLASSES\n",
    "SHUFFLEDCLASSES = CLASSES.copy()\n",
    "CONFIDENCE_SAMPLES = 5\n",
    "if DATASET == 'cifar-10':\n",
    "    CLASSES = cifar.CLASSES\n",
    "    CLASSES = CLASSES.copy()\n",
    "elif DATASET == 'cifar-100':\n",
    "    CLASSES = cifar.CLASSES_100_UNORDERED\n",
    "    SHUFFLEDCLASSES = CLASSES.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "# C\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(nclasses, nf=20):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The brains - here we define the memory facilities and the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offsets(task, nc_per_task, is_cifar):\n",
    "    \"\"\"\n",
    "        Compute offsets for cifar to determine which\n",
    "        outputs to select for a given task.\n",
    "    \"\"\"\n",
    "    val1 = max(PRETRAIN - nc_per_task, 0)\n",
    "    val2 = max(PRETRAIN - nc_per_task, 0)\n",
    "    if task == 0:\n",
    "        val1 = 0\n",
    "        val2 = max(PRETRAIN - nc_per_task, 0)\n",
    "    offset1 = task * nc_per_task + val1\n",
    "    offset2 = (task + 1) * nc_per_task + val2    \n",
    "    return offset1, offset2\n",
    "\n",
    "\n",
    "def store_grad(pp, grads, grad_dims, tid):\n",
    "    \"\"\"\n",
    "        This stores parameter gradients of past tasks.\n",
    "        pp: parameters\n",
    "        grads: gradients\n",
    "        grad_dims: list with number of parameters per layers\n",
    "        tid: task id\n",
    "    \"\"\"\n",
    "    # store the gradients\n",
    "    grads[:, tid].fill_(0.0)\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def overwrite_grad(pp, newgrad, grad_dims):\n",
    "    \"\"\"\n",
    "        This is used to overwrite the gradients with a new gradient\n",
    "        vector, whenever violations occur.\n",
    "        pp: parameters\n",
    "        newgrad: corrected gradient\n",
    "        grad_dims: list storing number of parameters at each layer\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            this_grad = newgrad[beg: en].contiguous().view(\n",
    "                param.grad.data.size())\n",
    "            param.grad.data.copy_(this_grad)\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\"\n",
    "        Solves the GEM dual QP described in the paper given a proposed\n",
    "        gradient \"gradient\", and a memory of task gradients \"memories\".\n",
    "        Overwrites \"gradient\" with the final projected update.\n",
    "\n",
    "        input:  gradient, p-vector\n",
    "        input:  memories, (t * p)-vector\n",
    "        output: x, p-vector\n",
    "    \"\"\"\n",
    "    memories_np = memories.cpu().t().double().numpy()\n",
    "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = memories_np.shape[0]\n",
    "    P = np.dot(memories_np, memories_np.transpose())\n",
    "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
    "    q = np.dot(memories_np, gradient_np) * -1\n",
    "    G = np.eye(t)\n",
    "    h = np.zeros(t) + margin\n",
    "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "    x = np.dot(v, memories_np) + gradient_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "def agemprojection(gradient, gradient_memory, margin=0.5, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Projection of gradients for A-GEM with the memory approach\n",
    "    Use averaged gradient memory for projection\n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "\n",
    "    gref = gradient_memory.t().double().mean(axis=0).cuda() # * margin\n",
    "    g = gradient.contiguous().view(-1).double().cuda()\n",
    "\n",
    "    dot_prod = torch.dot(g, gref)\n",
    "    \n",
    "    #if dot_prod < 0:\n",
    "    #    x = g\n",
    "    #    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    #    return\n",
    "    \n",
    "    # avoid division by zero\n",
    "    dot_prod = dot_prod/(torch.dot(gref, gref))\n",
    "    \n",
    "    # epsvector = torch.Tensor([eps]).cuda()\n",
    "    \n",
    "    x = g*0.5 + gref * abs(dot_prod)  # + epsvector\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    \n",
    "def replay(gradient, gradient_memory):\n",
    "    \"\"\"\n",
    "    Adds the gradients of the current task to the memory \n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "    g = gradient_memory.t().double().sum(axis=0).cuda()\n",
    "    gref = gradient.contiguous().view(-1).double().cuda()\n",
    "    # simply add the gradients\n",
    "    x = g + gref\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    \n",
    "def naiveretraining(gradient):\n",
    "    \"\"\"\n",
    "    Naive retraining of the model on the current task\n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "    g = gradient.t().double().mean(axis=0).cuda()\n",
    "    gradient.copy_(torch.Tensor(g).view(-1, 1))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_outputs,\n",
    "                 n_tasks,\n",
    "                 args):\n",
    "        super(Net, self).__init__()\n",
    "        nl, nh = args.n_layers, args.n_hiddens\n",
    "        self.margin = args.memory_strength\n",
    "        self.net = ResNet18(n_outputs)\n",
    "\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.opt = torch.optim.SGD(self.parameters(), args.lr)\n",
    "\n",
    "        self.n_memories = args.n_memories\n",
    "        self.gpu = args.cuda\n",
    "\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Allocate episodic memory\n",
    "        n_tasks: number of tasks\n",
    "        n_memories: number of memories per task\n",
    "        n_inputs: number of input features\n",
    "        \"\"\"\n",
    "\n",
    "        # allocate episodic memory\n",
    "        self.memory_data = torch.FloatTensor(\n",
    "            n_tasks, self.n_memories, n_inputs)\n",
    "        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)\n",
    "        if args.cuda:\n",
    "            self.memory_data = self.memory_data.cuda()\n",
    "            self.memory_labs = self.memory_labs.cuda()\n",
    "\n",
    "        # allocate temporary synaptic memory\n",
    "        \"\"\" This is the memory that stores the gradients of the parameters of the network\n",
    "            FOR each task. This is used to check for violations of the GEM constraint\n",
    "            Assume:\n",
    "\n",
    "            The model has 3 parameters with sizes 100, 200, and 300 elements respectively.\n",
    "            n_tasks = 5 (number of tasks).\n",
    "            The allocated tensors would have the following shapes:\n",
    "\n",
    "            self.grad_dims: [100, 200, 300]\n",
    "            self.grads: Shape [600, 5] (600 is the sum of 100, 200, and 300).\n",
    "        \"\"\"\n",
    "        self.grad_dims = []\n",
    "        for param in self.parameters():\n",
    "            self.grad_dims.append(param.data.numel())\n",
    "        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)\n",
    "        if args.cuda:\n",
    "            self.grads = self.grads.cuda()\n",
    "\n",
    "        # allocate counters\n",
    "        self.observed_tasks = []\n",
    "        self.old_task = -1\n",
    "        self.mem_cnt = 0\n",
    "        minus = 0\n",
    "        if PRETRAIN > 0:\n",
    "            minus = 1\n",
    "        else: \n",
    "            minus = 0\n",
    "        self.nc_per_task = int((n_outputs - PRETRAIN) / (n_tasks - minus))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        output = self.net(x)\n",
    "        if t == -1:\n",
    "            return output\n",
    "        # make sure we predict classes within the current task\n",
    "        val1 = 0\n",
    "        val2 = 0\n",
    "        if t != 0:\n",
    "            val1 = max(PRETRAIN - self.nc_per_task, 0)\n",
    "            val2 = val1\n",
    "        else:\n",
    "            val1 = 0\n",
    "            val2 = max(PRETRAIN - self.nc_per_task, 0)                                                 \n",
    "        offset1 = int(t * self.nc_per_task + val1) #t = 0 0, 5 -----t = 1 5 , 6 ## t = 0 0 ,5 --- t =1 5, 7\n",
    "        offset2 = int((t + 1) * self.nc_per_task + val2) \n",
    "        if offset1 > 0:\n",
    "            output[:, :offset1].data.fill_(-10e10)\n",
    "        if offset2 < self.n_outputs:\n",
    "            output[:, offset2:self.n_outputs].data.fill_(-10e10)\n",
    "        return output\n",
    "\n",
    "    def observe(self, algorithm, x, t, y):\n",
    "        # update memory\n",
    "        if t != self.old_task:\n",
    "            self.observed_tasks.append(t)\n",
    "            self.old_task = t\n",
    "            \n",
    "        val = 0\n",
    "        if t == 0:\n",
    "            val = max(PRETRAIN,1)\n",
    "        else:\n",
    "            val = 1\n",
    "        # Update ring buffer storing examples from current task\n",
    "        bsz = y.data.size(0)\n",
    "        if (algorithm == 'NAIVE'):\n",
    "            self.zero_grad()\n",
    "            loss = self.ce(self.forward(x, t), y)\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            return\n",
    "        \n",
    "        endcnt = min(self.mem_cnt + bsz, self.n_memories) #256\n",
    "        effbsz = endcnt - self.mem_cnt # 256\n",
    "        self.memory_data[t, self.mem_cnt: endcnt].copy_(\n",
    "            x.data[: effbsz])\n",
    "        if bsz == 1:\n",
    "            self.memory_labs[t, self.mem_cnt] = y.data[0]\n",
    "        else:\n",
    "            self.memory_labs[t, self.mem_cnt: endcnt].copy_(\n",
    "                y.data[: effbsz])\n",
    "        self.mem_cnt += effbsz\n",
    "        if self.mem_cnt == self.n_memories:\n",
    "            self.mem_cnt = 0\n",
    "\n",
    "        # compute gradient on previous tasks\n",
    "        # if PRETRAIN == 0:\n",
    "        #     val = 1\n",
    "        # else:\n",
    "        #     val = 0\n",
    "        if len(self.observed_tasks) > 0: ### CHANGED FROM 1 to 0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "            for tt in range(len(self.observed_tasks) -1): ### CHANGED FROM -1 to -0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "                self.zero_grad()\n",
    "                # fwd/bwd on the examples in the memory\n",
    "                past_task = self.observed_tasks[tt]\n",
    "                \n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                   self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                           past_task)\n",
    "\n",
    "        # now compute the grad on the current minibatch\n",
    "        self.zero_grad()\n",
    "\n",
    "        offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar) \n",
    "        loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)\n",
    "        loss.backward()\n",
    "\n",
    "        # check if gradient violates constraints\n",
    "        if len(self.observed_tasks) > 0: ### CHANGED FROM 1 to 0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "            if algorithm == 'AGEM':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    agemprojection(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "            # copy gradient\n",
    "            elif algorithm == 'GEM':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    project2cone2(self.grads[:, t].unsqueeze(1),\n",
    "                                self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "            elif algorithm == 'REPLAY':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                replay(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx))\n",
    "                # copy gradients back\n",
    "                overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                            self.grad_dims)\n",
    "        self.opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuum Learning (TIL)\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - Initializes the neural network `model` using the specified number of inputs, outputs, and tasks.\n",
    "   - Moves the model to CUDA if enabled in the arguments.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Loads the CIFAR dataset and shuffles the class labels according to the specified new order (`newclasses`).\n",
    "   - Maps the old class labels to their new ordering.\n",
    "   - Splits the dataset into pretraining and task-specific subsets:\n",
    "     - Pretraining data is used if `PRETRAIN > 0`.\n",
    "     - The remaining data is partitioned into tasks of size `size_of_task`.\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - Iterates through each task, training the model on its corresponding data.\n",
    "   - For each epoch:\n",
    "     - Divides the task's data into batches and trains the model using the `observe` method.\n",
    "   - Tests the model after each epoch and stops early if the training accuracy exceeds 75%.\n",
    "\n",
    "4. **Testing**:\n",
    "   - Evaluates the trained model on the test data for all tasks.\n",
    "   - Computes:\n",
    "     - **Test accuracy**: The fraction of correct predictions for each task.\n",
    "     - **Average confidence scores**: The mean confidence of the model's predictions (using softmax probabilities).\n",
    "\n",
    "5. **Return Values**:\n",
    "   - The function returns:\n",
    "     - The trained model.\n",
    "     - A list of test accuracies for all tasks.\n",
    "     - A list of average confidence scores for each task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define main function to run on the cifar dataset\n",
    "N_TASKS = 20 #[2 tasks [airplane, automobile, etc], [dog , frog, etc]]\n",
    "SIZE_OF_TASKS = 5\n",
    "N_OUTPUTS = 100\n",
    "N_INPUTS = 32 * 32 * 3\n",
    "def run_cifar(algorithm, args, n_inputs=N_INPUTS, n_outputs=N_OUTPUTS, n_tasks=N_TASKS, size_of_task=SIZE_OF_TASKS, newclasses = SHUFFLEDCLASSES):\n",
    "    # Set up the model\n",
    "    model = Net(n_inputs, n_outputs, n_tasks, args)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    model.is_cifar = True\n",
    "    test_bs = 1000\n",
    "    \n",
    "    test_accuracies = []\n",
    "\n",
    "    # Load data\n",
    "    train_data, train_labels, test_data, test_labels = load_data(DATASET_PATH, DATASET)\n",
    "    ## NEWCLASSES = shuffled CLASSES\n",
    "    NEWCLASSES = newclasses\n",
    "    print(\"new ordering of classes: \", NEWCLASSES)\n",
    "    oldClasstoNewClass = {}\n",
    "    for i in range(len(CLASSES)):\n",
    "        oldClasstoNewClass[i] = NEWCLASSES.index(CLASSES[i])\n",
    "    for i in range(len(train_labels)):\n",
    "        train_labels[i] = oldClasstoNewClass[train_labels[i]]\n",
    "    for i in range(len(test_labels)):\n",
    "        test_labels[i] = oldClasstoNewClass[test_labels[i]]\n",
    "\n",
    "    pretrain_classses = NEWCLASSES[:PRETRAIN]\n",
    "    pretrain_data, pretrain_labels = split_into_classes(train_data, train_labels, pretrain_classses, NEWCLASSES)\n",
    "    pretest_data, pretest_labels = split_into_classes(test_data, test_labels, pretrain_classses, NEWCLASSES)\n",
    "    tasks = []\n",
    "    tests = []\n",
    "    if PRETRAIN > 0:\n",
    "        tasks = [[pretrain_data, pretrain_labels]]\n",
    "        tests = [pretest_data, pretest_labels]\n",
    "    else:\n",
    "\n",
    "        tasks = []\n",
    "        tests = []\n",
    "    for i in range(n_tasks):\n",
    "        if i == 0 and PRETRAIN > 0: ## as task 1 we already grab from \n",
    "            continue\n",
    "        ## Since we have already pretrain on the first x classes, we need to offset our counter by x to learn the next set of classes\n",
    "        elif PRETRAIN > 0:\n",
    "            task_data, task_labels = split_into_classes(train_data, train_labels, NEWCLASSES[PRETRAIN + size_of_task * (i-1) : PRETRAIN + size_of_task * (i)], NEWCLASSES)\n",
    "            tasks.append([task_data, task_labels])\n",
    "            partition_test_data, partition_test_labels = split_into_classes(test_data, test_labels, NEWCLASSES[PRETRAIN + size_of_task * (i-1) : PRETRAIN + size_of_task * (i)], NEWCLASSES)\n",
    "            tests.append(partition_test_data)\n",
    "            tests.append(partition_test_labels)\n",
    "        ## no pretraining, carry on as normal\n",
    "        else:\n",
    "            task_data, task_labels = split_into_classes(train_data, train_labels, NEWCLASSES[size_of_task * i : size_of_task * (i + 1)] , NEWCLASSES)\n",
    "            tasks.append([task_data, task_labels])\n",
    "            partition_test_data, partition_test_labels = split_into_classes(test_data, test_labels, NEWCLASSES[size_of_task * i : size_of_task * (i + 1)] , NEWCLASSES)\n",
    "            tests.append(partition_test_data)\n",
    "            tests.append(partition_test_labels)\n",
    "\n",
    "    # Train the model\n",
    "    for task in range(n_tasks):\n",
    "        print(\"Training task: \", task  + 1)\n",
    "        \n",
    "        x = torch.Tensor(tasks[task][0].reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(tasks[task][1]).long()\n",
    "        \n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "        for epoch in range(args.n_epochs):\n",
    "            for j in range(0, len(tasks[task][0]), args.batch_size):\n",
    "                current_data = x[j: j + args.batch_size]\n",
    "                current_labels = y[j: j + args.batch_size]\n",
    "                model.train()\n",
    "                model.observe(algorithm, current_data, task, current_labels)\n",
    "            \n",
    "            #test the model after each epoch\n",
    "            correct = 0\n",
    "            total = len(tasks[task][0])\n",
    "            for j in range(0,len(tasks[task][0]), test_bs):\n",
    "                current_data = x[j: j + test_bs]\n",
    "                current_labels = y[j: j + test_bs]\n",
    "                output = model.forward(current_data, task)\n",
    "                pred = output.data.max(1)[1]\n",
    "\n",
    "            if correct / total > 0.75:\n",
    "                break\n",
    "            #   output loss only\n",
    "\n",
    "\n",
    "    # Test the model after training\n",
    "        average_confidence = []\n",
    "        for i in range(0, len(tests), 2):\n",
    "            correct = 0\n",
    "            total = len(tests[i])     \n",
    "\n",
    "            # Test the model\n",
    "            \n",
    "            x = torch.Tensor(tests[i].reshape(-1, 32*32*3)).float()\n",
    "            y = torch.Tensor(tests[i+1]).long()\n",
    "            if args.cuda:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            model.eval()\n",
    "            average_confidence_task = []\n",
    "            # keep track of average confidence score\n",
    "            for j in range(0,len(tests[i]), test_bs):\n",
    "                current_data = x[j: j + test_bs]\n",
    "                current_labels = y[j: j + test_bs]\n",
    "                output = model.forward(current_data, i // 2)\n",
    "                # apply softmax to get predictions\n",
    "                probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "                # get the maximum value of the probabilities for each image\n",
    "                predicted = torch.max(probabilities, 1).values\n",
    "                # get the average confidence score of the batch\n",
    "                average_confidence_task.append(torch.mean(predicted).item())\n",
    "                \n",
    "                pred = output.data.max(1)[1]\n",
    "                correct += (pred == current_labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct / total)\n",
    "            average_confidence.append(sum(average_confidence_task) / len(average_confidence_task))  \n",
    "    \n",
    "    return model, test_accuracies, average_confidence\n",
    "\n",
    "# sample usage\n",
    "# model, test_accuracies = run_cifar(Args())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
