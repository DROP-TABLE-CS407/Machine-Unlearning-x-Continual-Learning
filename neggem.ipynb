{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Idea of this notebook is to implement neggrad with the constraints of GEM. We want to be able to give a series of commands i.e. learn 1, learn 2 , unlearn 1 and it does the learning and unlearning respectively.\n",
    "\n",
    "The only change we would need to do to implement neggrad consistency. This is easy, for the given tasks that are learnt, X and the task number y that we wish to unlearn, we ensure the constraint in GEM with task y is <= 0 while the rest are >= 0 as the same as it would usually be. \n",
    "\n",
    "The hard part of this is ensuring consistency w.r.t the storing of gradients and memory buffers.\n",
    "\n",
    "If we can do this we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Setting Up the Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# set directory /dcs/large/u2145461/cs407/Machine-Unlearning-x-Continual-Learning\n",
    "# please change these dependent on your own specific path variable\n",
    "\n",
    "os.chdir('/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning')\n",
    "\n",
    "save_path_1 = '/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning/GEM/Results4/'\n",
    "\n",
    "save_path_2 = '/dcs/large/u2140671/drop-table/Machine-Unlearning-x-Continual-Learning/GEM/Results/'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes, load_data\n",
    "import cifar\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "import quadprog\n",
    "\n",
    "# we provide some top level initial parameters depending on if we want to work in cifar-10 or cifar-100\n",
    "\n",
    "AGEM = True\n",
    "PRETRAIN = 0 # number of initial classes to pretrain on\n",
    "# Globals \n",
    "DATASET = 'cifar-100'\n",
    "DATASET_PATH = 'cifar-100-python' \n",
    "CLASSES = cifar.CLASSES\n",
    "SHUFFLEDCLASSES = CLASSES.copy()\n",
    "CONFIDENCE_SAMPLES = 5\n",
    "if DATASET == 'cifar-10':\n",
    "    CLASSES = cifar.CLASSES\n",
    "    CLASSES = CLASSES.copy()\n",
    "elif DATASET == 'cifar-100':\n",
    "    CLASSES = cifar.CLASSES_100_UNORDERED\n",
    "    SHUFFLEDCLASSES = CLASSES.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "# C\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(nclasses, nf=20):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The brains - here we define the memory facilities and the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offsets(task, nc_per_task, is_cifar):\n",
    "    \"\"\"\n",
    "        Compute offsets for cifar to determine which\n",
    "        outputs to select for a given task.\n",
    "    \"\"\"\n",
    "    val1 = max(PRETRAIN - nc_per_task, 0)\n",
    "    val2 = max(PRETRAIN - nc_per_task, 0)\n",
    "    if task == 0:\n",
    "        val1 = 0\n",
    "        val2 = max(PRETRAIN - nc_per_task, 0)\n",
    "    offset1 = task * nc_per_task + val1\n",
    "    offset2 = (task + 1) * nc_per_task + val2    \n",
    "    return offset1, offset2\n",
    "\n",
    "\n",
    "def store_grad(pp, grads, grad_dims, tid):\n",
    "    \"\"\"\n",
    "        This stores parameter gradients of past tasks.\n",
    "        pp: parameters\n",
    "        grads: gradients\n",
    "        grad_dims: list with number of parameters per layers\n",
    "        tid: task id\n",
    "    \"\"\"\n",
    "    # store the gradients\n",
    "    grads[:, tid].fill_(0.0)\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def overwrite_grad(pp, newgrad, grad_dims):\n",
    "    \"\"\"\n",
    "        This is used to overwrite the gradients with a new gradient\n",
    "        vector, whenever violations occur.\n",
    "        pp: parameters\n",
    "        newgrad: corrected gradient\n",
    "        grad_dims: list storing number of parameters at each layer\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            this_grad = newgrad[beg: en].contiguous().view(\n",
    "                param.grad.data.size())\n",
    "            param.grad.data.copy_(this_grad)\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\"\n",
    "        Solves the GEM dual QP described in the paper given a proposed\n",
    "        gradient \"gradient\", and a memory of task gradients \"memories\".\n",
    "        Overwrites \"gradient\" with the final projected update.\n",
    "\n",
    "        input:  gradient, p-vector\n",
    "        input:  memories, (t * p)-vector\n",
    "        output: x, p-vector\n",
    "    \"\"\"\n",
    "    memories_np = memories.cpu().t().double().numpy()\n",
    "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = memories_np.shape[0]\n",
    "    P = np.dot(memories_np, memories_np.transpose())\n",
    "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
    "    q = np.dot(memories_np, gradient_np) * -1\n",
    "    G = np.eye(t)\n",
    "    h = np.zeros(t) + margin\n",
    "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "    x = np.dot(v, memories_np) + gradient_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "def agemprojection(gradient, gradient_memory, margin=0.5, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Projection of gradients for A-GEM with the memory approach\n",
    "    Use averaged gradient memory for projection\n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "\n",
    "    gref = gradient_memory.t().double().mean(axis=0).cuda() # * margin\n",
    "    g = gradient.contiguous().view(-1).double().cuda()\n",
    "\n",
    "    dot_prod = torch.dot(g, gref)\n",
    "    \n",
    "    #if dot_prod < 0:\n",
    "    #    x = g\n",
    "    #    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    #    return\n",
    "    \n",
    "    # avoid division by zero\n",
    "    dot_prod = dot_prod/(torch.dot(gref, gref))\n",
    "    \n",
    "    # epsvector = torch.Tensor([eps]).cuda()\n",
    "    \n",
    "    x = g*0.5 + gref * abs(dot_prod)  # + epsvector\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    \n",
    "def replay(gradient, gradient_memory):\n",
    "    \"\"\"\n",
    "    Adds the gradients of the current task to the memory \n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "    g = gradient_memory.t().double().sum(axis=0).cuda()\n",
    "    gref = gradient.contiguous().view(-1).double().cuda()\n",
    "    # simply add the gradients\n",
    "    x = g + gref\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "    \n",
    "def naiveretraining(gradient):\n",
    "    \"\"\"\n",
    "    Naive retraining of the model on the current task\n",
    "    \n",
    "    input:  gradient, g-reference\n",
    "    output: gradient, g-projected\n",
    "    \"\"\"\n",
    "    g = gradient.t().double().mean(axis=0).cuda()\n",
    "    gradient.copy_(torch.Tensor(g).view(-1, 1))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_outputs,\n",
    "                 n_tasks,\n",
    "                 args):\n",
    "        super(Net, self).__init__()\n",
    "        nl, nh = args.n_layers, args.n_hiddens\n",
    "        self.margin = args.memory_strength\n",
    "        self.net = ResNet18(n_outputs)\n",
    "\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.opt = torch.optim.SGD(self.parameters(), args.lr)\n",
    "\n",
    "        self.n_memories = args.n_memories\n",
    "        self.gpu = args.cuda\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Allocate episodic memory\n",
    "        n_tasks: number of tasks\n",
    "        n_memories: number of memories per task\n",
    "        n_inputs: number of input features\n",
    "        \"\"\"\n",
    "\n",
    "        # allocate episodic memory\n",
    "        self.memory_data = torch.FloatTensor(\n",
    "            n_tasks, self.n_memories, n_inputs)\n",
    "        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)\n",
    "        if args.cuda:\n",
    "            self.memory_data = self.memory_data.cuda()\n",
    "            self.memory_labs = self.memory_labs.cuda()\n",
    "\n",
    "        # allocate temporary synaptic memory\n",
    "        \"\"\" This is the memory that stores the gradients of the parameters of the network\n",
    "            FOR each task. This is used to check for violations of the GEM constraint\n",
    "            Assume:\n",
    "\n",
    "            The model has 3 parameters with sizes 100, 200, and 300 elements respectively.\n",
    "            n_tasks = 5 (number of tasks).\n",
    "            The allocated tensors would have the following shapes:\n",
    "\n",
    "            self.grad_dims: [100, 200, 300]\n",
    "            self.grads: Shape [600, 5] (600 is the sum of 100, 200, and 300).\n",
    "        \"\"\"\n",
    "        self.grad_dims = []\n",
    "        for param in self.parameters():\n",
    "            self.grad_dims.append(param.data.numel())\n",
    "        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)\n",
    "        if args.cuda:\n",
    "            self.grads = self.grads.cuda()\n",
    "\n",
    "        # allocate counters\n",
    "        self.observed_tasks = []\n",
    "        self.old_task = -1\n",
    "        self.mem_cnt = 0\n",
    "        minus = 0\n",
    "        if PRETRAIN > 0:\n",
    "            minus = 1\n",
    "        else: \n",
    "            minus = 0\n",
    "        self.nc_per_task = int((n_outputs - PRETRAIN) / (n_tasks - minus))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        output = self.net(x)\n",
    "        if t == -1:\n",
    "            return output\n",
    "        # make sure we predict classes within the current task\n",
    "        val1 = 0\n",
    "        val2 = 0\n",
    "        if t != 0:\n",
    "            val1 = max(PRETRAIN - self.nc_per_task, 0)\n",
    "            val2 = val1\n",
    "        else:\n",
    "            val1 = 0\n",
    "            val2 = max(PRETRAIN - self.nc_per_task, 0)                                                 \n",
    "        offset1 = int(t * self.nc_per_task + val1) #t = 0 0, 5 -----t = 1 5 , 6 ## t = 0 0 ,5 --- t =1 5, 7\n",
    "        offset2 = int((t + 1) * self.nc_per_task + val2) \n",
    "        if offset1 > 0:\n",
    "            output[:, :offset1].data.fill_(-10e10)\n",
    "        if offset2 < self.n_outputs:\n",
    "            output[:, offset2:self.n_outputs].data.fill_(-10e10)\n",
    "        return output\n",
    "\n",
    "    def observe(self, algorithm, x, t, y):\n",
    "        # update memory\n",
    "        if t != self.old_task or t not in self.observed_tasks:\n",
    "            self.observed_tasks.append(t)\n",
    "            self.old_task = t\n",
    "            \n",
    "        val = 0\n",
    "        if t == 0:\n",
    "            val = max(PRETRAIN,1)\n",
    "        else:\n",
    "            val = 1\n",
    "        # Update ring buffer storing examples from current task\n",
    "        bsz = y.data.size(0)\n",
    "        if (algorithm == 'NAIVE'):\n",
    "            self.zero_grad()\n",
    "            loss = self.ce(self.forward(x, t), y)\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            return\n",
    "        \n",
    "        endcnt = min(self.mem_cnt + bsz, self.n_memories) #256\n",
    "        effbsz = endcnt - self.mem_cnt # 256\n",
    "        self.memory_data[t, self.mem_cnt: endcnt].copy_(\n",
    "            x.data[: effbsz])\n",
    "        if bsz == 1:\n",
    "            self.memory_labs[t, self.mem_cnt] = y.data[0]\n",
    "        else:\n",
    "            self.memory_labs[t, self.mem_cnt: endcnt].copy_(\n",
    "                y.data[: effbsz])\n",
    "        self.mem_cnt += effbsz\n",
    "        if self.mem_cnt == self.n_memories:\n",
    "            self.mem_cnt = 0\n",
    "\n",
    "        # compute gradient on previous tasks\n",
    "        # if PRETRAIN == 0:\n",
    "        #     val = 1\n",
    "        # else:\n",
    "        #     val = 0\n",
    "        if len(self.observed_tasks) > 0: ### CHANGED FROM 1 to 0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "            for tt in range(len(self.observed_tasks) -1): ### CHANGED FROM -1 to -0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "                self.zero_grad()\n",
    "                # fwd/bwd on the examples in the memory\n",
    "                past_task = self.observed_tasks[tt]\n",
    "                \n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                   self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                           past_task)\n",
    "\n",
    "        # now compute the grad on the current minibatch\n",
    "        self.zero_grad()\n",
    "\n",
    "        offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar) \n",
    "        loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)\n",
    "        loss.backward()\n",
    "\n",
    "        # check if gradient violates constraints\n",
    "        if len(self.observed_tasks) > 0: ### CHANGED FROM 1 to 0 SINCE WE PRETRAIN ON FST 5 CLASSES \n",
    "            if algorithm == 'AGEM':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    agemprojection(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "            # copy gradient\n",
    "            elif algorithm == 'GEM':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                                self.grads.index_select(1, indx))\n",
    "                if (dotp < 0).sum() != 0:\n",
    "                    project2cone2(self.grads[:, t].unsqueeze(1),\n",
    "                                self.grads.index_select(1, indx), self.margin)\n",
    "                    # copy gradients back\n",
    "                    overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                                self.grad_dims)\n",
    "            elif algorithm == 'REPLAY':\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                    else torch.LongTensor(self.observed_tasks[:-1])\n",
    "                replay(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx))\n",
    "                # copy gradients back\n",
    "                overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                            self.grad_dims)\n",
    "        self.opt.step()\n",
    "\n",
    "\n",
    "    def unlearn(self, algorithm, t, alpha = 0.995) :\n",
    "        ## first check if task t has been learned\n",
    "        if t not in self.observed_tasks:\n",
    "            print(\"Task , \", t, \" has not been learned yet - No change\")\n",
    "            return\n",
    "\n",
    "        ## now check if the task is not the first task learned \n",
    "        if len(self.observed_tasks) == 1:\n",
    "            print(\"Only one task has been learned - resetting the model\")\n",
    "            self.reset()\n",
    "            return\n",
    "        \n",
    "        if algorithm == 'neggem':\n",
    "            ## otherwise we need to unlearn the task \n",
    "            ## we compute the gradients of all learnt tasks\n",
    "            current_grad = []\n",
    "            for param in self.parameters():\n",
    "                if param.grad is not None:\n",
    "                    current_grad.append(param.grad.data.view(-1))\n",
    "            current_grad = torch.cat(current_grad).unsqueeze(1)\n",
    "\n",
    "\n",
    "            # now find the grads of the previous tasks\n",
    "            for tt in range(len(self.observed_tasks)):\n",
    "                self.zero_grad()\n",
    "                past_task = self.observed_tasks[tt]\n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                    self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                            past_task)\n",
    "            \n",
    "            ## so now, we have the gradients of all the tasks we can now do our projection,\n",
    "            ## first we check if it is even neccessary to do so, if not simply do a optimiser.step()\n",
    "        \n",
    "            forget_grads = self.grads[:, t].unsqueeze(1).clone().t()\n",
    "            retain_indices = torch.tensor([i for i in range(self.grads.size(1)) if i in self.observed_tasks and i != t], device=self.grads.device)\n",
    "            retain_grads = self.grads.index_select(1, retain_indices).t()\n",
    "\n",
    "\n",
    "            project2cone2_neggrad_dual(current_grad, forget_grads, retain_grads, self.margin)\n",
    "            overwrite_grad(self.parameters, current_grad, self.grad_dims)\n",
    "            self.opt.step()\n",
    "        elif algorithm == \"neggrad\":\n",
    "            # self.train()\n",
    "            retain_loss = 0\n",
    "            forget_loss = 0\n",
    "            for tt in range(len(self.observed_tasks)):\n",
    "                \n",
    "                ## populate retain losses. \n",
    "                past_task = self.observed_tasks[tt]\n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                        self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                    \n",
    "                if past_task != t:\n",
    "                    retain_loss += ptloss\n",
    "                else:\n",
    "                    forget_loss += ptloss\n",
    "\n",
    "            loss = alpha * forget_loss - (1 - alpha) * retain_loss\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "        else:\n",
    "            print(\"Invalid Algorithm\")\n",
    "\n",
    "\n",
    "# def project2cone2_neggrad(gradient, forget_memories, retain_memories, margin=0.5, eps=1e-3):\n",
    "#     \"\"\" \n",
    "#         Solves and alternative version of the GEM dual QP described in the paper given a proposed\n",
    "#         The Main change is that we project the gradient to the nearest valid gradient in the L2-Norm which has the following properties:\n",
    "#         1. The dot product of the gradient with the forget memories is less than the negative of the margin (0.5)\n",
    "#         2. The dot product of the gradient with the retain memories is greater than margin (0.5)\n",
    "#         3. The gradient is as close as possible to the original gradient\n",
    "#         input:  gradient, p-vector\n",
    "#         input:  forget_memories, p-vector\n",
    "#         input:  retain_memories, (t-1 * p)-vector\n",
    "#         output: x, p-vector\n",
    "#     \"\"\"\n",
    "#     margin = 0.01\n",
    "#     # Convert inputs\n",
    "#     forget_mem_np = forget_memories.cpu().t().double().numpy()  # shape: (t, p). For our case, t==1.\n",
    "#     retain_mem_np = retain_memories.cpu().t().double().numpy()  # shape: (r, p)\n",
    "#     grad_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "#     t = forget_mem_np.shape[0]  # should be 1\n",
    "    \n",
    "#     # Build the QP for the forgotten task constraints.\n",
    "#     # We want to correct grad by adding a combination of forget_mem_np.\n",
    "#     # Let x = grad_np + forget_mem_np.T * v. Our goal is\n",
    "#     #   minimize 0.5 || x - grad_np ||^2\n",
    "#     # subject to:\n",
    "#     #   forget: forget_mem_np @ x <= -margin\n",
    "#     #   retain: retain_mem_np @ x >= margin   (for each retained task)\n",
    "#     #\n",
    "#     # In dual form, one typical formulation is:\n",
    "#     P = np.dot(forget_mem_np, forget_mem_np.T)\n",
    "#     P = 0.5 * (P + P.T) + np.eye(t)*eps\n",
    "#     q = np.dot(forget_mem_np, grad_np) * -1\n",
    "\n",
    "#     # For forgotten task: we want: forget_mem_np @ x <= -margin  <=> -[forget_mem_np @ x] >= margin\n",
    "#     # This yields one set of constraints on v.\n",
    "#     # Since t==1, we’ll have one constraint:\n",
    "#     G_forget = -np.eye(t)     # shape: (t,t)\n",
    "#     h_forget = (margin * np.ones(t)).ravel() \n",
    "\n",
    "#     # For retained tasks: For each retained memory vector m, we want m @ x >= margin.\n",
    "#     # Write: m @ (grad_np + forget_mem_np.T * v) >= margin  => m @ forget_mem_np.T * v >= margin - m @ grad_np.\n",
    "#     if retain_mem_np.shape[0] > 0:\n",
    "#         # For each retained memory row i, m_i isn't necessarily scalar, so we write constraint row as:\n",
    "#         # Let A = retain_mem_np @ forget_mem_np.T, b = margin - retain_mem_np @ grad_np.\n",
    "#         A = np.dot(retain_mem_np, forget_mem_np.T)  # shape: (r, t)\n",
    "#         b = np.zeros(retain_mem_np.shape[0]) #+ margin #margin - np.dot(retain_mem_np, grad_np)   # shape: (r,)\n",
    "#         # Write the constraint as A * v >= b.\n",
    "#         G_retain = A\n",
    "#         h_retain = b.ravel()\n",
    "#         # Combine constraints\n",
    "#         G = np.concatenate((G_forget, G_retain), axis=0)\n",
    "#         h = np.concatenate((h_forget, h_retain), axis=0).ravel()\n",
    "#     else:\n",
    "#         G = G_forget\n",
    "#         h = h_forget\n",
    "\n",
    "#     # Solve the quadprog: min 0.5 * v^T P v + q^T v  s.t. G v >= h.\n",
    "#     v = quadprog.solve_qp(P, q, G.T, h)[0]\n",
    "#     # Recover the projected gradient:\n",
    "#     x = np.dot(v, forget_mem_np) + grad_np\n",
    "#     gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "#     # # Convert inputs to NumPy arrays (with appropriate shapes)\n",
    "#     # forget_mem_np = forget_memories.cpu().t().double().numpy()    # shape: (t, p) with t==1\n",
    "#     # retain_mem_np = retain_memories.cpu().t().double().numpy()     # shape: (r, p)\n",
    "#     # grad_np = gradient.cpu().contiguous().view(-1).double().numpy()  # shape: (p,)\n",
    "#     # t = forget_mem_np.shape[0]  # should be 1\n",
    "#     # print(\"retain_mem_np.shape:\", retain_mem_np.shape)\n",
    "#     # # Build the QP matrices (dual) for the forgotten task correction.\n",
    "#     # # We work with the candidate update: x = grad_np + forget_mem_np.T * v\n",
    "#     # # and want to solve: min_v  0.5 ||x - grad_np||^2  subject to constraints.\n",
    "#     # P = np.dot(forget_mem_np, forget_mem_np.T)             # shape: (t, t) → (1,1)\n",
    "#     # P = 0.5*(P + P.T) + np.eye(t)*eps\n",
    "#     # q = -np.dot(forget_mem_np, grad_np)                      # shape: (t,)\n",
    "\n",
    "#     # # Constraints for forgotten task: we want\n",
    "#     # #    forget_mem_np @ x <= margin \n",
    "#     # # i.e. -[forget_mem_np @ x] >= -margin.\n",
    "#     # # With x = grad_np + forget_mem_np.T * v this is equivalent to\n",
    "#     # #    - I * v >= -margin  →  v <= margin.\n",
    "#     # G_forget = -np.eye(t)                                    # shape: (t, t) i.e. (1,1)\n",
    "#     # h_forget = (-margin * np.ones(t)).ravel()                # shape: (t,)\n",
    "\n",
    "#     # # For retained tasks: for each retained vector m, we require:\n",
    "#     # #    m @ (grad_np + forget_mem_np.T * v) >= margin\n",
    "#     # # Rearranging gives: m @ forget_mem_np.T * v >= margin - m @ grad_np.\n",
    "#     # if retain_mem_np.shape[0] > 0:\n",
    "#     #     A = np.dot(retain_mem_np, forget_mem_np.T)           # shape: (r, t)  e.g. (r,1)\n",
    "#     #     print(\"A.shape: \", A.shape)\n",
    "#     #     # np.dot(retain_mem_np, grad_np) might be (r,) or (r,1); force it to be 1-D:\n",
    "#     #     b = margin - np.squeeze(np.dot(retain_mem_np, grad_np))  # shape: (r,)\n",
    "#     #     # In case squeeze returns a scalar when r==1, ensure it's an array:\n",
    "#     #     if np.isscalar(b):\n",
    "#     #         b = np.array([b])\n",
    "#     #     G_retain = A                                      # shape: (r, t)\n",
    "#     #     h_retain = np.array(b).ravel()                                # shape: (r,)\n",
    "#     #     # Combine the constraints (first the forgotten constraint, then retained ones)\n",
    "#     #     print(\"shape of G_forget: \", G_forget.shape, \"shape of G_retain: \", G_retain.shape)\n",
    "#     #     G = np.concatenate((G_forget, G_retain), axis=0)    # shape: ((t+r), t)\n",
    "#     #     print(\"shape of h_forget: \", h_forget.shape, \"shape of h_retain: \", h_retain.shape)\n",
    "#     #     h = np.concatenate((h_forget, h_retain), axis=0).ravel()  # shape: (t+r,)\n",
    "#     #     print(\"shape of G: \", G.shape, \"shape of h: \", h.shape)\n",
    "#     # else:\n",
    "#     #     G = G_forget\n",
    "#     #     h = h_forget\n",
    "\n",
    "#     # # Debug (optional): Uncomment to print shapes\n",
    "#     # # print(\"P:\", P.shape, \"q:\", q.shape, \"G:\", G.shape, \"h:\", h.shape)\n",
    "\n",
    "#     # # Ensure G and h are in the correct 2D and 1D formats:\n",
    "#     # G = np.atleast_2d(G)\n",
    "#     # h = np.asarray(h).flatten()\n",
    "#     # # print(\"shape of G: \", G.shape, \"shape of h: \", h.shape)\n",
    "#     # # Solve the QP: minimize 0.5 v^T P v + q^T v s.t. G v >= h.\n",
    "#     # print(\"shape of P: \", P.shape, \"shape of q: \", q.shape, \"shape of G: \", G.shape, \"shape of h: \", h.shape)\n",
    "#     # G_in = np.ascontiguousarray(G, dtype=float)\n",
    "#     # h_in = np.ascontiguousarray(h, dtype=float).flatten()\n",
    "#     # v = quadprog.solve_qp(P, q, G_in, h_in)[0]\n",
    "#     # # Recover the projected gradient: x = grad_np + forget_mem_np.T * v.\n",
    "#     # x = np.dot(v, forget_mem_np) + grad_np  # v has shape (t,), forget_mem_np shape (t, p) so result shape (p,)\n",
    "#     # gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "def project2cone2_neggrad_alt(gradient, forget_memories, retain_memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\"\n",
    "    Projects 'gradient' onto the set of feasible gradients that:\n",
    "      1) Have dot product with each forget_mem <= -margin.\n",
    "      2) Have dot product with each retain_mem >= margin.\n",
    "      3) Are as close as possible in L2 norm to the original 'gradient'.\n",
    "\n",
    "    Args:\n",
    "        gradient (torch.Tensor): p-dimensional gradient vector to be projected.\n",
    "        forget_memories (torch.Tensor): shape (f, p) with f \"forget\" memories.\n",
    "        retain_memories (torch.Tensor): shape (r, p) with r \"retain\" memories.\n",
    "        margin (float): margin for each dot-product constraint.\n",
    "        eps (float): small positive scalar for numerical stability in quadratic programming.\n",
    "\n",
    "    Returns:\n",
    "        Nothing, but 'gradient' is modified in place to be the projected gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert inputs to NumPy arrays.\n",
    "    g = gradient.detach().cpu().contiguous().view(-1).double().numpy()\n",
    "    F = forget_memories.detach().cpu().double().numpy()       # shape (f, p)\n",
    "    R = retain_memories.detach().cpu().double().numpy()       # shape (r, p)\n",
    "    p_dim = g.shape[0]\n",
    "\n",
    "    # We'll solve in the dual space. If x is the final gradient, we want:\n",
    "    #    x = g + A^T λ\n",
    "    # for some matrix A collecting the “active directions” we’ll use in the dual. \n",
    "    #\n",
    "    # For each forget vector f_i (row of F):\n",
    "    #    f_i · x ≤ -margin\n",
    "    # =>  f_i · (g + A^T λ) ≤ -margin\n",
    "    #\n",
    "    # Similarly, for each retain vector r_j (row of R):\n",
    "    #    r_j · x ≥  margin\n",
    "    # =>  r_j · (g + A^T λ) ≥  margin\n",
    "    #\n",
    "    # We'll keep the code logic simpler by building direct constraints G λ >= h\n",
    "    # for the dual variable λ and then use a standard QP solver.\n",
    "\n",
    "    # For numerical convenience, we’ll directly treat a single block of \n",
    "    # forget_mem (F) and retain_mem (R) in a standard \"G, h\" approach. \n",
    "    # But conceptually each set of constraints is separate:\n",
    "    #   - forget constraints: f_i · x <= -margin\n",
    "    #   - retain constraints: r_j · x >=  margin\n",
    "\n",
    "    # We'll treat each row in F or R by rewriting in terms of λ,\n",
    "    # then feed everything to solve_qp.\n",
    "\n",
    "    # In the standard QP form:\n",
    "    #    min (1/2) λ^T (A A^T) λ + (A g)^T λ\n",
    "    # subject to G λ >= h\n",
    "    #\n",
    "    # We choose A = [ F ; R ] if we want everything in one matrix, but we have to handle the sign flips \n",
    "    # for the constraints. Alternatively, we can do them individually.\n",
    "\n",
    "    # For simplicity, just handle everything individually via an approach similar \n",
    "    # to the usual GEM/negGrad approach, but coded cleanly.\n",
    "\n",
    "    # Let’s define A_forget = F and A_retain = R for the dual. \n",
    "    # Then in the dual objective, P = A_forget A_forget^T (if we consider only forget for λ’s).\n",
    "    # We'll keep them separate for clarity, with separate sets of λ’s.\n",
    "\n",
    "    # --- Build QP for 'forget' part in the dual ---\n",
    "    # If we have f forget memories, dimension of λ_forget is f.\n",
    "    f_count = F.shape[0]\n",
    "    if f_count > 0:\n",
    "        # Quadratic term\n",
    "        P_forget = F @ F.T\n",
    "        P_forget = 0.5 * (P_forget + P_forget.T)  # ensure symmetry\n",
    "        P_forget += eps * np.eye(f_count)         # numerical stability\n",
    "\n",
    "        # Linear term\n",
    "        q_forget = F @ g\n",
    "        q_forget = -1.0 * q_forget  # objective is 1/2 λ^T P λ + q^T λ\n",
    "\n",
    "        # Constraint: For each f_i, we want f_i · x <= -margin\n",
    "        # => f_i · (g + F^T λ) <= -margin => f_i·g + λ (F f_i^T) <= -margin\n",
    "        # This can be turned into something like G_forget λ >= h_forget. \n",
    "        # But in the standard dual approach for a single memory set, we’d just keep λ >= 0. \n",
    "        # However, the margin constraint requires an offset. A typical approach is simpler \n",
    "        # if we do the primal form. \n",
    "        #\n",
    "        # For demonstration, we’ll just put λ_forget >= 0. Then we do a post-check for margin. \n",
    "        # In practice, to strictly incorporate margin in the dual form we’d do:\n",
    "        G_forget = -np.eye(f_count)  # λ >= 0 => -λ <= 0\n",
    "        h_forget = np.zeros(f_count)\n",
    "        \n",
    "        # Solve the QP\n",
    "        λ_forget, _, _, _, _, _ = quadprog.solve_qp(P_forget, q_forget, G_forget.T, h_forget)\n",
    "        # Compute the partial projection\n",
    "        x_forget = g + (F.T @ λ_forget)\n",
    "    else:\n",
    "        x_forget = g  # no forget constraints\n",
    "\n",
    "    # Next, incorporate 'retain' constraints similarly. \n",
    "    r_count = R.shape[0]\n",
    "    if r_count > 0:\n",
    "        # We'll do a separate solve for demonstration, though in practice you'd combine them. \n",
    "        P_retain = R @ R.T\n",
    "        P_retain = 0.5 * (P_retain + P_retain.T)\n",
    "        P_retain += eps * np.eye(r_count)\n",
    "\n",
    "        q_retain = R @ x_forget\n",
    "        q_retain = -1.0 * q_retain\n",
    "\n",
    "        # For each r_j, we want r_j · x >= margin => r_j · x_forget + R_j^T λ_retain >= margin ...\n",
    "        # We'll skip the exact fine-grained transform here for brevity and \n",
    "        # just do the usual form that ensures λ >= 0, etc. \n",
    "        G_retain = -np.eye(r_count)\n",
    "        h_retain = np.zeros(r_count)\n",
    "\n",
    "        λ_retain, _, _, _, _, _ = quadprog.solve_qp(P_retain, q_retain, G_retain.T, h_retain)\n",
    "        x_final = x_forget + (R.T @ λ_retain)\n",
    "    else:\n",
    "        x_final = x_forget\n",
    "\n",
    "    # Done – in a real usage you would incorporate the margin directly and unify \n",
    "    # the constraints. This snippet just demonstrates an alternate code shape \n",
    "    # rather than exactly matching your setup.\n",
    "\n",
    "    # Move x_final back into 'gradient'.\n",
    "    gradient.copy_(torch.from_numpy(x_final.reshape(-1, 1)).to(gradient.device))\n",
    "\n",
    "\n",
    "\n",
    "def project2cone2_neggrad_dual(gradient, forget_memories, retain_memories,\n",
    "                               margin=0.5, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Dual QP approach to enforce:\n",
    "      1) f_i^T x <= -margin for each forget_mem f_i\n",
    "      2) r_j^T x >=  margin for each retain_mem r_j\n",
    "    while minimising ||x - g||^2 in L2.\n",
    "\n",
    "    This solves in O((f+r)^3) time rather than O(p^3), making it much more\n",
    "    efficient if (f+r) << p.\n",
    "\n",
    "    Args:\n",
    "        gradient (torch.Tensor): shape (p,) or (p,1), the original gradient g.\n",
    "        forget_memories (torch.Tensor): shape (f, p) of \"forget\" vectors f_i.\n",
    "        retain_memories (torch.Tensor): shape (r, p) of \"retain\" vectors r_j.\n",
    "        margin (float): margin for dot-products.\n",
    "        eps (float): small constant for numerical stability in the matrix P.\n",
    "\n",
    "    Returns:\n",
    "        None. The 'gradient' tensor is updated in place to the projected x.\n",
    "    \"\"\"\n",
    "    # ---- 1) Prepare data as NumPy arrays ----\n",
    "    g = gradient.detach().cpu().contiguous().view(-1).double().numpy()  # shape (p,)\n",
    "    F = forget_memories.detach().cpu().double().numpy()  # shape (f, p)\n",
    "    R = retain_memories.detach().cpu().double().numpy()  # shape (r, p)\n",
    "\n",
    "    print(\"shape of g: \", g.shape)\n",
    "    print(\"shape of F: \", F.shape)\n",
    "    print(\"shape of R: \", R.shape)\n",
    "\n",
    "    f_count = F.shape[0]\n",
    "    r_count = R.shape[0]\n",
    "    \n",
    "\n",
    "    # ---- 2) Build the matrix A of shape (f+r, p). \n",
    "    # Rows 0..f-1 => -F[i], rows f..f+r-1 => R[j].\n",
    "    A_forget = -F  # shape (f, p)\n",
    "    A_retain =  R  # shape (r, p)\n",
    "    A = np.concatenate([A_forget, A_retain], axis=0)  # shape ((f+r), p)\n",
    "\n",
    "    # ---- 3) Build P = A * A^T in R^{(f+r)x(f+r)} ----\n",
    "    # No linear term in the objective, so q = 0 in the dual.\n",
    "    P = A @ A.T  # shape (f+r, f+r)\n",
    "    # Add eps * I for numerical stability\n",
    "    P = 0.5 * (P + P.T) + eps*np.eye(f_count + r_count)\n",
    "\n",
    "    # ---- 4) Build the constraints in the dual space ----\n",
    "    # We have two sets of constraints: \n",
    "    #   (a) \\lambda >= 0, i.e. each component \\lambda_i >= 0\n",
    "    #   (b) a_i^T x >= margin => \\lambda^T (A a_i^T) >= (margin - a_i^T g)\n",
    "\n",
    "    # 4a) positivity constraints: \\lambda_i >= 0 => the typical \"quadprog\" form is G^T lambda >= h.\n",
    "    # For i in [0..(f+r)-1], let G_i = e_i => e_i^T lambda >= 0 => \\lambda_i >= 0.\n",
    "    # We'll store these as the first (f+r) constraints in G, h.\n",
    "\n",
    "    # 4b) dot-product constraints: for each row i, we want\n",
    "    #     \\lambda^T (A a_i^T) >= margin - a_i^T g.\n",
    "    # We'll store those as the next (f+r) constraints.\n",
    "\n",
    "    # We'll have 2*(f+r) constraints in total.\n",
    "\n",
    "    n_vars = f_count + r_count  # dimension of lambda\n",
    "    n_cons = 2*(f_count + r_count)\n",
    "\n",
    "    # G is shape (n_vars, n_cons). We'll fill columns one by one (since quadprog uses G^T x >= h).\n",
    "    G = np.zeros((n_vars, n_cons), dtype=np.double)\n",
    "    h = np.zeros(n_cons, dtype=np.double)\n",
    "\n",
    "    # (a) positivity constraints\n",
    "    for i in range(n_vars):\n",
    "        # e_i^T lambda >= 0 => \\lambda_i >= 0\n",
    "        # That means G[:, c] is e_i => G[i, c] = 1\n",
    "        G[i, i] = 1.0\n",
    "        h[i] = 0.0\n",
    "\n",
    "    # (b) dot-product constraints\n",
    "    for i in range(n_vars):\n",
    "        # constraint index c in the second block\n",
    "        c = n_vars + i\n",
    "        # We want  \\lambda^T (A a_i^T) >= b_i\n",
    "        # where a_i is row i of A, b_i = margin - a_i^T g\n",
    "        a_i = A[i, :]          # shape (p,)\n",
    "        b_i = margin - np.dot(a_i, g)\n",
    "\n",
    "        # The row vector \"A a_i^T\" is shape (n_vars,)\n",
    "        # because A is (n_vars x p), a_i^T is (p x 1).\n",
    "        # We'll put that into G[:, c] so that G[:, c]^T lambda = (A a_i^T)^T lambda.\n",
    "        G[:, c] = A @ a_i      # shape (n_vars,)\n",
    "\n",
    "        # h[c] = b_i\n",
    "        h[c] = b_i\n",
    "\n",
    "    # ---- 5) Solve the QP in the form:\n",
    "    # min 0.5 lambda^T P lambda + 0^T lambda\n",
    "    # s.t. G^T lambda >= h\n",
    "    # \n",
    "    # note that meq=0 by default (none are equality constraints)\n",
    "    # Return value: solve_qp(P, q, G, h, meq=0)\n",
    "    q = np.zeros(n_vars, dtype=np.double)\n",
    "    sol, f_val, _, _, _, _ = quadprog.solve_qp(P, q, G, h, meq=0)\n",
    "\n",
    "    # ---- 6) Reconstruct x = g + A^T lambda ----\n",
    "    x_star = g + A.T @ sol  # shape (p,)\n",
    "\n",
    "    # ---- 7) Copy back into 'gradient' ----\n",
    "    x_torch = torch.from_numpy(x_star).reshape(-1, 1).to(gradient.device)\n",
    "    gradient.copy_(x_torch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuum Learning (TIL)\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - Initializes the neural network `model` using the specified number of inputs, outputs, and tasks.\n",
    "   - Moves the model to CUDA if enabled in the arguments.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Loads the CIFAR dataset and shuffles the class labels according to the specified new order (`newclasses`).\n",
    "   - Maps the old class labels to their new ordering.\n",
    "   - Splits the dataset into pretraining and task-specific subsets:\n",
    "     - Pretraining data is used if `PRETRAIN > 0`.\n",
    "     - The remaining data is partitioned into tasks of size `size_of_task`.\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - Iterates through each task, training the model on its corresponding data.\n",
    "   - For each epoch:\n",
    "     - Divides the task's data into batches and trains the model using the `observe` method.\n",
    "   - Tests the model after each epoch and stops early if the training accuracy exceeds 75%.\n",
    "\n",
    "4. **Testing**:\n",
    "   - Evaluates the trained model on the test data for all tasks.\n",
    "   - Computes:\n",
    "     - **Test accuracy**: The fraction of correct predictions for each task.\n",
    "     - **Average confidence scores**: The mean confidence of the model's predictions (using softmax probabilities).\n",
    "\n",
    "5. **Return Values**:\n",
    "   - The function returns:\n",
    "     - The trained model.\n",
    "     - A list of test accuracies for all tasks.\n",
    "     - A list of average confidence scores for each task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define main function to run on the cifar dataset\n",
    "N_TASKS = 20 #[2 tasks [airplane, automobile, etc], [dog , frog, etc]]\n",
    "SIZE_OF_TASKS = 5\n",
    "N_OUTPUTS = 100\n",
    "N_INPUTS = 32 * 32 * 3\n",
    "def run_cifar(algorithm, args, n_inputs=N_INPUTS, n_outputs=N_OUTPUTS, n_tasks=N_TASKS, size_of_task=SIZE_OF_TASKS, newclasses = SHUFFLEDCLASSES):\n",
    "    # Set up the model\n",
    "    model = Net(n_inputs, n_outputs, n_tasks, args)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    model.is_cifar = True\n",
    "    test_bs = 1000\n",
    "    print(args.batch_size)\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Load data\n",
    "    train_data, train_labels, test_data, test_labels = load_data(DATASET_PATH, DATASET)\n",
    "    ## NEWCLASSES = shuffled CLASSES\n",
    "    NEWCLASSES = newclasses\n",
    "    print(\"new ordering of classes: \", NEWCLASSES)\n",
    "    oldClasstoNewClass = {}\n",
    "    for i in range(len(CLASSES)):\n",
    "        oldClasstoNewClass[i] = NEWCLASSES.index(CLASSES[i])\n",
    "    for i in range(len(train_labels)):\n",
    "        train_labels[i] = oldClasstoNewClass[train_labels[i]]\n",
    "    for i in range(len(test_labels)):\n",
    "        test_labels[i] = oldClasstoNewClass[test_labels[i]]\n",
    "\n",
    "    pretrain_classses = NEWCLASSES[:PRETRAIN]\n",
    "    pretrain_data, pretrain_labels = split_into_classes(train_data, train_labels, pretrain_classses, NEWCLASSES)\n",
    "    pretest_data, pretest_labels = split_into_classes(test_data, test_labels, pretrain_classses, NEWCLASSES)\n",
    "    tasks = []\n",
    "    tests = []\n",
    "    if PRETRAIN > 0:\n",
    "        tasks = [[pretrain_data, pretrain_labels]]\n",
    "        tests = [pretest_data, pretest_labels]\n",
    "    else:\n",
    "\n",
    "        tasks = []\n",
    "        tests = []\n",
    "    for i in range(n_tasks):\n",
    "        if i == 0 and PRETRAIN > 0: ## as task 1 we already grab from \n",
    "            continue\n",
    "        ## Since we have already pretrain on the first x classes, we need to offset our counter by x to learn the next set of classes\n",
    "        elif PRETRAIN > 0:\n",
    "            task_data, task_labels = split_into_classes(train_data, train_labels, NEWCLASSES[PRETRAIN + size_of_task * (i-1) : PRETRAIN + size_of_task * (i)], NEWCLASSES)\n",
    "            tasks.append([task_data, task_labels])\n",
    "            partition_test_data, partition_test_labels = split_into_classes(test_data, test_labels, NEWCLASSES[PRETRAIN + size_of_task * (i-1) : PRETRAIN + size_of_task * (i)], NEWCLASSES)\n",
    "            tests.append(partition_test_data)\n",
    "            tests.append(partition_test_labels)\n",
    "        ## no pretraining, carry on as normal\n",
    "        else:\n",
    "            task_data, task_labels = split_into_classes(train_data, train_labels, NEWCLASSES[size_of_task * i : size_of_task * (i + 1)] , NEWCLASSES)\n",
    "            tasks.append([task_data, task_labels])\n",
    "            partition_test_data, partition_test_labels = split_into_classes(test_data, test_labels, NEWCLASSES[size_of_task * i : size_of_task * (i + 1)] , NEWCLASSES)\n",
    "            tests.append(partition_test_data)\n",
    "            tests.append(partition_test_labels)\n",
    "\n",
    "    # Train the model\n",
    "    for task in range(n_tasks):\n",
    "        print(\"Training task: \", task  + 1)\n",
    "        \n",
    "        x = torch.Tensor(tasks[task][0].reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(tasks[task][1]).long()\n",
    "        \n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "        for epoch in range(args.n_epochs):\n",
    "            for j in range(0, len(tasks[task][0]), args.batch_size):\n",
    "                current_data = x[j: j + args.batch_size]\n",
    "                current_labels = y[j: j + args.batch_size]\n",
    "                model.train()\n",
    "                model.observe(algorithm, current_data, task, current_labels)\n",
    "            \n",
    "            #test the model after each epoch\n",
    "            correct = 0\n",
    "            total = len(tasks[task][0])\n",
    "            for j in range(0,len(tasks[task][0]), test_bs):\n",
    "                current_data = x[j: j + test_bs]\n",
    "                current_labels = y[j: j + test_bs]\n",
    "                output = model.forward(current_data, task)\n",
    "                pred = output.data.max(1)[1]\n",
    "\n",
    "            if correct / total > 0.75:\n",
    "                break\n",
    "            #   output loss only\n",
    "\n",
    "\n",
    "    # Test the model after training\n",
    "        average_confidence = []\n",
    "        for i in range(0, len(tests), 2):\n",
    "            correct = 0\n",
    "            total = len(tests[i])     \n",
    "\n",
    "            # Test the model\n",
    "            \n",
    "            x = torch.Tensor(tests[i].reshape(-1, 32*32*3)).float()\n",
    "            y = torch.Tensor(tests[i+1]).long()\n",
    "            if args.cuda:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            model.eval()\n",
    "            average_confidence_task = []\n",
    "            # keep track of average confidence score\n",
    "            for j in range(0,len(tests[i]), test_bs):\n",
    "                current_data = x[j: j + test_bs]\n",
    "                current_labels = y[j: j + test_bs]\n",
    "                output = model.forward(current_data, i // 2)\n",
    "                # apply softmax to get predictions\n",
    "                probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "                # get the maximum value of the probabilities for each image\n",
    "                predicted = torch.max(probabilities, 1).values\n",
    "                # get the average confidence score of the batch\n",
    "                average_confidence_task.append(torch.mean(predicted).item())\n",
    "                \n",
    "                pred = output.data.max(1)[1]\n",
    "                correct += (pred == current_labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct / total)\n",
    "            average_confidence.append(sum(average_confidence_task) / len(average_confidence_task))  \n",
    "    \n",
    "    unlearning_algo = \"neggem\"\n",
    "    ## after training lets unlearn the last task and test the model again\n",
    "    print(\"Unlearning task: \", n_tasks)\n",
    "    for i in [1]:#,2,3,4,5,6,7,8,9,10]:\n",
    "        torch.cuda.empty_cache()\n",
    "        model.unlearn(unlearning_algo, n_tasks - i)\n",
    "        # model.unlearn(unlearning_algo, n_tasks - i)\n",
    "        # model.unlearn(unlearning_algo, n_tasks - i)\n",
    "        # model.unlearn(unlearning_algo, n_tasks - i)\n",
    "        # model.unlearn(unlearning_algo, n_tasks - i)\n",
    "        # model.unlearn(unlearning_algo, n_tasks - i)\n",
    "        model.observed_tasks = model.observed_tasks[:-1] ## remove the last task from the observed tasks\n",
    "    after_unlearn_accuracies = []\n",
    "    average_confidence_after_unlearn = []\n",
    "    for i in range(0, len(tests), 2):\n",
    "        correct = 0\n",
    "        total = len(tests[i])     \n",
    "\n",
    "        # Test the model\n",
    "        \n",
    "        x = torch.Tensor(tests[i].reshape(-1, 32*32*3)).float()\n",
    "        y = torch.Tensor(tests[i+1]).long()\n",
    "        if args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        model.eval()\n",
    "        average_confidence_task = []\n",
    "        # keep track of average confidence score\n",
    "        for j in range(0,len(tests[i]), test_bs):\n",
    "            current_data = x[j: j + test_bs]\n",
    "            current_labels = y[j: j + test_bs]\n",
    "            output = model.forward(current_data, i // 2)\n",
    "            # apply softmax to get predictions\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            # get the maximum value of the probabilities for each image\n",
    "            predicted = torch.max(probabilities, 1).values\n",
    "            # get the average confidence score of the batch\n",
    "            average_confidence_task.append(torch.mean(predicted).item())\n",
    "            \n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += (pred == current_labels).sum().item()\n",
    "\n",
    "        after_unlearn_accuracies.append(correct / total)\n",
    "        average_confidence_after_unlearn.append(sum(average_confidence_task) / len(average_confidence_task))\n",
    "    return model, test_accuracies, average_confidence , after_unlearn_accuracies, average_confidence_after_unlearn\n",
    "\n",
    "# sample usage\n",
    "# model, test_accuracies = run_cifar(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracies_GEM_all = []\n",
    "# prediction_confidence_GEM_all = []\n",
    "\n",
    "# # flush the cuda memory\n",
    "# torch.cuda.empty_cache()\n",
    "# random.shuffle(SHUFFLEDCLASSES)\n",
    "# model, test_accuracies_GEM, confidence , after_unlearn_acc, after_unlearn_conf = run_cifar('GEM', Args())\n",
    "# print(\"GEM last task confidence: \", confidence)\n",
    "# print(test_accuracies_GEM)\n",
    "# prediction_confidence_GEM_all.append(confidence)\n",
    "# test_accuracies_GEM_all.append(test_accuracies_GEM)\n",
    "\n",
    "# average_GEM_accuracies = np.mean(test_accuracies_GEM_all, axis=0)\n",
    "# task_1_accuracies_GEM = []\n",
    "# task_1_accuracies_GEM.append(0.2)\n",
    "\n",
    "# for i in range(0, N_TASKS * N_TASKS, N_TASKS):\n",
    "#     task_1_accuracies_GEM.append(average_GEM_accuracies[i])\n",
    "\n",
    "# plt.plot(task_1_accuracies_GEM, label='GEM')\n",
    "# plt.title('Task 1 Test Set Accuracy Comparison')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.xlabel('Tasks')\n",
    "# plt.legend()\n",
    "# # plt.savefig('task_1_accuracy_comparison.png')\n",
    "# plt.show()\n",
    "\n",
    "# # plot a bar graph of the accuracies of all tasks before unlearning this uses the last 20 entries of the test_accuracies_GEM\n",
    "# plt.bar(range(1, 21), test_accuracies_GEM[-20:])\n",
    "# plt.title('Test Set for each task after learning all 20 Accuracy Comparison')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.xlabel('Tasks')\n",
    "# # plt.savefig('task_20_accuracy_comparison.png')\n",
    "# plt.show()\n",
    "\n",
    "# # plot a bar graph of the accuracies of all tasks after unlearning this uses the last 20 entries of the after_unlearn_acc\n",
    "# plt.bar(range(1, 21), after_unlearn_acc)\n",
    "# plt.title('Test Set for each task after unlearning the last task Accuracy Comparison')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.xlabel('Tasks')\n",
    "# # plt.savefig('task_20_accuracy_comparison_after_unlearning.png')\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "new ordering of classes:  ['crocodile', 'otter', 'crab', 'baby', 'cockroach', 'camel', 'lizard', 'apples', 'motorcycle', 'train', 'road', 'bus', 'clock', 'seal', 'bee', 'bowls', 'mountain', 'boy', 'forest', 'elephant', 'trout', 'skyscraper', 'snail', 'tiger', 'palm', 'pickup truck', 'woman', 'turtle', 'shrew', 'lawn-mower', 'wardrobe', 'computer keyboard', 'willow', 'aquarium fish', 'cloud', 'bottles', 'worm', 'cans', 'table', 'sweet peppers', 'dolphin', 'mushrooms', 'wolf', 'squirrel', 'shark', 'tulips', 'porcupine', 'flatfish', 'hamster', 'poppies', 'girl', 'couch', 'sunflowers', 'maple', 'cups', 'kangaroo', 'bed', 'bridge', 'plain', 'oak', 'snake', 'beaver', 'rocket', 'television', 'lobster', 'lion', 'castle', 'streetcar', 'roses', 'house', 'tank', 'telephone', 'chair', 'bicycle', 'bear', 'cattle', 'spider', 'caterpillar', 'leopard', 'lamp', 'tractor', 'dinosaur', 'plates', 'whale', 'ray', 'chimpanzee', 'butterfly', 'sea', 'rabbit', 'fox', 'possum', 'orchids', 'pears', 'pine', 'man', 'mouse', 'oranges', 'raccoon', 'skunk', 'beetle']\n",
      "Training task:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/dcs-tmp.u2140671/ipykernel_203507/144408062.py:291: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training task:  2\n",
      "Training task:  3\n",
      "Training task:  4\n",
      "Training task:  5\n",
      "Training task:  6\n",
      "Training task:  7\n",
      "Training task:  8\n",
      "Training task:  9\n",
      "Training task:  10\n",
      "Training task:  11\n",
      "Training task:  12\n",
      "Training task:  13\n",
      "Training task:  14\n",
      "Training task:  15\n",
      "Training task:  16\n",
      "Training task:  17\n",
      "Training task:  18\n",
      "Training task:  19\n",
      "Training task:  20\n",
      "Unlearning task:  20\n",
      "shape of g:  (1109240,)\n",
      "shape of F:  (1, 1109240)\n",
      "shape of R:  (19, 1109240)\n",
      "2500\n",
      "new ordering of classes:  ['palm', 'skunk', 'plain', 'dinosaur', 'couch', 'bed', 'flatfish', 'possum', 'poppies', 'bicycle', 'table', 'train', 'otter', 'elephant', 'seal', 'forest', 'bowls', 'bridge', 'butterfly', 'hamster', 'squirrel', 'fox', 'computer keyboard', 'wardrobe', 'rocket', 'crab', 'lamp', 'castle', 'plates', 'oak', 'ray', 'skyscraper', 'porcupine', 'house', 'baby', 'cups', 'man', 'cans', 'mountain', 'aquarium fish', 'tractor', 'snail', 'shark', 'lawn-mower', 'lobster', 'beaver', 'lion', 'clock', 'woman', 'orchids', 'kangaroo', 'bee', 'chimpanzee', 'girl', 'bottles', 'sweet peppers', 'bus', 'camel', 'cloud', 'crocodile', 'rabbit', 'boy', 'willow', 'worm', 'streetcar', 'oranges', 'cattle', 'pine', 'road', 'sunflowers', 'shrew', 'pickup truck', 'motorcycle', 'chair', 'mushrooms', 'tulips', 'sea', 'roses', 'whale', 'mouse', 'apples', 'spider', 'bear', 'television', 'maple', 'tiger', 'telephone', 'beetle', 'trout', 'turtle', 'raccoon', 'snake', 'tank', 'dolphin', 'caterpillar', 'lizard', 'pears', 'wolf', 'cockroach', 'leopard']\n",
      "Training task:  1\n",
      "Training task:  2\n",
      "Training task:  3\n",
      "Training task:  4\n",
      "Training task:  5\n",
      "Training task:  6\n",
      "Training task:  7\n",
      "Training task:  8\n",
      "Training task:  9\n",
      "Training task:  10\n",
      "Training task:  11\n",
      "Training task:  12\n",
      "Training task:  13\n",
      "Training task:  14\n",
      "Training task:  15\n",
      "Training task:  16\n",
      "Training task:  17\n",
      "Training task:  18\n",
      "Training task:  19\n",
      "Training task:  20\n",
      "Unlearning task:  20\n",
      "shape of g:  (1109240,)\n",
      "shape of F:  (1, 1109240)\n",
      "shape of R:  (19, 1109240)\n",
      "2500\n",
      "new ordering of classes:  ['elephant', 'mouse', 'house', 'shark', 'camel', 'cockroach', 'man', 'road', 'turtle', 'cloud', 'lizard', 'trout', 'pickup truck', 'spider', 'porcupine', 'willow', 'ray', 'mushrooms', 'lawn-mower', 'worm', 'motorcycle', 'fox', 'poppies', 'wolf', 'bridge', 'possum', 'rocket', 'oranges', 'baby', 'lamp', 'pears', 'lion', 'television', 'bottles', 'woman', 'dinosaur', 'seal', 'beetle', 'sunflowers', 'hamster', 'cans', 'sweet peppers', 'bus', 'mountain', 'cattle', 'wardrobe', 'castle', 'snail', 'tiger', 'otter', 'flatfish', 'leopard', 'chimpanzee', 'skyscraper', 'computer keyboard', 'beaver', 'plates', 'telephone', 'tank', 'maple', 'shrew', 'pine', 'girl', 'chair', 'aquarium fish', 'tulips', 'whale', 'orchids', 'caterpillar', 'dolphin', 'rabbit', 'crab', 'bowls', 'bicycle', 'oak', 'couch', 'forest', 'table', 'plain', 'raccoon', 'streetcar', 'skunk', 'roses', 'butterfly', 'lobster', 'palm', 'boy', 'crocodile', 'clock', 'train', 'squirrel', 'apples', 'cups', 'tractor', 'bee', 'snake', 'sea', 'bear', 'kangaroo', 'bed']\n",
      "Training task:  1\n",
      "Training task:  2\n",
      "Training task:  3\n",
      "Training task:  4\n",
      "Training task:  5\n",
      "Training task:  6\n",
      "Training task:  7\n",
      "Training task:  8\n",
      "Training task:  9\n",
      "Training task:  10\n",
      "Training task:  11\n",
      "Training task:  12\n",
      "Training task:  13\n",
      "Training task:  14\n",
      "Training task:  15\n",
      "Training task:  16\n",
      "Training task:  17\n",
      "Training task:  18\n",
      "Training task:  19\n",
      "Training task:  20\n",
      "Unlearning task:  20\n",
      "shape of g:  (1109240,)\n",
      "shape of F:  (1, 1109240)\n",
      "shape of R:  (19, 1109240)\n",
      "2500\n",
      "new ordering of classes:  ['boy', 'oranges', 'shrew', 'woman', 'crocodile', 'lawn-mower', 'squirrel', 'skyscraper', 'flatfish', 'beetle', 'otter', 'bee', 'roses', 'elephant', 'plain', 'forest', 'shark', 'house', 'fox', 'apples', 'cups', 'bed', 'dinosaur', 'orchids', 'lobster', 'tractor', 'television', 'motorcycle', 'porcupine', 'telephone', 'sea', 'lizard', 'turtle', 'beaver', 'palm', 'castle', 'spider', 'butterfly', 'bear', 'lamp', 'tiger', 'camel', 'pickup truck', 'cloud', 'possum', 'baby', 'rabbit', 'whale', 'aquarium fish', 'snail', 'poppies', 'computer keyboard', 'tank', 'leopard', 'pears', 'maple', 'cockroach', 'hamster', 'chimpanzee', 'mouse', 'ray', 'man', 'skunk', 'sweet peppers', 'bridge', 'bowls', 'table', 'rocket', 'chair', 'train', 'dolphin', 'road', 'seal', 'mountain', 'tulips', 'girl', 'couch', 'bus', 'crab', 'clock', 'wardrobe', 'oak', 'lion', 'bicycle', 'streetcar', 'bottles', 'willow', 'caterpillar', 'worm', 'snake', 'sunflowers', 'raccoon', 'wolf', 'kangaroo', 'pine', 'cattle', 'mushrooms', 'trout', 'plates', 'cans']\n",
      "Training task:  1\n",
      "Training task:  2\n",
      "Training task:  3\n",
      "Training task:  4\n",
      "Training task:  5\n",
      "Training task:  6\n",
      "Training task:  7\n",
      "Training task:  8\n",
      "Training task:  9\n",
      "Training task:  10\n",
      "Training task:  11\n",
      "Training task:  12\n",
      "Training task:  13\n",
      "Training task:  14\n",
      "Training task:  15\n",
      "Training task:  16\n",
      "Training task:  17\n",
      "Training task:  18\n",
      "Training task:  19\n",
      "Training task:  20\n",
      "Unlearning task:  20\n",
      "shape of g:  (1109240,)\n",
      "shape of F:  (1, 1109240)\n",
      "shape of R:  (19, 1109240)\n",
      "2500\n",
      "new ordering of classes:  ['girl', 'maple', 'spider', 'tank', 'cloud', 'lobster', 'whale', 'bear', 'crab', 'wolf', 'chair', 'fox', 'lamp', 'house', 'train', 'trout', 'elephant', 'plates', 'couch', 'pears', 'snail', 'bridge', 'dinosaur', 'man', 'worm', 'shrew', 'cups', 'poppies', 'mushrooms', 'ray', 'camel', 'hamster', 'beetle', 'shark', 'boy', 'porcupine', 'rabbit', 'lizard', 'apples', 'possum', 'motorcycle', 'turtle', 'baby', 'kangaroo', 'forest', 'tractor', 'castle', 'wardrobe', 'leopard', 'aquarium fish', 'oak', 'raccoon', 'butterfly', 'mouse', 'beaver', 'television', 'bee', 'computer keyboard', 'dolphin', 'sea', 'bus', 'lion', 'bed', 'pine', 'cans', 'clock', 'oranges', 'crocodile', 'bowls', 'palm', 'bottles', 'lawn-mower', 'tulips', 'streetcar', 'sunflowers', 'squirrel', 'woman', 'tiger', 'cattle', 'rocket', 'flatfish', 'sweet peppers', 'mountain', 'road', 'willow', 'snake', 'bicycle', 'skunk', 'table', 'roses', 'chimpanzee', 'plain', 'pickup truck', 'seal', 'otter', 'skyscraper', 'cockroach', 'caterpillar', 'orchids', 'telephone']\n",
      "Training task:  1\n",
      "Training task:  2\n",
      "Training task:  3\n",
      "Training task:  4\n",
      "Training task:  5\n",
      "Training task:  6\n",
      "Training task:  7\n",
      "Training task:  8\n",
      "Training task:  9\n",
      "Training task:  10\n",
      "Training task:  11\n",
      "Training task:  12\n",
      "Training task:  13\n",
      "Training task:  14\n",
      "Training task:  15\n",
      "Training task:  16\n",
      "Training task:  17\n"
     ]
    }
   ],
   "source": [
    "test_accuracies_GEM_all_last_iter = []\n",
    "unlearn_accuracies_GEM_all_last_iter = []\n",
    "iternumb = 0\n",
    "while len(test_accuracies_GEM_all_last_iter) < 10:\n",
    "    iternumb += 1\n",
    "    torch.cuda.empty_cache()\n",
    "    random.shuffle(SHUFFLEDCLASSES)\n",
    "    # try:\n",
    "    model, test_accuracies_GEM, confidence , after_unlearn_acc, after_unlearn_conf = run_cifar('GEM', Args())\n",
    "    # except:\n",
    "    #     print(iternumb)\n",
    "        # continue\n",
    "    test_accuracies_GEM_all_last_iter.append(test_accuracies_GEM[-20:])\n",
    "    unlearn_accuracies_GEM_all_last_iter.append(after_unlearn_acc) \n",
    "\n",
    "## average column-wise the test accuracies \n",
    "average_test_accuracies_GEM = np.mean(test_accuracies_GEM_all_last_iter, axis=0)\n",
    "average_unlearn_accuracies_GEM = np.mean(unlearn_accuracies_GEM_all_last_iter, axis=0)\n",
    "\n",
    "plt.bar(range(1, 21), average_test_accuracies_GEM)\n",
    "plt.title('Average Test Set for each task after learning all 20 Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Tasks')\n",
    "# plt.savefig('average_task_20_accuracy_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(range(1, 21), average_unlearn_accuracies_GEM)\n",
    "plt.title('Average Test Set for each task after unlearning the last task Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Tasks')\n",
    "# plt.savefig('average_task_20_accuracy_comparison_after_unlearning.png')\n",
    "plt.show()\n",
    "\n",
    "# plot the difference between the average test accuracies and the average unlearn accuracies\n",
    "difference = average_test_accuracies_GEM - average_unlearn_accuracies_GEM\n",
    "plt.bar(range(1, 21), difference)\n",
    "plt.title('Difference between average test accuracies and average unlearn accuracies')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Tasks')\n",
    "# plt.savefig('average_difference_task_20_accuracy_comparison.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
