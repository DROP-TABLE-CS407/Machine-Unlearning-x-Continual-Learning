|| conv1 weight size:  torch.Size([64, 3, 7, 7])
|| fc weight size:  torch.Size([1000, 512])
Layer: conv1 -> Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Layer: bn1 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Layer: relu -> ReLU(inplace=True)
Layer: maxpool -> Identity()
Layer: layer1 -> Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (1): BasicBlock(
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Layer: layer2 -> Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Layer: layer3 -> Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Layer: layer4 -> Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Layer: avgpool -> AdaptiveAvgPool2d(output_size=(1, 1))
Layer: fc -> Linear(in_features=512, out_features=10, bias=True)
|| conv1 weight size:  torch.Size([64, 3, 3, 3])
|| fc weight size:  torch.Size([10, 512])
||==========================GPU is available==========================||
||==========================TRAINING HYPERPARAMETERS========================||
|| Train data size =  50000
|| Test data size =  10000
|| Optimizer = Adam
|| Loss function = CrossEntropyLoss
|| Learning rate =  0.0005
|| Epochs =  400
|| Batch size =  32
||=========================START TRAINING=======================||
|| Epoch 1 Loss: 2.012430191040039
|| Epoch 2 Loss: 1.7116649150848389
|| Epoch 3 Loss: 1.0893594026565552
|| Epoch 4 Loss: 0.7839586734771729
|| Epoch 5 Loss: 0.37029579281806946
|| Epoch 6 Loss: 0.4607502222061157
|| Epoch 7 Loss: 0.3623247742652893
|| Epoch 8 Loss: 0.4771062433719635
|| Epoch 9 Loss: 0.14061635732650757
|| Epoch 10 Loss: 0.076090507209301
|| Epoch 11 Loss: 0.05105907469987869
|| Epoch 12 Loss: 0.035673998296260834
|| Epoch 13 Loss: 0.032082293182611465
|| Epoch 14 Loss: 0.20781365036964417
|| Epoch 15 Loss: 0.06881265342235565
|| Epoch 16 Loss: 0.001401255140081048
|| Epoch 17 Loss: 0.19748567044734955
|| Epoch 18 Loss: 0.12237896025180817
|| Epoch 19 Loss: 0.010442585684359074
|| Epoch 20 Loss: 0.41471126675605774
|| Epoch 21 Loss: 0.05918021872639656
|| Epoch 22 Loss: 0.004889900796115398
|| Epoch 23 Loss: 0.00511138467118144
|| Epoch 24 Loss: 0.004313859157264233
|| Epoch 25 Loss: 0.21275442838668823
|| Epoch 26 Loss: 0.0046303351409733295
|| Epoch 27 Loss: 0.01030790340155363
|| Epoch 28 Loss: 0.0265184435993433
|| Epoch 29 Loss: 0.28610286116600037
|| Epoch 30 Loss: 0.00289902207441628
|| Epoch 31 Loss: 0.13368824124336243
|| Epoch 32 Loss: 0.20977520942687988
|| Epoch 33 Loss: 0.019890405237674713
|| Epoch 34 Loss: 1.2637593746185303
|| Epoch 35 Loss: 0.14705102145671844
|| Epoch 36 Loss: 0.0060081216506659985
|| Epoch 37 Loss: 0.020686252042651176
|| Epoch 38 Loss: 1.8490938600734808e-05
|| Epoch 39 Loss: 0.02079078182578087
|| Epoch 40 Loss: 0.0009003793238662183
|| Epoch 41 Loss: 0.00027833544299937785
|| Epoch 42 Loss: 0.00036602740874513984
|| Epoch 43 Loss: 0.007775247097015381
|| Epoch 44 Loss: 0.007727083750069141
|| Epoch 45 Loss: 0.0006355373188853264
|| Epoch 46 Loss: 0.002931048395112157
|| Epoch 47 Loss: 0.19872307777404785
|| Epoch 48 Loss: 0.008806792087852955
|| Epoch 49 Loss: 0.04956817626953125
|| Epoch 50 Loss: 0.5133223533630371
|| Epoch 51 Loss: 0.003108675591647625
|| Epoch 52 Loss: 0.1585799753665924
|| Epoch 53 Loss: 0.14827947318553925
|| Epoch 54 Loss: 2.4875294911907986e-05
|| Epoch 55 Loss: 0.01473313570022583
|| Epoch 56 Loss: 0.40249085426330566
|| Epoch 57 Loss: 0.001018306938931346
|| Epoch 58 Loss: 1.0162291800952516e-05
|| Epoch 59 Loss: 0.001054040272720158
|| Epoch 60 Loss: 1.3410985957307275e-06
|| Epoch 61 Loss: 0.031141532585024834
|| Epoch 62 Loss: 0.0045202430337667465
|| Epoch 63 Loss: 0.001197476522065699
|| Epoch 64 Loss: 0.0005209402297623456
|| Epoch 65 Loss: 2.0426197806955315e-05
|| Epoch 66 Loss: 0.005866715684533119
|| Epoch 67 Loss: 0.04143870621919632
|| Epoch 68 Loss: 0.0014350390993058681
|| Epoch 69 Loss: 6.0476173530332744e-05
|| Epoch 70 Loss: 0.06536634266376495
|| Epoch 71 Loss: 0.003186618210747838
|| Epoch 72 Loss: 0.021733038127422333
|| Epoch 73 Loss: 0.011197368614375591
|| Epoch 74 Loss: 0.0003020739241037518
|| Epoch 75 Loss: 0.0011187527561560273
|| Epoch 76 Loss: 0.0032124498393386602
|| Epoch 77 Loss: 0.0034583532251417637
|| Epoch 78 Loss: 0.030290154740214348
|| Epoch 79 Loss: 0.00016375791165046394
|| Epoch 80 Loss: 0.056355491280555725
|| Epoch 81 Loss: 0.00048008604790084064
|| Epoch 82 Loss: 0.01473243162035942
|| Epoch 83 Loss: 0.0007907744729891419
|| Epoch 84 Loss: 5.811437517877494e-07
|| Epoch 85 Loss: 0.002184234792366624
|| Epoch 86 Loss: 0.00010920291242655367
|| Epoch 87 Loss: 8.866163625498302e-07
|| Epoch 88 Loss: 0.0011760732159018517
|| Epoch 89 Loss: 0.0037923997733742
|| Epoch 90 Loss: 4.882970461039804e-05
|| Epoch 91 Loss: 0.0020535793155431747
|| Epoch 92 Loss: 0.04587944969534874
|| Epoch 93 Loss: 0.002354957861825824
|| Epoch 94 Loss: 0.0010688143083825707
|| Epoch 95 Loss: 0.000249991164309904
|| Epoch 96 Loss: 0.0008352327276952565
|| Epoch 97 Loss: 0.006640834733843803
|| Epoch 98 Loss: 0.004901392851024866
|| Epoch 99 Loss: 1.6062633221736178e-05
|| Epoch 100 Loss: 0.006699541583657265
|| Epoch 101 Loss: 0.00016584218246862292
|| Epoch 102 Loss: 0.0006556806620210409
|| Epoch 103 Loss: 0.0014787421096116304
|| Epoch 104 Loss: 0.0002487217425368726
|| Epoch 105 Loss: 0.03186170756816864
|| Epoch 106 Loss: 0.0006024959729984403
|| Epoch 107 Loss: 5.451137985801324e-05
|| Epoch 108 Loss: 0.00023020838852971792
|| Epoch 109 Loss: 0.0005163789610378444
|| Epoch 110 Loss: 0.09712483733892441
|| Epoch 111 Loss: 0.010708058252930641
|| Epoch 112 Loss: 0.005489962641149759
|| Epoch 113 Loss: 0.0007087280391715467
|| Epoch 114 Loss: 0.41517162322998047
|| Epoch 115 Loss: 7.737995474599302e-05
|| Epoch 116 Loss: 9.85676706477534e-06
|| Epoch 117 Loss: 0.034335650503635406
|| Epoch 118 Loss: 1.4788100997975562e-05
|| Epoch 119 Loss: 0.0002482914424035698
|| Epoch 120 Loss: 1.087775785890699e-06
|| Epoch 121 Loss: 9.782162123883609e-06
|| Epoch 122 Loss: 5.036484708398348e-06
|| Epoch 123 Loss: 1.0273570296703838e-05
|| Epoch 124 Loss: 0.0002109476045006886
|| Epoch 125 Loss: 2.2351740014414645e-08
|| Epoch 126 Loss: 2.7268629310128745e-06
|| Epoch 127 Loss: 2.0784224034287035e-05
|| Epoch 128 Loss: 0.13920213282108307
|| Epoch 129 Loss: 0.009155862033367157
|| Epoch 130 Loss: 4.310765871196054e-05
|| Epoch 131 Loss: 2.942947958217701e-06
|| Epoch 132 Loss: 0.00025064096553251147
|| Epoch 133 Loss: 9.508828952675685e-05
|| Epoch 134 Loss: 0.0003200864011887461
|| Epoch 135 Loss: 0.0024040578864514828
|| Epoch 136 Loss: 0.06526528298854828
|| Epoch 137 Loss: 0.0019172077300027013
|| Epoch 138 Loss: 0.018825791776180267
|| Epoch 139 Loss: 1.1942392120545264e-05
|| Epoch 140 Loss: 0.007353541906923056
|| Epoch 141 Loss: 6.817068424425088e-06
|| Epoch 142 Loss: 1.5497046206291998e-06
|| Epoch 143 Loss: 0.004478370770812035
|| Epoch 144 Loss: 1.2144407719461014e-06
|| Epoch 145 Loss: 0.00016713443619664758
|| Epoch 146 Loss: 0.000748945982195437
|| Epoch 147 Loss: 1.5533563782810234e-05
|| Epoch 148 Loss: 5.7942284911405295e-05
|| Epoch 149 Loss: 0.0007944427197799087
|| Epoch 150 Loss: 0.0033340645022690296
|| Epoch 151 Loss: 0.00042420439422130585
|| Epoch 152 Loss: 0.006096071097999811
|| Epoch 153 Loss: 0.0006138182361610234
|| Epoch 154 Loss: 0.0035553984344005585
|| Epoch 155 Loss: 0.00012034733663313091
|| Epoch 156 Loss: 0.0009549923706799746
|| Epoch 157 Loss: 0.08730169385671616
|| Epoch 158 Loss: 3.887288039550185e-05
|| Epoch 159 Loss: 2.2351740014414645e-08
|| Epoch 160 Loss: 8.00146153778769e-06
|| Epoch 161 Loss: 9.062053868547082e-05
|| Epoch 162 Loss: 2.2351740014414645e-08
|| Epoch 163 Loss: 1.0579781246633502e-06
|| Epoch 164 Loss: 0.00028460874455049634
|| Epoch 165 Loss: 0.010713462717831135
|| Epoch 166 Loss: 1.415609034438603e-07
|| Epoch 167 Loss: 0.0015061255544424057
|| Epoch 168 Loss: 0.011655360460281372
|| Epoch 169 Loss: 0.00011148596968268976
|| Epoch 170 Loss: 1.74407414306188e-05
|| Epoch 171 Loss: 5.513417136171483e-07
|| Epoch 172 Loss: 6.034945272404002e-07
|| Epoch 173 Loss: 0.000325642351526767
|| Epoch 174 Loss: 4.023300448352529e-07
|| Epoch 175 Loss: 0.02967870980501175
|| Epoch 176 Loss: 1.7881373537420586e-07
|| Epoch 177 Loss: 8.940692453052179e-08
|| Epoch 178 Loss: 0.0
|| Epoch 179 Loss: 0.00011892226029885933
|| Epoch 180 Loss: 0.057369474321603775
|| Epoch 181 Loss: 5.13460545334965e-05
|| Epoch 182 Loss: 0.00014112626377027482
|| Epoch 183 Loss: 0.0017134076915681362
|| Epoch 184 Loss: 0.023323526605963707
|| Epoch 185 Loss: 0.0027320804074406624
|| Epoch 186 Loss: 0.005944044329226017
|| Epoch 187 Loss: 0.08272561430931091
|| Epoch 188 Loss: 0.009495651349425316
|| Epoch 189 Loss: 1.1666778846119996e-05
|| Epoch 190 Loss: 9.42814294830896e-05
|| Epoch 191 Loss: 3.651517181424424e-05
|| Epoch 192 Loss: 1.9659240933833644e-05
|| Epoch 193 Loss: 0.03701317310333252
|| Epoch 194 Loss: 8.679335223860107e-06
|| Epoch 195 Loss: 3.2394200388807803e-05
|| Epoch 196 Loss: 0.007263021543622017
|| Epoch 197 Loss: 0.041065361350774765
|| Epoch 198 Loss: 0.00012478436110541224
|| Epoch 199 Loss: 3.352753878971271e-07
|| Epoch 200 Loss: 1.2665982751514093e-07
|| Epoch 201 Loss: 2.6523509859543992e-06
|| Epoch 202 Loss: 7.450577754752885e-08
|| Epoch 203 Loss: 1.6167675767064793e-06
|| Epoch 204 Loss: 3.74206829292234e-05
|| Epoch 205 Loss: 6.332967359412578e-07
|| Epoch 206 Loss: 1.6479876649100333e-05
|| Epoch 207 Loss: 4.783098574989708e-06
|| Epoch 208 Loss: 3.799791841174738e-07
|| Epoch 209 Loss: 1.7447537175030448e-05
|| Epoch 210 Loss: 2.8237423066457268e-06
|| Epoch 211 Loss: 3.840836507151835e-05
|| Epoch 212 Loss: 0.004918763879686594
|| Epoch 213 Loss: 2.1010328055126593e-06
|| Epoch 214 Loss: 0.023723402991890907
|| Epoch 215 Loss: 1.3261944786790991e-06
|| Epoch 216 Loss: 4.157287094130879e-06
|| Epoch 217 Loss: 6.705521116145974e-08
|| Epoch 218 Loss: 7.450579175838357e-08
|| Epoch 219 Loss: 0.0017621918814256787
|| Epoch 220 Loss: 8.210013220377732e-06
|| Epoch 221 Loss: 4.397371594677679e-05
|| Epoch 222 Loss: 2.9802318834981634e-08
|| Epoch 223 Loss: 0.0
|| Epoch 224 Loss: 4.544839100617537e-07
|| Epoch 225 Loss: 4.4795106077799574e-05
|| Epoch 226 Loss: 9.1483787400648e-05
|| Epoch 227 Loss: 0.001454582903534174
|| Epoch 228 Loss: 2.5240287868655287e-05
|| Epoch 229 Loss: 0.04343147575855255
|| Epoch 230 Loss: 0.0005246145301498473
|| Epoch 231 Loss: 1.906465695356019e-05
|| Epoch 232 Loss: 2.1781701434520073e-05
|| Epoch 233 Loss: 5.199092993279919e-05
|| Epoch 234 Loss: 0.007094911299645901
|| Epoch 235 Loss: 1.635943590372335e-05
|| Epoch 236 Loss: 2.7994852644042112e-05
|| Epoch 237 Loss: 0.080011747777462
|| Epoch 238 Loss: 3.751738404389471e-05
|| Epoch 239 Loss: 0.000562151602935046
|| Epoch 240 Loss: 8.642622901788855e-07
|| Epoch 241 Loss: 4.470347292340193e-08
|| Epoch 242 Loss: 0.00015403158613480628
|| Epoch 243 Loss: 1.6508307453477755e-05
|| Epoch 244 Loss: 8.940691031966708e-08
|| Epoch 245 Loss: 0.00048444682033732533
|| Epoch 246 Loss: 1.3000014405406546e-05
|| Epoch 247 Loss: 7.674075277463999e-07
|| Epoch 248 Loss: 3.799790135872172e-07
|| Epoch 249 Loss: 0.0043367124162614346
|| Epoch 250 Loss: 9.685751223287298e-08
|| Epoch 251 Loss: 3.3750670809240546e-06
|| Epoch 252 Loss: 9.21613991522463e-06
|| Epoch 253 Loss: 6.474219389929203e-06
|| Epoch 254 Loss: 0.028172863647341728
|| Epoch 255 Loss: 0.0002575388934928924
|| Epoch 256 Loss: 2.1233961433608783e-06
|| Epoch 257 Loss: 9.819251317821909e-06
|| Epoch 258 Loss: 1.87443511094898e-05
|| Epoch 259 Loss: 0.00014944429858587682
|| Epoch 260 Loss: 0.10713618993759155
|| Epoch 261 Loss: 5.550467449211283e-06
|| Epoch 262 Loss: 0.0008695707656443119
|| Epoch 263 Loss: 0.0007369120139628649
|| Epoch 264 Loss: 0.030028749257326126
|| Epoch 265 Loss: 1.3037156350037549e-05
|| Epoch 266 Loss: 3.7252794982123305e-07
|| Epoch 267 Loss: 4.507465746428352e-06
|| Epoch 268 Loss: 1.3893790310248733e-05
|| Epoch 269 Loss: 8.195592613446934e-07
|| Epoch 270 Loss: 2.9802315282267955e-08
|| Epoch 271 Loss: 3.97109761252068e-06
|| Epoch 272 Loss: 2.011653492672849e-07
|| Epoch 273 Loss: 1.4901144140821998e-07
|| Epoch 274 Loss: 7.450580152834618e-09
|| Epoch 275 Loss: 0.0
|| Epoch 276 Loss: 2.0240055164322257e-05
|| Epoch 277 Loss: 8.77434722497128e-05
|| Epoch 278 Loss: 8.208128565456718e-05
|| Epoch 279 Loss: 2.1606646782856842e-07
|| Epoch 280 Loss: 0.0006421564612537622
|| Epoch 281 Loss: 5.8874029491562396e-05
|| Epoch 282 Loss: 1.0132707757293247e-06
|| Epoch 283 Loss: 2.756710841822496e-07
|| Epoch 284 Loss: 3.2260295483865775e-06
|| Epoch 285 Loss: 9.66646330198273e-05
|| Epoch 286 Loss: 1.5063415048643947e-05
|| Epoch 287 Loss: 1.7245714843738824e-05
|| Epoch 288 Loss: 1.0877756722038612e-06
|| Epoch 289 Loss: 8.1955943187495e-07
|| Epoch 290 Loss: 7.450580152834618e-09
|| Epoch 291 Loss: 1.3364913684199564e-05
|| Epoch 292 Loss: 0.0
|| Epoch 293 Loss: 0.0
|| Epoch 294 Loss: 0.0003932646941393614
|| Epoch 295 Loss: 0.0
|| Epoch 296 Loss: 3.0025119031051872e-06
|| Epoch 297 Loss: 1.151758260675706e-05
|| Epoch 298 Loss: 2.8609836135728983e-06
|| Epoch 299 Loss: 0.0001796236028894782
|| Epoch 300 Loss: 1.3559910030380706e-06
|| Epoch 301 Loss: 0.00016157288337126374
|| Epoch 302 Loss: 4.701139459939441e-06
|| Epoch 303 Loss: 0.0005591832450591028
|| Epoch 304 Loss: 0.02865210361778736
|| Epoch 305 Loss: 1.1473788390503614e-06
|| Epoch 306 Loss: 6.705520405603238e-08
|| Epoch 307 Loss: 7.525042633460544e-07
|| Epoch 308 Loss: 0.00011248444207012653
|| Epoch 309 Loss: 0.0
|| Epoch 310 Loss: 3.020483381988015e-05
|| Epoch 311 Loss: 0.169379323720932
|| Epoch 312 Loss: 7.450580152834618e-09
|| Epoch 313 Loss: 1.5539993910351768e-05
|| Epoch 314 Loss: 3.2782509151729755e-07
|| Epoch 315 Loss: 1.3112892247590935e-06
|| Epoch 316 Loss: 8.388982678297907e-06
|| Epoch 317 Loss: 2.9802318834981634e-08
|| Epoch 318 Loss: 0.15851876139640808
|| Epoch 319 Loss: 7.450580152834618e-09
|| Epoch 320 Loss: 0.0
|| Epoch 321 Loss: 1.0207214700130862e-06
|| Epoch 322 Loss: 0.0
|| Epoch 323 Loss: 9.74855647655204e-05
|| Epoch 324 Loss: 4.82041423310875e-06
|| Epoch 325 Loss: 2.756708852302836e-07
|| Epoch 326 Loss: 1.3953379493614193e-05
|| Epoch 327 Loss: 0.00025706092128530145
|| Epoch 328 Loss: 0.00020939757814630866
|| Epoch 329 Loss: 3.4408734791213647e-05
|| Epoch 330 Loss: 3.613481021602638e-05
|| Epoch 331 Loss: 0.00027900448185391724
|| Epoch 332 Loss: 4.041382999275811e-05
|| Epoch 333 Loss: 8.940644420363242e-07
|| Epoch 334 Loss: 0.0031347176991403103
|| Epoch 335 Loss: 1.4900994074196205e-06
|| Epoch 336 Loss: 1.0728756478783907e-06
|| Epoch 337 Loss: 5.177949788048863e-06
|| Epoch 338 Loss: 1.4901160305669237e-08
|| Epoch 339 Loss: 0.0004116276977583766
|| Epoch 340 Loss: 2.7566575226956047e-06
|| Epoch 341 Loss: 7.532297786383424e-06
|| Epoch 342 Loss: 0.00017884095723275095
|| Epoch 343 Loss: 4.343541149864905e-06
|| Epoch 344 Loss: 7.040595392027171e-06
|| Epoch 345 Loss: 1.5497055301239016e-06
|| Epoch 346 Loss: 0.01149687822908163
|| Epoch 347 Loss: 0.0
|| Epoch 348 Loss: 4.470347292340193e-08
|| Epoch 349 Loss: 2.8907652449561283e-06
|| Epoch 350 Loss: 2.6049250664073043e-05
|| Epoch 351 Loss: 0.00014295459550339729
|| Epoch 352 Loss: 2.2351740014414645e-08
|| Epoch 353 Loss: 4.6169305278453976e-05
|| Epoch 354 Loss: 0.10443701595067978
|| Epoch 355 Loss: 1.7357571778120473e-05
|| Epoch 356 Loss: 5.4581465519731864e-05
|| Epoch 357 Loss: 0.0032658728305250406
|| Epoch 358 Loss: 2.2351740014414645e-08
|| Epoch 359 Loss: 0.0
|| Epoch 360 Loss: 4.768275175592862e-06
|| Epoch 361 Loss: 0.00025187284336425364
|| Epoch 362 Loss: 1.5301744497264735e-05
|| Epoch 363 Loss: 0.0002555161772761494
|| Epoch 364 Loss: 1.796821743482724e-05
|| Epoch 365 Loss: 4.698966222349554e-05
|| Epoch 366 Loss: 3.372929495526478e-05
|| Epoch 367 Loss: 1.862642307060014e-07
|| Epoch 368 Loss: 0.00856634508818388
|| Epoch 369 Loss: 7.450556722687907e-07
|| Epoch 370 Loss: 8.940695295223122e-08
|| Epoch 371 Loss: 9.594750008545816e-05
|| Epoch 372 Loss: 8.396301382163074e-06
|| Epoch 373 Loss: 0.0003195481258444488
|| Epoch 374 Loss: 0.04383034631609917
|| Epoch 375 Loss: 3.7252892326478104e-08
|| Epoch 376 Loss: 2.5629472020227695e-06
|| Epoch 377 Loss: 6.586113613593625e-06
|| Epoch 378 Loss: 1.8626437281454855e-07
|| Epoch 379 Loss: 5.960188900644425e-06
|| Epoch 380 Loss: 3.725289587919178e-08
|| Epoch 381 Loss: 8.419121968472609e-07
|| Epoch 382 Loss: 0.4147453010082245
|| Epoch 383 Loss: 0.0
|| Epoch 384 Loss: 1.4901159417490817e-08
|| Epoch 385 Loss: 3.062124733332894e-06
|| Epoch 386 Loss: 5.483435870701214e-06
|| Epoch 387 Loss: 0.0
|| Epoch 388 Loss: 6.705518984517767e-08
|| Epoch 389 Loss: 0.034781694412231445
|| Epoch 390 Loss: 0.0
|| Epoch 391 Loss: 0.0
|| Epoch 392 Loss: 1.0207218110735994e-06
|| Epoch 393 Loss: 1.9371481130292523e-07
|| Epoch 394 Loss: 0.0
|| Epoch 395 Loss: 6.168788786453661e-06
|| Epoch 396 Loss: 8.724021427042317e-06
|| Epoch 397 Loss: 0.010872059501707554
|| Epoch 398 Loss: 1.2293337476876331e-06
|| Epoch 399 Loss: 6.655054312432185e-05
|| Epoch 400 Loss: 3.547980304574594e-05
||==========================END TRAINING=======================||



||===================START TEST/TRAIN ACCURACY=================||
|| Train Set Accuracy: 99.92%
|| Test Set Accuracy: 77.34%
||==============================END==========================||
|| Time to load data: 2.65s
|| Time to train model: 10794.53s
|| Time to get accuracy: 7.61s
|| Total time: 10804.79s
||==============================END==========================||
