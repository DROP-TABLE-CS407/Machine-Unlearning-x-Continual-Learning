{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Replay based Class Incremental Learning (CIL) with PyTorch\n",
    "\n",
    "This notebook covers a simplified implementation of class incremental learning (CIL) using replay-based techniques. The goal of CIL is to learn a model that can incrementally learn new classes without forgetting the previously learned classes. In this notebook, we will use a replay-based technique to store and replay the data from the previous classes to prevent forgetting.\n",
    "\n",
    "To do this, we will use the following steps:\n",
    "Train base model on the first set of two classes e.g. 0 and 1 for 100 epochs utilising Stochastic Gradient Descent (SGD) with a learning rate of 0.01.\n",
    "After training our base model, we will then store a subset of our data from the first two classes in a replay buffer, let $R_i$ be the replay buffer for class $i$ and $D_i$ be the training data for class $i$ up to that training step inclusive of all previous classes.\n",
    "\n",
    "We denote: $R_i \\subseteq D_i$\n",
    "\n",
    "And: $D_i = \\cup_{x = 0}^{i}d_x$\n",
    "\n",
    "Where $d_x$ is the data for class $x$.\n",
    "\n",
    "i.e. the replay buffer will only contain a variant subset of the data from the previous classes as opposed to all the data of that class which\n",
    "we can change to determine the effect of the replay buffer size on the model's performance.\n",
    "\n",
    "From this point onwards, we will incrementally train our model utilising Class-Incremental Learning (CIL) by training on the class and replaying the data from the previous classes.\n",
    "\n",
    "This model will utilise a standard implementation of a ResNet18 CNN model with a single fully connected layer at the end to classify the images. It will also be trained on the CIFAR-10 dataset, which contains 60,000 32x32 colour images in 10 classes, with 6,000 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom ResNet18 model for CIFAR10\n",
    "\n",
    "In this case we chance the convolution kernel to 3x3 and the stride to 1 for the first layer. We will also change the number of classes on the final softmax layer to 10 to match the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18CIFAR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        # change the first layer to accept 32x32 images with 3 channels rather than 224x224 images\n",
    "        # check the size of the input layer\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.resnet.bn1 = torch.nn.BatchNorm2d(128)\n",
    "        # change number of blocks per layer\n",
    "        self.resnet.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.resnet.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.resnet.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.resnet.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024)\n",
    "        )\n",
    "        # change input layer to accept 32x32 images by utilising smaller convolutional kernel\n",
    "        self.resnet.fc = torch.nn.Linear(1024, 10)\n",
    "        # start with 5 classes and add more as needed\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "        # maxpool worsens performance and is unnecessary for small image sizes\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "model = ResNet18CIFAR()\n",
    "model.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Preprocessing into separate class datasets\n",
    "\n",
    "Utilise the CIFAR-10 dataset alongside a prebuilt dataloader to load the data into separate class datasets.\n",
    "(Krishi's code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Algorithm\n",
    "\n",
    "Train initial model on the first two classes.\n",
    "\n",
    "For each class $i$ in the dataset:\n",
    "\n",
    "1. Train the model on the current class $i$ and replay the data from the previous classes.\n",
    "\n",
    "2. Store a subset of the data from the current class $i$ in the replay buffer $R_i$.\n",
    "\n",
    "3. Evaluate the model on the test set.\n",
    "\n",
    "4. Increment the number of classes seen.\n",
    "\n",
    "5. Repeat steps 1-4 for each class in the dataset.\n",
    "\n",
    "Note: the size of the replay buffer is a hyperparameter that can be tuned to determine the effect of the replay buffer size on the model's performance.\n",
    "\n",
    "The following hyperparameters will be used in this notebook:\n",
    "\n",
    "- Epochs: 100 (per class)\n",
    "- Learning rate: 0.00005\n",
    "- Momentum: 0.9 (if using SGD)\n",
    "- Batch size: 512\n",
    "- Replay buffer size: Variant (e.g. 1000, 2000, 3000, 4000, 5000 per class)\n",
    "\n",
    "### References\n",
    "\n",
    "1. [Continual Learning with Deep Architectures: A Review](https://arxiv.org/abs/1907.04471)\n",
    "\n",
    "2. [RECALL Replay Based Continual Learning in Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Maracani_RECALL_Replay-Based_Continual_Learning_in_Semantic_Segmentation_ICCV_2021_paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, accuracies, input_data, input_labels, task):\n",
    "    # calculate accuracy\n",
    "    correct = 0\n",
    "    total = len(input_data)\n",
    "    batch = 100\n",
    "    for j in range(0, len(input_data), batch):\n",
    "        image = input_data[j:j+batch]\n",
    "        labels = input_labels[j:j+batch]\n",
    "        outputs = model(image)\n",
    "        if task == 1:\n",
    "            outputs[:, :5].data.fill_(-10e10)\n",
    "        if task == 0:\n",
    "            outputs[:, 5:].data.fill_(-10e10)\n",
    "        # get the index of the highest value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # add the number of correct predictions to the total\n",
    "        correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy_string = \"Accuracy: \"\n",
    "    accuracies.append(correct/total * 100)\n",
    "    accuracy_string += (f\"{correct/total * 100:.2f}% \")\n",
    "\n",
    "    return accuracy_string, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "memory = torch.cuda.memory_allocated()\n",
    "print(f\"Memory allocated: {memory/1e9} GB\")\n",
    "\n",
    "replay_memory = {cls: [] for cls in CLASSES}\n",
    "accuracies = []\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# output is softmax gaussian probability distribution, so cross entropy loss is appropriate\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "\n",
    "# firstly, train on 5 classes utilising Adam\n",
    "current_train_data, current_train_labels = split_into_classes(train_data, train_labels, ['airplane', 'automobile', 'bird', 'cat', 'deer'])\n",
    "\n",
    "# convert to tensors\n",
    "current_train_data = torch.tensor(current_train_data).float()\n",
    "current_train_labels = torch.tensor(current_train_labels)\n",
    "\n",
    "test_data_per_class_1, test_labels_per_class_1 = split_into_classes(test_data, test_labels, ['airplane', 'automobile', 'bird', 'cat', 'deer'])\n",
    "test_data_per_class_2, test_labels_per_class_2 = split_into_classes(test_data, test_labels, ['dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "test_data_per_class_1 = torch.tensor(test_data_per_class_1).float()\n",
    "test_labels_per_class_1 = torch.tensor(test_labels_per_class_1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    current_train_data = current_train_data.cuda()\n",
    "    current_train_labels = current_train_labels.cuda()\n",
    "    test_data_per_class_1 = test_data_per_class_1.cuda()\n",
    "    test_labels_per_class_1 = test_labels_per_class_1.cuda()\n",
    "\n",
    "# we have figured out CUDA for jupyter notebooks :)\n",
    "for i in range(epochs):\n",
    "    accuracy_string, accuracies = calculate_accuracy(model, accuracies, current_train_data, current_train_labels, 0)\n",
    "    # calculate accuracy\n",
    "    \n",
    "    for j in range(0, len(current_train_data), batch_size):\n",
    "        optimiser.zero_grad()\n",
    "        images = current_train_data[j:j+batch_size]\n",
    "        labels = current_train_labels[j:j+batch_size]\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        if j == 0:\n",
    "            losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(f\"Epoch {i+1}/{epochs}, \" + accuracy_string + f\"Loss: {losses[len(losses)-1]}\")\n",
    "    # accuracy greater than 80% we stop training\n",
    "    if accuracies[len(accuracies)-1] > 85:\n",
    "        break\n",
    "    \n",
    "accuracy_string_1, _ = calculate_accuracy(model, accuracies, test_data_per_class_1, test_labels_per_class_1, 0)\n",
    "# remove last element from accuracies\n",
    "accuracies.pop()\n",
    "\n",
    "# print accuracy\n",
    "print(\"Task 1 test accuracy: \" + accuracy_string_1)\n",
    "\n",
    "test_data_per_class_2 = torch.tensor(test_data_per_class_2).float()\n",
    "test_labels_per_class_2 = torch.tensor(test_labels_per_class_2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    test_data_per_class_2 = test_data_per_class_2.cuda()\n",
    "    test_labels_per_class_2 = test_labels_per_class_2.cuda()\n",
    "\n",
    "accuracy_string_2, _ = calculate_accuracy(model, accuracies, test_data_per_class_2, test_labels_per_class_2, 1)\n",
    "accuracies.pop()\n",
    "\n",
    "print(\"Task 2 test accuracy: \" + accuracy_string_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Results for five classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and accuracy\n",
    "# fix axes labels\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "# show airplane and automobile accuracies\n",
    "\n",
    "plt.plot(accuracies, label='Train Set Accuracy')\n",
    "plt.title('Train Set Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# extract numerical values from accuracy strings\n",
    "accuracy_value_1 = float(accuracy_string_1.split(': ')[1].replace('%', ''))\n",
    "accuracy_value_2 = float(accuracy_string_2.split(': ')[1].replace('%', ''))\n",
    "\n",
    "# plot task 1 and task 2 test accuracies on a bar chart\n",
    "plt.bar(['Task 1', 'Task 2'], [accuracy_value_1, accuracy_value_2])\n",
    "plt.title('Test Set Accuracy for Task 1 and Task 2 (Post Task 1 Training)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_class(model, accuracies, data, labels):\n",
    "        # calculate accuracy\n",
    "        correct = [0] * 10\n",
    "        total = len(data)/10\n",
    "        batch_size = 1000\n",
    "        with torch.no_grad():\n",
    "            for j in range(0, len(data), batch_size):\n",
    "                image = data[j:j+batch_size]\n",
    "                label = labels[j:j+batch_size]\n",
    "                outputs = model(image)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                for i in range(10):  \n",
    "                    correct[i] += (predicted[label == i] == label[label == i]).sum().item()\n",
    "        \n",
    "        j = 0\n",
    "        accuracy_string = \"Accuracy: \"\n",
    "        for classes in accuracies:\n",
    "             accuracies[classes].append(correct[j]/total * 100)\n",
    "             accuracy_string += (f\"\" + classes + f\": {correct[j]/total * 100:.2f}%, \")\n",
    "             j += 1\n",
    "\n",
    "        return accuracy_string, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on next 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar import split_into_classes\n",
    "\n",
    "MEMORY_BUFFER = 250\n",
    "\n",
    "replay_memory_data, replay_memory_labels = split_into_classes(train_data, train_labels, ['airplane', 'automobile', 'bird', 'cat', 'deer'])\n",
    "replay_memory_data = replay_memory_data[:MEMORY_BUFFER]\n",
    "replay_memory_labels = replay_memory_labels[:MEMORY_BUFFER]\n",
    "old_weights = model.resnet.fc.weight.data.clone()\n",
    "new_10_output_layer = torch.nn.Linear(1024, 10)\n",
    "new_10_output_layer.weight.data[:10] = old_weights\n",
    "model.resnet.fc = new_10_output_layer\n",
    "\n",
    "# free up gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "memory = torch.cuda.memory_allocated()\n",
    "\n",
    "accuracies_class = {cls: [] for cls in CLASSES}\n",
    "accuracies_class_test = {cls: [] for cls in CLASSES}\n",
    "\n",
    "accuracies = []\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# output is softmax gaussian probability distribution, so cross entropy loss is appropriate\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "\n",
    "# firstly, train on 5 classes utilising SGD\n",
    "current_train_data, current_train_labels = split_into_classes(train_data, train_labels, ['dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "current_train_data = list(replay_memory_data) + list(current_train_data)\n",
    "current_train_labels = list(replay_memory_labels) + list(current_train_labels)\n",
    "\n",
    "# shuffle the data\n",
    "shuffled = list(zip(current_train_data, current_train_labels))\n",
    "random.shuffle(shuffled)\n",
    "current_train_data, current_train_labels = zip(*shuffled)\n",
    "\n",
    "# convert to tensors\n",
    "current_train_data = torch.tensor(current_train_data).float()\n",
    "current_train_labels = torch.tensor(current_train_labels)\n",
    "\n",
    "train_data = torch.tensor(train_data).float()\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    current_train_data = current_train_data.cuda()\n",
    "    current_train_labels = current_train_labels.cuda()\n",
    "\n",
    "# we have figured out CUDA for jupyter notebooks :)\n",
    "for i in range(epochs):\n",
    "    accuracy_string, accuracies = calculate_accuracy(model, accuracies, current_train_data, current_train_labels, -1)\n",
    "    _, accuracies_class = calculate_accuracy_class(model, accuracies_class, current_train_data, current_train_labels)\n",
    "    # calculate accuracy\n",
    "    \n",
    "    for j in range(0, len(current_train_data), batch_size):\n",
    "        optimiser.zero_grad()\n",
    "        images = current_train_data[j:j+batch_size]\n",
    "        labels = current_train_labels[j:j+batch_size]\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        if j == 0:\n",
    "            losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(f\"Epoch {i+1}/{epochs}, \" + accuracy_string + f\"Loss: {losses[len(losses)-1]}\")\n",
    "    # accuracy greater than 80% we stop training\n",
    "    if accuracies[len(accuracies)-1] > 85:\n",
    "        break\n",
    "\n",
    "accuracy_string_1, _ = calculate_accuracy(model, accuracies, test_data_per_class_1, test_labels_per_class_1, 0)\n",
    "# remove last element from accuracies\n",
    "accuracies.pop()\n",
    "\n",
    "# print accuracy\n",
    "print(\"Task 1 test accuracy: \" + accuracy_string_1)\n",
    "\n",
    "accuracy_string_2, _ = calculate_accuracy(model, accuracies, test_data_per_class_2, test_labels_per_class_2, 1)\n",
    "accuracies.pop()\n",
    "\n",
    "print(\"Task 2 test accuracy: \" + accuracy_string_2)\n",
    "\n",
    "current_test_data = torch.tensor(test_data).float()\n",
    "current_test_labels = torch.tensor(test_labels)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current_test_data = current_test_data.cuda()\n",
    "    current_test_labels = current_test_labels.cuda()\n",
    "    \n",
    "_, accuracies_class_test = calculate_accuracy_class(model, accuracies_class_test, current_test_data, current_test_labels)\n",
    "\n",
    "# print all classes in accuracies_class_test\n",
    "#for cls in CLASSES:\n",
    "#    print(f\"Test accuracy for {cls}: {accuracies_class_test[cls]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and accuracy\n",
    "# fix axes labels\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "# for i in range(len(accuracies)):\n",
    "#     print(f\"Accuracy at {i} : {accuracies[i]}\")\n",
    "# \n",
    "# for cls in CLASSES:\n",
    "#     plt.plot(accuracies_class[cls], label=f'{cls}')\n",
    "# \n",
    "# plt.title('Train Set Accuracy over Epochs')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plot the test set accuracy as a bar chart\n",
    "#plt.bar(CLASSES, [accuracies_class_test[cls][0] for cls in CLASSES])\n",
    "#plt.title('Test Set Accuracy for Each Class')\n",
    "#plt.xlabel('Classes')\n",
    "#plt.ylabel('Accuracy (%)') \n",
    "#plt.show()\n",
    "#for cls in CLASSES:\n",
    "#    print(f\"Test accuracy for {cls}: {accuracies_class_test[cls]}\")\n",
    "    \n",
    "# extract numerical values from accuracy strings\n",
    "accuracy_value_1 = float(accuracy_string_1.split(': ')[1].replace('%', ''))\n",
    "accuracy_value_2 = float(accuracy_string_2.split(': ')[1].replace('%', ''))\n",
    "    \n",
    "# plot task 1 and task 2 test accuracies on a bar chart\n",
    "plt.bar(['Task 1', 'Task 2'], [accuracy_value_1, accuracy_value_2])\n",
    "plt.title('Test Set Accuracy for Task 1 and Task 2 (Post Task 2 Training)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
