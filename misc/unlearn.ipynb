{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e8eb68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {

      "image/png": "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",

      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "# from GEM.gem import *\n",
    "from GEM.args import *\n",
    "\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "\n",
    "# import quadprog\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\"))  # Adds the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360b5c4aaa0b84a",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545fa36453e0dda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T15:16:56.387836Z",
     "start_time": "2024-11-13T15:16:56.266657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH = 'cifar-10-python' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# split the data into 10 classes by doing sort by key where in the keys are the labels and the values are the data\n",
    "train_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(train_data, train_labels):\n",
    "    train_split[CLASSES[label]].append(img)\n",
    "    \n",
    "# this makes more sense to me, effectively indexes 0-5000 are all airplanes, 5000-10000 are all automobiles etc\n",
    "test_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(test_data, test_labels):\n",
    "    test_split[CLASSES[label]].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849681c0f0bdd16",
   "metadata": {},
   "source": [
    "# PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b345dbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "Using cache found in /dcs/21/u2110391/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",

      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of ResNet18CIFAR(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "# make cuda available if available\n",
    "\n",
    "from cifar import load_cifar10_data, show_image\n",
    "\n",
    "# we want to create a resnet 18 model for 32x32 images\n",
    "# avoid upscaling, the model will take 32x32 images on input as opposed to 224x224\n",
    "\n",
    "initialisation = time.time()\n",
    "\n",
    "class ResNet18CIFAR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        # change the first layer to accept 32x32 images with 3 channels rather than 224x224 images\n",
    "        # check the size of the input layer\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.resnet.bn1 = torch.nn.BatchNorm2d(128)\n",
    "        # change number of blocks per layer\n",
    "        self.resnet.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.resnet.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.resnet.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.resnet.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(1024)\n",
    "        )\n",
    "        # change input layer to accept 32x32 images by utilising smaller convolutional kernel\n",
    "        self.resnet.fc = torch.nn.Linear(1024, 10)\n",
    "        # start with 5 classes and add more as needed\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "        # maxpool worsens performance and is unnecessary for small image sizes\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "model = ResNet18CIFAR()\n",
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9dd6044c0b8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||===================START CLASS-BY-CLASS ACCURACY=================||\n",

      "|| Overall Test Accuracy: 11.35% ||\n",
      "|| Accuracy for Class 0: 9.30% ||\n",
      "|| Accuracy for Class 1: 7.30% ||\n",
      "|| Accuracy for Class 2: 8.80% ||\n",
      "|| Accuracy for Class 3: 10.40% ||\n",
      "|| Accuracy for Class 4: 26.50% ||\n",
      "|| Accuracy for Class 5: 6.40% ||\n",
      "|| Accuracy for Class 6: 12.30% ||\n",
      "|| Accuracy for Class 7: 8.50% ||\n",
      "|| Accuracy for Class 8: 10.90% ||\n",
      "|| Accuracy for Class 9: 13.10% ||\n"


     ]
    }
   ],
   "source": [
    "# turn the data into a tensor\n",
    "test_data_tensor = torch.tensor(test_data).float()\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# Define the number of classes in your dataset\n",
    "num_classes = 10\n",
    "# Initialize counters for each class\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "print(\"||===================START CLASS-BY-CLASS ACCURACY=================||\")\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    test_data_tensor = test_data_tensor.cuda()\n",
    "    test_labels_tensor = test_labels_tensor.cuda()\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_data), 1000):\n",
    "        # Get the input and output\n",
    "        img = test_data_tensor[i:i + 1000]\n",
    "        label = test_labels_tensor[i:i + 1000]\n",
    "        \n",
    "        model = model.cuda()\n",
    "\n",
    "        # Get the prediction\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update per-class counters\n",
    "        for lbl, pred in zip(label, predicted):\n",
    "            class_total[lbl.item()] += 1\n",
    "            if lbl.item() == pred.item():\n",
    "                class_correct[lbl.item()] += 1\n",
    "\n",
    "        del img\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_accuracy = sum(class_correct) / sum(class_total) * 100\n",
    "print(f\"|| Overall Test Accuracy: {overall_accuracy:.2f}% ||\")\n",
    "\n",
    "# Print accuracy for each class\n",
    "for cls in range(num_classes):\n",
    "    if class_total[cls] > 0:\n",
    "        class_accuracy = class_correct[cls] / class_total[cls] * 100\n",
    "        print(f\"|| Accuracy for Class {cls}: {class_accuracy:.2f}% ||\")\n",
    "    else:\n",
    "        print(f\"|| No samples for Class {cls} ||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55059d",
   "metadata": {},
   "source": [
    "# UNLEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c22b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dcs/21/u2110391/CS407/Machine-Unlearning-x-Continual-Learning/Unlearning\n"
     ]
    }
   ],
   "source": [
    "%cd Unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8076b",
   "metadata": {},
   "source": [
    "### Prepare Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8280c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to be Unlearnt\n",
    "classes_to_unlearn = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c590ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from unlearn.scrub import train_distill, iterative_unlearn, scrub\n",
    "from utils import DistillKL, AverageMeter, accuracy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert data and labels to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32).cuda()\n",
    "train_labels_tensor = torch.tensor(train_labels).cuda()\n",
    "\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32).cuda()\n",
    "test_labels_tensor = torch.tensor(test_labels).cuda()\n",
    "\n",
    "# Split training data into forget and retain sets\n",
    "forget_mask = torch.isin(train_labels_tensor, torch.tensor(classes_to_unlearn).cuda())\n",
    "retain_mask = ~forget_mask\n",
    "\n",
    "# Get the indices of the forget and retain subsets\n",
    "forget_indices = forget_mask.nonzero(as_tuple=True)[0]\n",
    "retain_indices = retain_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Create the forget and retain datasets using the indices\n",
    "forget_dataset = TensorDataset(train_data_tensor[forget_indices], train_labels_tensor[forget_indices])\n",
    "retain_dataset = TensorDataset(train_data_tensor[retain_indices], train_labels_tensor[retain_indices])\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "data_loaders = OrderedDict(\n",
    "    forget = DataLoader(forget_dataset, batch_size=64, shuffle=True),\n",
    "    retain = DataLoader(retain_dataset, batch_size=64, shuffle=True),\n",
    "    test =  DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1726b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18CIFAR(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to device\n",
    "import copy\n",
    "umodel = copy.deepcopy(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "umodel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be992da",
   "metadata": {},
   "source": [
    "### DEFINE ALGORITHM HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78e88c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.unlearn_lr = 0.001         # Learning rate for unlearning\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 0.0005\n",
    "        self.dataset = ''      # Change as needed\n",
    "        self.num_classes = 10         # Number of classes in the dataset\n",
    "        self.batch_size = 64\n",
    "        self.print_freq = 10\n",
    "        self.warmup = 0               # Number of warmup epochs\n",
    "        self.imagenet_arch = False    # Set to True if using ImageNet architecture\n",
    "        self.seed = 42       \n",
    "        \n",
    "        # SCRUB SPECIFIC\n",
    "        self.kd_T = 1\n",
    "        self.msteps = 1\n",
    "        self.gamma = 10\n",
    "        self.beta = 1\n",
    "\n",
    "\n",
    "        # Add the following attributes to ensure compatibility\n",
    "        self.decreasing_lr = '50,75'  # Comma-separated epochs where LR decays\n",
    "        self.rewind_epoch = 0         # Epoch to rewind to; set to 0 if not using rewinding\n",
    "        self.rewind_pth = ''          # Path to the rewind checkpoint\n",
    "        self.gpu = 0                  # GPU ID to use; adjust as needed\n",
    "        self.surgical = False         # Whether to use surgical unlearning\n",
    "        self.unlearn = 'SCRUB'      # Unlearning method, e.g., 'retrain'\n",
    "        self.choice = []              # Layers to unlearn surgically; list of layer names\n",
    "        self.unlearn_epochs = 10     # Number of epochs for unlearning\n",
    "        self.epochs = 100   \n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79564f8",
   "metadata": {},
   "source": [
    "### Apply Unlearning Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f08fd0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Maximize step ***\n",
      "Epoch: [0][390/391]\tTime 0.035 (0.039)\tData 0.001 (0.000)\tLoss -13589.0020 (-2626.5295)\tForget_Acc@1 7.500 (9.476)\n",
      "*** Minimize step ***\n",
      "Epoch: [0][390/391]\tTime 0.035 (0.038)\tData 0.001 (0.000)\tLoss 61.0033 (2871.9625)\tRetain_Acc@1 22.500 (19.736)\n",
      "Epoch: [0]\t train-acc:\t19.736\t train-loss: 2871.962479925537\n",
      "one epoch duration:30.097084522247314\n",
      "Epoch #1, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Maximize step ***\n",
      "Epoch: [1][390/391]\tTime 0.035 (0.039)\tData 0.001 (0.000)\tLoss -172702.9219 (-26203.1169)\tForget_Acc@1 20.000 (18.464)\n",
      "*** Minimize step ***\n",
      "Epoch: [1][390/391]\tTime 0.035 (0.039)\tData 0.001 (0.000)\tLoss 59.1345 (53164.7128)\tRetain_Acc@1 10.000 (20.168)\n",
      "Epoch: [1]\t train-acc:\t20.168\t train-loss: 53164.71283635376\n",
      "one epoch duration:30.211318016052246\n",
      "Epoch #2, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [2][390/391]\tTime 0.035 (0.039)\tData 0.001 (0.000)\tLoss 85.5741 (55.5030)\tRetain_Acc@1 2.500 (22.748)\n",
      "Epoch: [2]\t train-acc:\t22.748\t train-loss: 55.5030003125\n",
      "one epoch duration:15.125917434692383\n",
      "Epoch #3, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [3][390/391]\tTime 0.035 (0.039)\tData 0.001 (0.000)\tLoss 44.5923 (41.0853)\tRetain_Acc@1 17.500 (23.376)\n",
      "Epoch: [3]\t train-acc:\t23.376\t train-loss: 41.08531365478515\n",
      "one epoch duration:15.259817123413086\n",
      "Epoch #4, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [4][390/391]\tTime 0.035 (0.039)\tData 0.001 (0.000)\tLoss 33.8473 (33.7872)\tRetain_Acc@1 20.000 (25.220)\n",
      "Epoch: [4]\t train-acc:\t25.22\t train-loss: 33.78717355712891\n",
      "one epoch duration:15.408300638198853\n",
      "Epoch #5, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [5][390/391]\tTime 0.036 (0.039)\tData 0.001 (0.000)\tLoss 35.9450 (33.0869)\tRetain_Acc@1 27.500 (24.564)\n",
      "Epoch: [5]\t train-acc:\t24.564\t train-loss: 33.086851896972654\n",
      "one epoch duration:15.434386253356934\n",
      "Epoch #6, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [6][390/391]\tTime 0.036 (0.040)\tData 0.001 (0.000)\tLoss 52.7853 (30.7297)\tRetain_Acc@1 10.000 (24.768)\n",
      "Epoch: [6]\t train-acc:\t24.768\t train-loss: 30.729704060058594\n",
      "one epoch duration:15.46924114227295\n",
      "Epoch #7, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [7][390/391]\tTime 0.036 (0.040)\tData 0.001 (0.000)\tLoss 23.4319 (28.3196)\tRetain_Acc@1 22.500 (24.548)\n",
      "Epoch: [7]\t train-acc:\t24.548\t train-loss: 28.319623963012695\n",
      "one epoch duration:15.471822023391724\n",
      "Epoch #8, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [8][390/391]\tTime 0.036 (0.040)\tData 0.001 (0.000)\tLoss 37.7431 (31.1538)\tRetain_Acc@1 37.500 (24.756)\n",
      "Epoch: [8]\t train-acc:\t24.756\t train-loss: 31.153844461669923\n",
      "one epoch duration:15.500604629516602\n",
      "Epoch #9, Learning rate: 0.001\n",
      "len(r_loader): 391, len(f_loader): 391\n",
      "*** Minimize step ***\n",
      "Epoch: [9][390/391]\tTime 0.036 (0.040)\tData 0.001 (0.000)\tLoss 38.2419 (27.7958)\tRetain_Acc@1 40.000 (24.660)\n",
      "Epoch: [9]\t train-acc:\t24.66\t train-loss: 27.795830544433592\n",
      "one epoch duration:15.517923831939697\n"
     ]
    }
   ],
   "source": [
    "import unlearn\n",
    "unlearn_method = unlearn.get_unlearn_method(args.unlearn)\n",
    "if args.unlearn == 'SCRUB':\n",
    "    model_s = copy.deepcopy(umodel)\n",
    "    model_t = copy.deepcopy(umodel)\n",
    "    module_list = nn.ModuleList([model_s, model_t])\n",
    "    unlearn_method(data_loaders, module_list, criterion, args)\n",
    "    umodel = module_list[0]\n",
    "else:\n",
    "    unlearn_method(data_loaders, umodel, criterion, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431fe87",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for each class\n",
    "from Unlearning import utils\n",
    "from Unlearning.trainer.val import validate\n",
    "\n",
    "\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "# Evaluate the unlearned model\n",
    "umodel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in data_loaders[\"test\"]:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        outputs = umodel(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update total and correct counts for each class\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            class_total[label.item()] += 1\n",
    "            if label.item() == prediction.item():\n",
    "                class_correct[label.item()] += 1\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_accuracy = 100 * sum(class_correct) / sum(class_total)\n",
    "print('Overall accuracy of the unlearned model on the test data: {:.2f}%'.format(overall_accuracy))\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        class_accuracy = 100 * class_correct[i] / class_total[i]\n",
    "        print('Accuracy for class {}: {:.2f}%'.format(i, class_accuracy))\n",
    "    else:\n",
    "        print('No samples for class {}'.format(i))\n",
    "\n",
    "\n",
    "for name, loader in data_loaders.items():\n",
    "        utils.dataset_convert_to_test(loader.dataset, args)\n",
    "        val_acc = validate(loader, umodel, criterion, args)\n",
    "        print(f\"{name} acc: {val_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"


  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
