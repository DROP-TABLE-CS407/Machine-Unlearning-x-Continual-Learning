{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay based Class Incremental Learning (CIL) with PyTorch\n",
    "\n",
    "This notebook covers a simplified implementation of class incremental learning (CIL) using replay-based techniques. The goal of CIL is to learn a model that can incrementally learn new classes without forgetting the previously learned classes. In this notebook, we will use a replay-based technique to store and replay the data from the previous classes to prevent forgetting.\n",
    "\n",
    "To do this, we will use the following steps:\n",
    "Train base model on the first set of two classes e.g. 0 and 1 for 100 epochs utilising Stochastic Gradient Descent (SGD) with a learning rate of 0.01 and momentum of 0.9.\n",
    "After training our base model, we will then store a subset of our data from the first two classes in a replay buffer, let $R_i$ be the replay buffer for class $i$ and $D_i$ be the training data for class $i$ up to that training step inclusive of all previous classes.\n",
    "\n",
    "We denote: $R_i \\subseteq D_i$\n",
    "\n",
    "And: $D_i = \\cup_{x = 0}^{i}d_x$\n",
    "\n",
    "Where $d_x$ is the data for class $x$.\n",
    "\n",
    "i.e. the replay buffer will only contain a variant subset of the data from the previous classes as opposed to all the data of that class which\n",
    "we can change to determine the effect of the replay buffer size on the model's performance.\n",
    "\n",
    "From this point onwards, we will incrementally train our model utilising Class-Incremental Learning (CIL) by training on the class and replaying the data from the previous classes.\n",
    "\n",
    "This model will utilise a standard implementation of a ResNet18 CNN model with a single fully connected layer at the end to classify the images. It will also be trained on the CIFAR-10 dataset, which contains 60,000 32x32 colour images in 10 classes, with 6,000 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from cifar import load_cifar10_data, split_into_classes, get_class_indexes \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = 'cifar-10-batches-py' \n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom ResNet18 model for CIFAR10\n",
    "\n",
    "In this case we chance the convolution kernel to 3x3 and the stride to 1 for the first layer. We will also change the number of classes on the final softmax layer to 10 to match the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18CIFAR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        # change the first layer to accept 32x32 images with 3 channels rather than 224x224 images\n",
    "        # check the size of the input layer\n",
    "        print(\"|| conv1 weight size: \", self.resnet.conv1.weight.size())\n",
    "        print(\"|| fc weight size: \", self.resnet.fc.weight.size())\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # change input layer to accept 32x32 images by utilising smaller convolutional kernel\n",
    "        self.resnet.fc = torch.nn.Linear(512, 10)\n",
    "        # the architecture is already built to support 10 classes\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "        # maxpool worsens performance and is unnecessary for small image sizes\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "# make a resnet model\n",
    "model = ResNet18CIFAR()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Preprocessing into separate class datasets\n",
    "\n",
    "Utilise the CIFAR-10 dataset alongside a prebuilt dataloader to load the data into separate class datasets.\n",
    "(Krishi's code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(DATASET_PATH)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# split the data into 10 classes by doing sort by key where in the keys are the labels and the values are the data\n",
    "train_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(train_data, train_labels):\n",
    "    train_split[CLASSES[label]].append(img)\n",
    "    \n",
    "# this makes more sense to me, effectively indexes 0-5000 are all airplanes, 5000-10000 are all automobiles etc\n",
    "test_split = {cls: [] for cls in CLASSES}\n",
    "for img, label in zip(test_data, test_labels):\n",
    "    test_split[CLASSES[label]].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Algorithm\n",
    "\n",
    "Train initial model on the first two classes.\n",
    "\n",
    "For each class $i$ in the dataset:\n",
    "\n",
    "1. Train the model on the current class $i$ and replay the data from the previous classes.\n",
    "\n",
    "2. Store a subset of the data from the current class $i$ in the replay buffer $R_i$.\n",
    "\n",
    "3. Evaluate the model on the test set.\n",
    "\n",
    "4. Increment the number of classes seen.\n",
    "\n",
    "5. Repeat steps 1-4 for each class in the dataset.\n",
    "\n",
    "Note: the size of the replay buffer is a hyperparameter that can be tuned to determine the effect of the replay buffer size on the model's performance.\n",
    "\n",
    "The following hyperparameters will be used in this notebook:\n",
    "\n",
    "- Epochs: 100 (per class)\n",
    "- Learning rate: 0.005\n",
    "- Momentum: 0.9\n",
    "- Batch size: 64\n",
    "- Replay buffer size: Variant (e.g. 2000, 4000, 6000, 8000, 10000 per class)\n",
    "\n",
    "### References\n",
    "\n",
    "1. [Continual Learning with Deep Architectures: A Review](https://arxiv.org/abs/1907.04471)\n",
    "\n",
    "2. [RECALL Replay Based Continual Learning in Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Maracani_RECALL_Replay-Based_Continual_Learning_in_Semantic_Segmentation_ICCV_2021_paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (j \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(current_train_data)\u001b[38;5;241m/\u001b[39mbatch_size) \u001b[38;5;241m*\u001b[39m (batch_size\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m/local/java/python-pip-packages.cs413/lib64/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/java/python-pip-packages.cs413/lib64/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/java/python-pip-packages.cs413/lib64/python3.9/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_memory = {cls: [] for cls in CLASSES} \n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# output is softmax gaussian probability distribution, so cross entropy loss is appropriate\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# firstly, train on airplanes and automobiles for 100 epochs utilising SGD with momentum\n",
    "current_train_data = train_split['airplane'] + train_split['automobile']\n",
    "current_train_labels = [0]*len(train_split['airplane']) + [1]*len(train_split['automobile'])\n",
    "\n",
    "# convert to tensors\n",
    "current_train_data = torch.tensor(current_train_data).float()\n",
    "current_train_labels = torch.tensor(current_train_labels)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    current_train_data = current_train_data.cuda()\n",
    "    current_train_labels = current_train_labels.cuda()\n",
    "\n",
    "# haven't figured out CUDA for jupyter notebooks yet\n",
    "for i in range(epochs):\n",
    "    for j in range(0, len(current_train_data), batch_size):\n",
    "        optimiser.zero_grad()\n",
    "        images = current_train_data[j:j+batch_size]\n",
    "        labels = current_train_labels[j:j+batch_size]\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        if (j == 0 and i == 0) or (j == (len(current_train_data)/batch_size) * (batch_size-1)):\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # calculate accuracy\n",
    "            correct = 0\n",
    "            total = len(current_train_data)\n",
    "            with torch.no_grad():\n",
    "                for k in range(0, len(current_train_data), batch_size):\n",
    "                    images = current_train_data[k:k+batch_size]\n",
    "                    labels = current_train_labels[k:k+batch_size]\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            accuracies.append(correct/total * 100)\n",
    "            print(f\"Epoch {i+1}/{epochs}, Loss: {loss.item()}, Accuracy: {correct/total * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Results for two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and accuracy\n",
    "# fix axes labels\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(accuracies)\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
